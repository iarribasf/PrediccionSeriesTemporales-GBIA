---
title: "Procesos ARIMA"
subtitle: "Previsión con Datos Temporales (GBIA)"
author: "Iván Arribas (Depto. Análisis Económico. Universitat de València)"
date: "Curso 2020-21"
output: 
  html_document:
    code_download: yes
    df_print: kable
    fig_caption: no
    highlight: pygments
    number_sections: yes
    self_contained: yes
    theme: cerulean
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
---

```{r chunk_setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      comment = "",
                      fig.align = "center", 
                      fig.show = "hold",
                      fig.height = 4,
                      fig.width = 8,
                      out.width = "80%") 
```

```{r options_setup, echo = FALSE}
options(scipen = 999) #- para quitar la notacion cientifica
```

```{r librerias, echo = FALSE}
library(forecast)
library(ggplot2); theme_set(theme_bw())
library(gridExtra)
library(grid)
library(aod)
```


# Introducción

Los __modelos ARIMA__ han mostrado ser uno se los métodos de ajuste de series temporales más valiosos desde que fueran formalizados en 1976 por Box y Jenkins, en su libro [*Time series analysis, forecasting and control*](http://www.amazon.com/Time-Analysis-Forecasting-George-Box/dp/0470272848). Además, dieron las pautas a seguir en el ajuste de una serie temporal para alcanzar buenas predicciones (véase epígrafe 6).

En este tema, y el siguiente, definiremos estos procesos y aprenderemos a identificarlos, estimarlos y hacer predicciones.

__Los procesos ARIMA son ahora el tronco de una amplia familia de procesos__ que requieren menos hipótesis para su aplicación o ajustan mejor bajo diferentes hipótesis: ARCH, GARCH, NGARCH,  IGARCH, EGARCH, GARCH-M, QGARCH, GJR-GARCH, TGARCH, fGARCH...

__Los procesos ARIMA y los métodos de Alisado Exponencial son complementarios__:

* Los modelos de Alisado lineales son casos especiales de modelos Arima,
* Los modelos de Alisado no lineales no tienen su contrapartida en modelos Arima
* Muchos modelos Arima no tiene contrapartida en los modelos de Alisado.  

Pero antes de entrar en materia es necesario definir una serie de conceptos que permitirán entender mejor __una serie temporal como una muestra de un proceso generador de datos__ (PGD). 

\
\

# Proceso estocástico

\

## Definición e hipótesis sobre el proceso

Un __proceso estocástico__ $Y_t$ es (sin excesiva precisión) una variable aleatoria que corresponde a momentos sucesivos del tiempo. A diferencia de los temas previos, en este vamos a estimar modelos sobre procesos estocásticos. Sería el equivalente para series temporales al modelo de regresión lineal que viste en el primer semestre para datos transversales.

Al igual que en _Predicción con datos transversales_, la aplicación de estos modelos requiere del cumplimiento de una serie de hipótesis. Para el caso de series temporales el proceso debe ser __normal__, __estacionario__ y __ergódico__.
  
\
  
### Proceso estacionario

Un proceso es __estacionario en sentido estricto__ cuando la distribución conjunta no varía al realizar un desplazamiento en el tiempo de todas las variables.

* Si $F(Y_{t_1},..., Y_{t_k})$ es la función de distribución conjunta y $h>0$, entonces el proceso es estacionario en sentido estricto si
$$F(Y_{t_1},..., Y_{t_k}) = F(Y_{t_1+h},..., Y_{t_k+h})$$
      
Intuitivamente, _la distribución de un proceso estocástico es independiente del momento del tiempo_.

Comprobar si un proceso es estacionario en sentido estricto es muy difícil, así que vamos a encontrar condiciones suficientes: _estacionariedad en media_ y en _sentido amplio_ (covarianza). **Bajo normalidad** un proceso estacionario en sentido amplio también lo será en sentido estricto.

**Proceso estacionario en media**

Un proceso es estacionario en media (o de primer orden) si su nivel se mantiene en el tiempo:
$$E[Y_t] = \mu \; \; \forall t$$

**Proceso estacionario en sentido amplio**

Un proceso (ya estacionario en media) es estacionario en sentido amplio, o de segundo orden, si sus momentos de orden dos no dependen del tiempo:

* La (auto)covarianza entre dos periodos de tiempo es finita y sólo depende del intervalo de tiempo transcurrido entre estos dos periodos:
$$Cov[Y_t, Y_{t+k}] = E[(Y_t - \mu)(Y_{t+k} - \mu)] = \gamma_k,\,\,\,\forall t$$

Observa que la varianza será entonces $Var[Y_t] = E[(Y_t - \mu)^2] = \gamma_0$.

\

La figura 1 muestra la serie Nacimientos que no es estacionaria ni en media, ni en varianza. No lo es en media por que presenta largos periodos de tendencia creciente y decreciente; y no lo es en varianza por que al inicio de la serie los datos presentna más variabilidad que a finales del siglo pasado. 

```{r, echo=FALSE}
nacimientos <- read.csv2("./series/nacimientos.csv", header = TRUE)
nacimientos <- ts(nacimientos[, 2],
                  start = c(1975, 1),
                  freq = 12)

autoplot(nacimientos,
         xlab = "",
         ylab = "Nacimientos",
         main = "Figura 1. Nacimientos mensuales")
```


**Proceso ergódico**

Para que un proceso sea ergódico las observaciones nuevas tienen que aportar suficiente información para que la varianza del valor medio converja a 0. Esto no ocurre si la dependencia entre las variables es muy fuerte.

Una condición necesaria pero no suficiente para que un proceso estacionario sea ergódico es:
$$\lim_{k\rightarrow \infty} \gamma_k = 0.$$

\
\

# Transformaciones de una serie

\

## Ideas generales

Una serie temporal $\{y_t\}_{t=1}^T$ no tiene porque verificar las condiciones de estacionariedad y ergodicidad. A continuación, veremos una serie de transformaciones que convierten una serie no estacionaria en estacionaria; no ergódica en ergódica; y, de paso, facilitan la verificación de la hipótesis de normalidad, que dejaremos para más adelante.
  
En el panel superior de la figura 2 vuelves a tener la serie de nacimientos, que denominaremos $y_t$, y en panel inferior tienes la diferencia de la transformación logarítmica de la serie, $log(y_t) - log(y_{t-1})$. La serie nacimientos no es estacionaria en media ni en varianza, pero la transformación logarítmica y la diferencia han logrado que sea estacionaria en ambos sentidos.

```{r, echo=FALSE}
cbind("Nacidos" = nacimientos,
      "Dif. de log nacidos" = diff(log(nacimientos))) %>%
  autoplot(facets = TRUE,
           xlab = "",
           ylab = "",
           main = "Figura 2. Nacimientos y diferencia del logaritmo de Nacimientos")
```

\

## Diferenciación

__La diferenciación permite transformar una serie no estacionaria en media en estacionaria en media__.
  
Diferenciar de orden $k$ consiste en restar a la observación de un periodo la de $k$ periodos antes:
$$\nabla_k y_t = y_t - y_{t-k}.$$

### Diferenciación regular ($k=1$) {-}
  
Un caso concreto es la __diferenciación regular o diferenciación de orden uno__, que consiste en restar a la observación de un periodo la del periodo precedente:
$$\nabla y_t = y_t - y_{t-1}.$$

Si $\nabla y_t$ no fuera estacionaria, se diferenciaría (regularmente) una segunda vez para obtener una doble diferenciación de primer orden:
$$\nabla^{2} y_t = \nabla(\nabla y_t) = \nabla y_t - \nabla y_{t-1} = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2}$$

En la práctica una sola diferenciación suele ser suficiente para obtener la estacionariedad en media; diferenciar dos veces es excepcional; y diferenciar tres o más veces no se da.

### Diferenciación estacional ($k=m$) {-}

Existe la __diferencia estacional__, que consiste en restar a la observación de un periodo la observación precedente de la misma estación. Si el orden estacional es $m$, entonces la diferencia estacional de $y_t$ es 
$$\nabla_m y_t = y_t - y_{t-m}.$$
Una serie no estacionaria en media puede pasar a serlo tras diferenciarla estacionalmente. Es decir, cualquiera de las dos diferenciaciones (regular o estacional) o ambas a la vez son alternativas para obtener la estacionariedad en media.

Además, __la diferenciación (regular, estacional o ambas) también permite alcanzar la ergodicidad__.

La figura 3 muestra un ejemplo de diferenciación regular y/o estacional. En el primer panel aparece la serie original Nacimientos $y_t$; el segundo panel muestra la serie diferenciada regularmente $\nabla y_t$; en el tercer panel la serie diferenciada estacionalmente $\nabla_m y_t$; y en el cuarto panel muestra la serie diferenciada regular y estacionalmente $\nabla\nabla_m y_t$.

```{r, echo=FALSE, fig.height = 5}
cbind("Nacidos" = nacimientos,
      "Dif. regular" = diff(nacimientos),
      "Dif. estacional" = diff(nacimientos, lag = 12),
      "Dif. reg. y esta." = diff(diff(nacimientos, lag = 12))) %>%
  autoplot(facets = TRUE,
           xlab = "",
           ylab = "",
           main = "Figura 3. Nacimientos")
```

_¿Qué transformación para nacimientos consideras que genera una serie estacionaria, tanto en media como en varianza?_ Siempre hay un cierto grado de subjetividad en la elección de las diferencias que hay que aplicar a una serie. En la figura 3 podemos considerar que la diferenciación regular (panel 2) es suficiente para lograr la estacionariedad en media y en varianza y terminar el proceso de diferenciación. Pero también podemos considerar que la serie es estacionaria en media pero no lo suficiente en varianza, y optar por la doble diferenciación, regular y estacional (panel 4).

### Diferenciación con `R` {-}

`R` dispone de la función `diff` para diferenciar una serie:

* `diff(x)` calcula la diferencia regular o de orden $1$, $\nabla y_t$
* `diff(x, lag = k)` calcula la diferencia de orden $k$, $\nabla_k y_t$
* `diff(x, lag = k, difference = d)` calcula $d$ diferencias de orden $k$, $\nabla_k^d y_t$

Si necesitas calcular una diferencia regular y otra estacional, $\nabla\nabla_m y_t$, debes usar `diff(diff(x, lag = m))`. El orden de las diferenciaciones no cambia el resultado.

Además, en `forecast` está disponible las funciones `ndiffs` y `nsdiffs` que estiman el número de diferencias regulares y estacionales, respectivamente, necesarias para que una serie sea estacionaria. Para la primera usa un contraste de raíces unitarias (que no veremos en este curso) y para la segunda un criterio _ad-hoc_.


### Operador Retardo {-}

Definimos el __operador retardo__ $L$ como $Ly_t = y_{t-1}$, es decir, retrasa un periodo la serie. En inglés se denomina _lag operator_ (L) o _backward shift_ (B)

Así, se tiene que
$$L^k y_t = y_{t-k}$$
y por tanto que
$$
\begin{aligned}
  \nabla y_t & = y_t - y_{t-1} = y_t - Ly_t = (1-L)y_t \\
  \nabla^d y_t & = (1-L)^d y_t \\
  \nabla_m y_t & = (1-L^m) y_t
\end{aligned}
$$

La siguiente tabla muestra un sencillo ejemplo del efecto del operador retardo sobre la serie $y_t$
```{r, echo=FALSE}
data.frame(y = 1:7, lag1_y = c(NA, 1:6), lag2_y = c(NA,NA,1:5) )
```



\

## Transformación logarítmica

Si la serie original no es estacionaria en varianza porque los datos crecen con el nivel de la serie, es posible obtener la estacionariedad por medio de transformaciones simples.
  
La transformación logarítmica de una serie es una alternativa. La figura 4 muestra la serie Nacimientos y su logaritmo. La variabilidad estacional con la transformación logarítmica (panel inferior) es menor que en la serie original (panel superior). 

```{r, echo=FALSE, fig.height = 5}
cbind("Nacidos" = nacimientos,
      "log(Nacidos)" = log(nacimientos)) %>%
  autoplot(facets = TRUE,
           xlab = "",
           ylab = "",
           main = "Figura 5. Serie Nacimientos y su transformaciones logarítmica")
```

**Nota:** la transformación logarítmica es un caso concreto de otra más general, la transformación de Box-Cox que no veremos en este curso:
$$
z_t = 
    \begin{cases}
      (y_t^{\lambda}-1)/\lambda & \,\,\,\lambda \neq 0 \\
      \log(y_t) & \,\,\, \lambda = 0 
    \end{cases}
$$

Se puede demostrar que $\lim_{\lambda \rightarrow 0} \;\; (y_t^{\lambda}-1)/\lambda = log(y_t)$.

\

## Diferencia, Logaritmo y Tasa de variación

La transformación $\nabla y_t$ se puede interpretar como variaciones en nivel, pero $\nabla^2 y_t$ no tiene ninguna interpretación. __No conviene perder de vista la interpretabilidad de las observaciones__.

Cuando una serie tiene que ser diferenciada dos veces para conseguir su estacionariedad, vale la pena probar una transformación alternativa que sea interpretable: $\nabla \log(y_t)$ o $\nabla_m \log(y_t)$.

Por un lado,
  $$\nabla \log(y_t) = \log(y_t) - \log(y_{t-1}) = \log\big(\frac{y_t}{y_{t-1}} \big)
  \approx \frac{y_t}{y_{t-1}} - 1 = \frac{y_t - y_{t-1}}{y_{t-1}} =TV y_t.$$

Por ejemplo, para una serie mensual _la diferencia regular del logaritmo (natural) es la Tasa de Variación Mensual de la serie_ $(\nabla \log(y_t)=TVM y_t)$, que tiene una clara interpretación como variación porcentual.

Por otro lado, 
$$\nabla_m \log(y_t) \approx \frac{y_t - y_{t-m}}{y_{t-m}} =TV_m y_t.$$

Es decir, para una serie mensual _la diferencia estacional del logaritmo es la Tasa de Variación Anual de la serie_ $(\nabla_{12} \log(y_t)=TVA y_t)$.

\
\

# Función de autocorrelación

Si la serie es estacionaria y ergódica, el valor medio de la serie es constante y, por tanto, no informativo. **Son las covarianzas los que caracterizan el proceso estocástico**.

Recordemos que $\gamma_k=Cov(y_t,y_{t-k})$ es la autocovarianza de orden k. Por tanto $\gamma_0$ es la varianza de la serie $y_t$. Sea $\rho_k$ la autocorrelación se orden $k$. Se puede verificar que:
$$\rho_k = cor(y_t, y_{t-k}) =\frac{\gamma_k}{\gamma_0}.$$

* $\rho_1$ mide la información que se transmite de un periodo al siguiente periodo.
* $\rho_k$ mide la información que se transmite k periodos hacia adelante.

Las autocorrelaciones caracterizan el proceso estocástico, y __la función de autocorrelación o correlograma  (FAC, o ACF en inglés) es el gráfico de $r_k$ contra $k$__, donde $r_k$ es la estimación de $\rho_k$ obtenida con las observaciones.

La figura 6 muestra la FAC para la serie Nacimientos y algunas de sus transformaciones. Observa el diferente uso del argumento `lag`: en la función `diff` indica el orden de la diferenciación, y en la función `ggAcf` indica el orden máximo de la autocorrelación. La primera columna muestra la FAC para Nacimientos y varias diferenciaciones, mientras que la segunda columna muestra la FAC para el logaritmo de los nacimientos y sus diferenciaciones. Se puede observar que:

* La FAC de una serie y de su transformación logarítmica son muy similares.
* En los paneles de la primera y tercera fila las autocorrelaciones decrecen muy lentamente, indicando que la serie analizada no es estacionaria ni ergódica. 
* En los paneles de la segunda fila las autocorrelaciones de orden estacional (12, 24,...) también decrecen lentamente, indicando que la serie analizada no es ergódica. 
* Solo la doble diferenciación regular y estacional de la serie (original o su logaritmo) muestran un rápido descenso en los coeficiente de autocorrelación (paneles de la última fila), indicando que la serie así transformada es estacionaria en media y ergódica.

```{r, eval = FALSE}
ggAcf(nacimientos, lag = 48)
ggAcf(log(nacimientos), lag = 48)
ggAcf(diff(nacimientos), lag = 48)
ggAcf(diff(log(nacimientos)), lag = 48)
ggAcf(diff(nacimientos, lag = 12),lag = 48)
ggAcf(diff(log(nacimientos), lag = 12), lag = 48)
ggAcf(diff(diff(nacimientos, lag=12)), lag = 48)
ggAcf(diff(diff(log(nacimientos), lag=12)), lag = 48)
```

```{r, echo = FALSE, fig.height= 8}
grid.arrange(
  ggAcf(nacimientos, lag = 48, main = "Figura 6. FAC para Nacimientos", xlab = "", ylab = expression(y[t])),
  ggAcf(log(nacimientos), lag = 48, main = "", xlab = "", ylab = expression("log("*y[t]*")")),
  ggAcf(diff(nacimientos), lag = 48, main = "", xlab = "", ylab = expression(nabla*y[t])),
  ggAcf(diff(log(nacimientos)), lag = 48, main = "", xlab = "", ylab = expression(nabla~"log("*y[t]*")")),
  ggAcf(diff(nacimientos, lag = 12), lag = 48, main = "", xlab = "", ylab = expression(nabla[12]*y[t])),
  ggAcf(diff(log(nacimientos), lag = 12), lag = 48, main = "", xlab = "", ylab = expression(nabla[12]~"log("*y[t]*")")),
  ggAcf(diff(diff(nacimientos), lag = 12), lag = 48, main = "", xlab = "", ylab = expression(nabla*nabla[12]*y[t])),
  ggAcf(diff(diff(log(nacimientos)), lag = 12), lag = 48, main = "", xlab = "", ylab = expression(nabla*nabla[12]~"log("*y[t]*")")),
  nrow = 4
)
```

También podemos pedir `R` que nos sugiera los órdenes de diferenciación con las funciones `ndiffs` y `nsdiffs`. 
```{r}
ndiffs(nacimientos)
nsdiffs(nacimientos)
```

Si queremos ver los valores numéricos de las autocorrelaciones debemos añadir a la función `ggAfc` el argumento `plot = FALSE`. Para la serie doblemente diferenciada vemos que las relaciones más elevadas se dan para un retardo ($r_{1}=-0.396$) y para 12 retardos ($r_{12}=-0.298$), el dato que más se parece a los nacimientos de un mes son los nacimientos del mes previo y los nacimientos del mismo mes del año previo.

```{r}
ggAcf(diff(diff(nacimientos), lag = 12), lag=12, plot = FALSE)
```

Las bandas azules de la FAC muestran el intervalo de confianza al 95% (IC95). Si $\rho_k = 0$, la distribución del estimador $r_k$ se distribuye aproximadamente como una normal de media $-1/T$ y varianza $1/T$. Las líneas punteadas de la FAC están dibujadas en las posiciones $\frac{-1}{T} \pm \frac{1.96}{\sqrt{T}}$.
 
* Si un $r_k$ cae fuera del IC95 hay evidencia para rechazar la hipótesis nula de que $\rho_k = 0$ a un nivel del 5%. Recordemos que incluso si todos los $\rho_k$ son cero, cabe esperar que un 5% de sus estimaciones $r_k$ caigan fuera del IC95.

* Los $\rho_k$ no son independientes. Si uno cae fuera del IC95, es más probable que los valores vecinos caigan también fuera.

\
\

# Procesos ARIMA

ARIMA surge de combinar las siglas de tres procesos diferentes: __AR__ de AutoRegresive, __I__ de Integrated y __MA__ de Moving Average. Veamos cada uno de estos tres conceptos por separado y luego su combinación. 

A lo largo de lo que resta del tema asumiremos que:

* $\{y_t\}_{t=1}^T$ es una realización de un proceso estocástico desconocido.
    
* El proceso estocástico es __estacionario en sentido amplio__:
$$E[y_t]  = \mu < \infty \;\;\; \forall t,$$
$$Cov[y_t, y_{t-k}]  = \gamma_k  \;\;\; \forall k.$$
     
* El proceso estocástico es __ergódico__, o su condición suficiente: 
$$\lim_{k \rightarrow \infty} \gamma_k  = 0.$$

\

## Procesos autorregresivos AR(p)

### Definición {-}

El modelo general __autorregresivo de orden p__, $y_t \sim AR(p)$ viene definido por
$$y_t=c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \varepsilon_t,$$
\noindent que usando el operador retardo queda
$$(1 - \phi_1 L - \phi_2 L^2 - ... - \phi_p L^p)y_t = c + \varepsilon_t$$

En este y en cualquier proceso ARIMA, al polinomio en $L$ que acompaña a $y_t$ se le denomina **polinomio autoregresivo**.

Se suele asumir que el error del modelo $\varepsilon_t$ verifica las hipótesis estándar de media cero, incorrelación, homocedasticidad e idéntica distribución: $\varepsilon_t \sim iid(0, \sigma^2)$. En este curso no vamos a prestar atención a este conjunto de hipótesis porque no jugarán ningún papel en la elección del modelo óptimo --aquel con mejores predicciones.


### Ejemplos {-}

* $y_t \sim AR(1): \;\;y_t = c + \phi_1 y_{t-1} + \varepsilon_t$ o $(1 - \phi_1 L)y_t = c + \varepsilon_t$
* $y_t \sim AR(2): \;\;y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \varepsilon_t$ o $(1 - \phi_1 L - \phi_2 L^2)y_t = c + \varepsilon_t$

\

## Procesos en medias móviles MA(q)

### Definición {-}

El modelo general __en medias móviles de orden q__, $y_t \sim MA(q)$ viene definido por
$$y_t=c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... + \theta_q \varepsilon_{t-q},$$
\noindent que usando el operador retardo queda
$$y_t = c + (1 + \theta_1 L + \theta_2 L^2 + ... + \theta_q L^q) \varepsilon_t$$

### Ejemplos {-}

* $y_t \sim MA(1): \;\;y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1}$ o $y_t = c + (1 + \theta_1 L)\varepsilon_t$
* $y_t \sim MA(2): \;\;y_t=c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2}$ o $y_t = c + (1 + \theta_1 L + \theta_2 L^2)\varepsilon_t$

\

## Procesos ARMA(p,q)

### Definición {-}

El modelo general $y_t \sim ARMA(p,q)$ viene dado por
$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p}  + 
        \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + ... +
        \theta_q \varepsilon_{t-q}+ \varepsilon_t,$$
\noindent que usando el operador retardo queda
$$(1 - \phi_1 L - ... - \phi_p L^p)y_t = c + (1 + \theta_1 L + ... + \theta_q L^q) \varepsilon_t.$$

### Ejemplos {-}

* $y_t \sim ARMA(1, 1): \;\;y_t = c  + \phi_1 y_{t-1} + \theta_1 \varepsilon_{t-1} + \varepsilon_{t}$ o $(1 - \phi_1 L)y_t = c + (1 + \theta_1 L)\varepsilon_t$.
* $y_t \sim ARMA(0, 0): \;\;y_t = c + \varepsilon_{t}$. Si $c = 0$, a este proceso se le denommina **ruido blanco**.

\

## Proceso ARIMA(p,d,q)

__Si la serie $y_t$ no es estacionaria pero tras diferenciarla $d$ veces se hace estacionaria, diremos que la serie es integrada de orden $d$__: $y_t \sim I(d)$. Por tanto,

* una serie estacionaria se indicará como $y_t \sim I(0)$
* $y_t \sim I(d)$ es equivalente a $\nabla^d y_t = (1 - L)^d y_t \sim I(0)$

Una serie $y_t$ sigue un proceso __$ARIMA(p,d,q)$__ si:

1. $y_t \sim I(d)$ (hay que diferenciarla $d$ veces para hacerla estacionaria), y
2. $\nabla^d y_t \sim ARMA(p,q)$.
      
Entonces, podemos escribir: 
$$y_t \sim  ARIMA(p,d,q): \;\;\; (1 - \phi_1 L - \ldots - \phi_p L^p)(1- L)^d y_t = c + (1 + \theta_1 L + ... + \theta_q L^q) \varepsilon_t.$$

### Ejemplos {-}

* $y_t \sim ARIMA(1, 1, 1): \;\;(1 - \phi_1 L)(1- L) y_t = c + (1 + \theta_1 L) \varepsilon_t$ o $y_t = c + y_{t-1} + \phi_1(y_{t-1} - y_{t-2}) + \theta_1 \varepsilon_{t-1} + \varepsilon_t$.
* $y_t \sim ARIMA(0, 1, 0): \;\;(1- L) y_t = c + \varepsilon_t$ o $y_t = c + y_{t-1} + \varepsilon_t$. Si $c=0$, tenemos un **paseo aletorio**; si $c \neq 0$, tenemos un **paseo aleatorio con deriva**.
 
\
\

# Aproximación de Box-Jenkins

La siguiente figura muestra el flujo de procesos asociado a la modelización por modelos ARIMA, con cuatro grandes áreas:

* __Identificación__, que requiere primero transformar la serie para que sea estacionaria y ergódica, para después identificar los valores de p y q.

  La FAC y la función de autocorrelación parcial (que no hemos visto) teóricas son diferentes en cada tipo de proceso. Idealmente, su estimación a partir de la serie temporal podría servir de ayuda en la identificación del proceso estocástico. En la práctica, la funciones estimadas son tan diferentes de las teóricas que resultan de muy poca ayuda. 
  
  Nosotros haremos uso de algunas funciones de _auto_ identificación que nos ayudaran en este punto.
  
* __Estimación__ de los parámetros del modelo, incluidas las variables de intervención y obtención del error. Dado un proceso, el método usual de estimación de sus parámetros es por máxima verosimilitud.

* __Validación__ de las hipótesis sobre el modelo. 

  En un curso tradicional de modelos Arima supondría validar la hipótesis sobre el residuo, contrastes de significatividad de los parámetros estimados y comprobación de que no hay más intervención. 
  
  En nuestro caso, que hemos reducido las hipótesis a su mínima expresión, simplemente analizaremos que no es necesaria más intervención y veremos la pertinencia de los parámetros del modelo (bien contrastando su significatividad o bien por alguna regla más sencilla).
  
  Si la validación no se pasa, puede ser necesario volver al proceso inicial y realizar una nueva identificación del modelo.
  
* __Predicción__ e interpretación del modelo válido. Si las predicciones se alejan de los valores reales más de lo esperado o presentan sesgo, puede ser necesario identificar y estimar un nuevo modelo.

![Aproximación Box-Jenkins](./imagenes/BoxJenkins.png)

\
\

### Identificación automática {-}

El paquete `forecast` dispone de la función `auto.arima()` que localiza el mejor modelo basándose en el AIC corregido para pequeñas muestras (`AICc`). No hay que fiarse ciegamente de los resultados de esta función, pero ayuda en la identificación. Básicamente el algoritmo seguido es el siguiente:

1. Determina el orden de diferenciación regular $0 \leq d \leq 2$ usando la función `ndiffs`. 
2. Tras diferenciar la serie:
    * se estiman una serie de modelos básicos predeterminados. 
    * se usa el criterio AICc para seleccionar el mejor de estos modelos. 
    * a partir del modelo seleccionado, se hacen pequeñas variaciones modificando en una unidad _p_ y _q_ y añadiendo/quitando la constante y se vuelve a seleccionar el mejor de los nuevos modelos.
3. Se repite el paso 2 hasta que no se puede mejorar el AICc. 

Cuando usemos esta función, debemos tener cuenta que:

* La función `auto.arima` no permite contante si la suma de las diferenciaciones es 2 o superior. 
* Si se desea hacer una búsqueda exhaustiva entre todos los posibles modelos se debe usar el argumento `stepwise = FALSE`.
* Si se desea que el cálculo de AICc sea exacto (por defecto para ganar tiempo calcula una aproximación), se debe usar el argumento `approximation = FALSE`.
* Si se desea ver para todos los modelos analizados el valor de AICc, se debe incluir el argumento `trace = TRUE`.

La función `auto.arima` tiende a sobre-parametrizar los modelos y es muy recomendable _ayudarla_ indicando las diferenciaciones, los posibles valores extremos... 

\
\

# Ejemplos

\

## Títulos de libros y panfletos

Vamos a aplicar la metodología de Box-Jenkins a la serie Libros (número de títulos publicados anualmente en España desde 1993 hasta 2018).


```{r}
libros <- read.csv2("./series/libros.csv", header = TRUE)
libros <- ts(libros[, 2], start = 1993, frequency = 1)

autoplot(libros,
         xlab = "", 
         ylab = "", 
         main = "Figura 7. Títulos publicados")
```

### Transformación de la serie {-}

El primer paso es transformar la serie original para que sea estacionaria. La figura 8 muestra la gráfica temporal y la FAC para la serie original y su primera diferencia. 

```{r, eval = FALSE}
autoplot(libros, xlab = "", ylab = "", main = "Libros")
autoplot(diff(libros), xlab = "", ylab = "", main = "Diferencia libros")
ggAcf(libros, xlab = "", ylab = "FAC", main = "")
ggAcf(diff(libros), xlab = "", ylab = "FAC", main = "")
```

```{r, echo = FALSE}
grid.arrange(
  autoplot(libros, xlab = "Libros", ylab = "", main = "Figura 9. Títulos publicados"),
  autoplot(diff(libros), xlab = "Diferencia libros", ylab = "", main = ""),
  ggAcf(libros, xlab = "", ylab = "FAC", main = ""),
  ggAcf(diff(libros), xlab = "", ylab = "FAC", main = ""),
  nrow = 2
)
```

Además,
```{r}
ndiffs(libros)
```

Podemos cocluir que la primera diferencia de la serie Libros es estacionaria y ergódica. Es decir, $d=1$ o $libros \sim I(1)$.

### Identificación {-}

Tras diferenciar la serie, vamos a identificar los valores de $p$ y $q$. Este es el proceso más difícil y para simplificar las cosas vamos a ayudarnos de la función `auto.arima`.

```{r}
auto.arima(libros, trace = TRUE)
```

Observa como la identificación automática da como mejor modelo $libros_t \sim ARIMA(0,1,0)$ sin deriva, por tanto, ruido blanco, $p=q=0$:
$$libros_t = libros_{t-1} + \varepsilon_t$$

### Estimación {-}

Aunque existe la función `arima` de `stats`, vamos a usar la función `Arima` de la librería `forecast` para estimar el modelo identificado por ser más versátil. El argumento `order` indica los valores de (p, d , q) como un vector y el argumento lógico `include.constant` indica si se desea incluir la constante $c$ en el modelo. (Mira en la ayuda de la función `Arima` la diferencia entre los argumentos `include.mean`, `include.drift` e `include.constant`).

```{r}
arima010 <- Arima(libros, 
                 order=c(0, 1, 0), 
                 include.constant = FALSE)
arima010
```

Nuestro modelo estimado es: $\widehat{libros}_t = libros_{t-1}$. La mejor predicción para un año, es la observación del año anterior, ¡el método ingenuo I!

### Intervención {-}

Se analiza si para algún año se observa un error atípico  (por ejemplo 3 veces superior al error estándar). La figura 9 muestra que en este caso en dos periodos, años 2008 y 2013, el residuo sobrepasa los dos errores estándar pero queda lejos de los tres errores estándar así que asumiremos que no hay valores atípicos.

```{r}
error <- residuals(arima010)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 9. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1993, 2019, 2)) 
```

### Medidas de error {-}

El error medio es `r round(accuracy(arima010)[2],0)` títulos (RMSE) y el error porcentual medio (MAPE) es `r round(accuracy(arima010)[5],2)`%.

```{r, eval=FALSE}
accuracy(arima010)
```

```{r,echo=FALSE}
round(accuracy(arima010),2)
```

### Predicción {-}

Una vez validado el modelo podemos pasar a realizar __predicciones__, en este caso a 5 años vista.

```{r}
parima010 <- forecast(arima010, h = 5, level = 95)
parima010
```

```{r}
autoplot(parima010, 
         xlab = "", 
         ylab = "Títulos",
         main = "Figure 10. Libros (1993-2018) y predicción (2019-2023)") +
  scale_x_continuous(breaks= seq(1993, 2023, 2)) 
```


La figura 10 muestra la serie, la previsión y el intervalo de confianza al 95%. La predicción es constante e igual al último dato. En las series diferenciadas el intervalo de confianza de las predicciones crece muy rápidamente porque los errores se van acumulando sin ningún tipo de amortiguamiento. 


\

## Aforo de vehículos

Vamos a aplicar de nuevo la metodología de Box-Jenkins a la serie __aforo de vehículos__ por Oropesa, carretera N-340, km. 996,48 (fuente Ministerio de Fomento). La serie es anual de 1960 a 2018 (59 datos).
    
```{r}
aforo <- read.csv2("./series/aforo_oropesa.csv", header = TRUE)
aforo <- ts(aforo, start = 1960, freq = 1)

autoplot(aforo, 
         xlab = "", 
         ylab = "Vehículos (000)",
         main = "Figura 11. Aforo de vehículos en N-340, Oropesa")
```

En este ejemplo vamos a trabajar con el logaritmo de la serie para poder ver algunos detalles relacionados con la interpretación del modelo y la predicción. Además, incluiremos, por primera vez, intervención y veremos como __la presencia de valores atípicos puede distorsionar el proceso de identificación__. Por ello, es conveniente realizar en paralelo ambas actividades, identificar el proceso y detectar valores atípicos.
      
### Transformación de la serie {-}

La figura 12 muestra que la serie Aforo (log) no es estacionaria. Así, el primer paso es transformar la serie original para que lo sea. La figura 12 no deja claro si la primera diferencia es suficiente para alcanzar la estacionariedad (gráficos de la segunda columna). Sin embargo, tras diferenciar dos veces la serie es claramente estacionaria. Por tanto se opta por considerar $d=2$ o $log(aforo_t) \sim I(2)$. La función `ndiffs` también aconseja la doble diferenciación.

```{r, eval = FALSE}
autoplot(log(aforo), 
         xlab = "log(Aforo)", ylab = "", main = "")
autoplot(diff(log(aforo)), 
         xlab = "Una diferencia de log(Aforo)", ylab = "", main = "")
autoplot(diff(log(aforo), differences = 2), 
         xlab = "Dos diferencias de log(Aforo)", ylab = "", main = "")
ggAcf(log(aforo), xlab = "", ylab = "FAC", main = "")
ggAcf(diff(log(aforo)), xlab = "", ylab = "FAC", main = "")
ggAcf(diff(log(aforo), differences = 2), xlab = "", ylab = "FAC", main = "")
```

```{r, echo = FALSE}
grid.arrange(
  autoplot(log(aforo), xlab = "log(Aforo)", ylab = "", main = "Figura 12. Aforo de vehículos"),
  autoplot(diff(log(aforo)), xlab = "Una diferencia de log(Aforo)", ylab = "", main = ""),
  autoplot(diff(log(aforo), differences = 2), xlab = "Dos diferencias de log(Aforo)", ylab = "", main = ""),
  ggAcf(log(aforo), xlab = "", ylab = "FAC", main = ""),
  ggAcf(diff(log(aforo)), xlab = "", ylab = "FAC", main = ""),
  ggAcf(diff(log(aforo), differences = 2), xlab = "", ylab = "FAC", main = ""),
  nrow = 2
)
```

```{r}
ndiffs(log(aforo))
```


### Identificación {-}

Veamos a identificar los valores de $p$ y $q$ a partir de `auto.arima`. La función sugiere un proceso MA(2). Parece que los dos coeficientes estimados son significativos --la estimación es mayor que dos veces su error estándar (standard error, s.e).

```{r} 
auto.arima(aforo, 
           lambda = 0)
```

Vamos a ver la gráfica de los residuos del modelo MA(2), vamos a identificar los valores extremos (intervención) y vamos a solicitar una vez más la auto-identificación pero incluyendo las variables ficticias asociadas a cada valor extremo.


```{r}
arima022 <- Arima(aforo, 
                  order = c(0, 2, 2),
                  lambda = 0)

error <- residuals(arima022)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 13. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1960, 2014, 4)) 
```

Se identifican dos posibles valores extremos en los años 1979 y 1981. Además, vamos a incluir otras dos intervenciones para los años 1984 y 2011 porque si no serían necesarias más adelante. Entonces, creamos una variable ficticia asociada a cada año d1979, d1981, d1984 y d2011, y las incluimos en la auto-identificación.

```{r}
d1979 <- 1*(time(error) == 1979)
d1981 <- 1*(time(error) == 1981)
d1984 <- 1*(time(error) == 1984)
d2011 <- 1*(time(error) == 2011)

auto.arima(aforo, 
           lambda = 0, 
           xreg = cbind(d1979, d1981, d1984, d2011))
```

Observa como la inclusión de intervención modifica la auto-identificación, que ahora es un proceso ARIMA(1,2,0). Asumimos que $log(aforo_t) \sim ARIMA(1,2,0)$ con intervención.

### Estimación {-}

```{r}
arima120 <- Arima(aforo, 
                  order = c(1, 2, 0), 
                  lambda = 0,  
                  xreg = cbind(d1979, d1981, d1984, d2011))
arima120
```

La identificación de errores atípicos --para la posterior inclusión de sus variables de intervención asociadas-- ha sido un tanto arbitraria: ¿es atípico el error que supera las 2 desviaciones típicas, las dos y media, las tres desviaciones típicas?

A fin de poner un poco de _objetividad_ en la decisión, podemos ver si sus coeficientes son significativos y dejar solo aquellas variables de intervención que lo sean. Aunque si la serie es suficientemente larga, también podríamos saltarnos este paso y dejar las variables de intervención que mejoren las predicciones extra-muestrales del modelo o las que recojan efectos con una gran evidencia.

La prueba de Wald permite contrastar si un subconjunto de coeficientes es significativo (se precisa la librería `aod`). Esta función requiere de tres argumentos: el vector de coeficientes (`b`), su matriz de covarianzas (`Sigma`) y la posición de los coeficientes cuya significatividad conjunta deseamos contrastar (`Terms`). Los dos primeros argumentos los podemos obtener del objeto `arima120` con las funciones `coef` y `vcov`. 

Por ejemplo, para ver la significatividad de la primera variable de intervención (segundo coeficiente del modelo), tendríamos
```{r}
wald.test(b = coef(arima120), 
                  Sigma = vcov(arima120), 
                  Terms = 2)
```

Con algo más de código se puede comprobar que todas las variables son significativas.

```{r}
ancho <- max(nchar(names(coef(arima120)))) + 2
for(i in 1:length(coef(arima120))) {
  wt <- wald.test(b = coef(arima120), 
                  Sigma = vcov(arima120), 
                  Terms = i)
  cat("\nCoeficiente: ", format(names(coef(arima120))[i], width = ancho), "valor de p: ", 
      formatC(wt$result$chi2[3], digits = 4, format = "f"))
}
```

### Intervención {-}

La figura 14 muestra que para ningún año se observa un error atípico. Si se incluye una intervención asociada al residuo más elevado en 1974, no resulta significativa.

```{r}
error <- residuals(arima120)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 14. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1960, 2014, 4)) 
```

\

### Medidas de error {-}

El error medio es `r round(accuracy(arima120)[2],0)` miles de vehículos (RMSE) y el error porcentual medio (MAPE) es `r round(accuracy(arima120)[5],2)`%.

```{r, eval=FALSE}
accuracy(arima120)
```

```{r,echo=FALSE}
round(accuracy(arima120),2)
```

### Interpretación del modelo {-}

El __modelo teórico__ es $log(aforo_t) \sim ARIMA(1,2,0) + d1979 + d1981 + d9184 + d2011$:
$$(1 - \phi_1 L)(1 - L)^2 log(aforo_t) =  \varepsilon_t + \gamma_1 \cdot d1979 + \gamma_2 \cdot d1981 + \gamma_3 \cdot d1984 + \gamma_4 \cdot d2011.$$

Si sustituimos $(1 - L)^2 log(aforo_t)$ por $(1 - L) TVAaforo_t$, donde $TVAaforo$ es la tasa de variación anual del aforo, y desarrollamos, queda:
$$TVAaforo_t = TVAaforo_{t-1} + \phi_1(TVAaforo_{t-1}-TVAaforo_{t-2}) +$$
$$\gamma_1 \cdot d1979 + \gamma_2 \cdot d1981 + \gamma_3 \cdot d1984 + \gamma_4 \cdot d2011 + \varepsilon_t.$$

Finalmente. el __modelo estimado__ es:
$$\widehat{TVAaforo}_t = TVAaforo_{t-1} -0.66(TVAaforo_{t-1}-TVAaforo_{t-2}) + $$
$$ - 0.15 \cdot d1979 + 0.08 \cdot d1981 + 0.08 \cdot d1984 - 0.09 \cdot d2011.$$
Cada año la tasa de variación del aforo es el misma que la del año pasado menos un 66% del último incremento entre las tasas de variación.
    
Respecto de la intervención, en 1979 hubo un 15% menos de vehículos de lo esperado, en 1981 y 1984 en torno a un 8% más y en 2011 un 9% menos.
      

### Predicción {-}

Como hemos incluido cuatro variables ficticias en el ajuste, de cara a predecir el aforo hemos de indicar cuales serán los valores futuros para estas variables. En este caso serán ceros puesto que son intervenciones que no responden a efectos calendario.
  
En `R` esto se hace incluyendo en el comando `forecast` el argumento `xreg = cbind(rep(0, 5), rep(0, 5), rep(0, 5), rep(0, 5))` que añade cinco ceros por cada variable de intervención porque la predicción va a ser a cinco años vista.

```{r}
parima120 <- forecast(arima120, 
                      h = 5, 
                      level = 95,
                      xreg = cbind(d1979=rep(0, 5), d1981=rep(0, 5), 
                                   d1984=rep(0, 5), d2011=rep(0, 5)))
parima120
```

```{r}
autoplot(parima120, 
     ylab = 'Vehículos (000)',
     main = 'Figura 15. Aforo (1960-2018) y predicción (2019-2023)') +
  scale_x_continuous(breaks= seq(1960, 2023, 4)) 
```

\

## Aforo de vehículos revisado

Vamos a volver a identificar el *mejor* modelo ARIMA para la serie Aforo, pero cambiando el enfoque. La función de autoidentificación usa como criterio de optimalidad un criterio de ajuste (AIC, BIC). Si lo que deseamos es identificar el modelo con mejores predicciones extra-muestrales, hay que proceder de otra forma. Vamos seleccionar un conjunto amplio de modelos, vamos a estimar cada uno de ellos y vamos a calcular el error (MAPE) para previsones extra-muestrales según el horizonte temporal. El mejor modelo será el que cometa menor error de previsión para el horizonte temporal necesario.

El siguiente código define un total de 96 modelos, resultado de la combinación de los posibles valores del orden autoregresivo $p = 0, 1, 2, 3$; el número de diferenciaciones $d = 0, 1, 2$; los posibles valores del orden en medias móviles $q = 0, 1, 2, 3$ y el uso de la transformación logarítmica (sí/no). 

Por simplicidad, hemos asumido que el modelo no tiene constante y no hemos incluido la intervención.

Por defecto, la función `Arima` solo estima modelos estacionarios. Para eludir esta restricción y que estime cualquier modelo, sea o no estacionario, hemos incluido el argumento `method = "ML"`.


```{r}
p <- 0:3
d <- 0:2
q <- 0:3
l <- 0:1
parametros <- expand.grid(p,d,q,l)
colnames(parametros) <- c("p", "d", "q", "log")

k <- 40                 #Minimo numero de datos para estimar
h <-  5                 #Horizonte de las predicicones
TT <- length(aforo)     #Longitud serie
s <- TT - k - h         #Total de estimaciones

MAPE <- matrix(NA, nrow(parametros), h)

for (para in 1:nrow(parametros)) {

  identificacion <- as.numeric(parametros[para, - 4])
  mapeArima <- matrix(NA, s + 1, h)
  for (i in 0:s) {
    train.set <- subset(aforo, start = i + 1, end = i + k)
    test.set <-  subset(aforo, start = i + k + 1, end = i + k + h)
    
    if(parametros[para, 4] == 0) 
      fit <- Arima(train.set, order = identificacion, method = "ML") else 
                     fit <- Arima(train.set, order = identificacion, 
                                  lambda = 0, method = "ML")
    fcast <- forecast(fit, h = h)
    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set
  }
  
  MAPE[para, ] <- colMeans(mapeArima)

}

# Mejores modelos si h = 1
ii <- order(MAPE[, 1], decreasing = FALSE)
cbind(parametros[ii[1:3],], error = round(MAPE[ii[1:3], 1], 3))

# Mejores modelos si h = 2
ii <- order(MAPE[, 2], decreasing = FALSE)
cbind(parametros[ii[1:3],], error = round(MAPE[ii[1:3], 2], 3))

# Mejores modelos si h = 3
ii <- order(MAPE[, 3], decreasing = FALSE)
cbind(parametros[ii[1:3],], error = round(MAPE[ii[1:3], 3], 3))

# Mejores modelos si h = 4
ii <- order(MAPE[, 4], decreasing = FALSE)
cbind(parametros[ii[1:3],], error = round(MAPE[ii[1:3], 4], 3))

# Mejores modelos si h = 5
ii <- order(MAPE[, 5], decreasing = FALSE)
cbind(parametros[ii[1:3],], error = round(MAPE[ii[1:3], 5], 3))
```

Las tablas identifican los tres mejores modelos según que el orden de previsión de interés, desde uno a cinco años.

* Para previsiones a un año vista el mejor modelo es $log(Aforo_t) \sim ARIMA(2,1,2)$. Aunque los otros dos modelos mostrados resultan equivalentes.
* Para previsiones a más de un año vista el mejor modelo es $log(Aforo_t) \sim ARIMA(2,0,3)$, en ocasiones muy superior al siguiente _mejor_ modelo.
* Ninguno de los modelo que mejores predicciones generan es el que mejor ajusta a los datos.


\

## Consumo de alimentos en el hogar per cápita

Analizaremos el __consumo alimentario en hogar per cápita__ en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (disponible en el Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (disponible en el Instituto Nacional de Estadística). Es una serie anual de 1987 a 2018 (32 datos) y la unidad es el Kg per cápita. La figura 23 muestra que es una serie muy irregular, con cambios de tendencia constantes.

```{r}
alimentospc <- read.csv2("./series/alimentacionpc.csv", header = TRUE)
alimentospc <- ts(alimentospc, start = 1987, freq = 1)
    
autoplot(alimentospc, 
         xlab = "", 
         ylab = "Kg per cápita",
         main = "Figura 16. Consumo alimentario en hogar")
```

### Transformación de la serie {-}

La figura 17 indica que la serie original ya es estacionaria y la función `ndiffs` lo corrobora. Por tanto asumimos que $d=0$ o $alimentospc_t \sim I(0)$.

```{r, eval = FALSE}
autoplot(alimentospc, xlab = "", ylab = "", main = "Alimentos")
autoplot(diff(alimentospc), xlab = "", ylab = "", main = "Diferencia alimentos")
ggAcf(alimentospc, xlab = "", ylab = "FAC", main = "")
ggAcf(diff(alimentospc), xlab = "", ylab = "FAC", main = "")
```

```{r, echo = FALSE}
grid.arrange(
  autoplot(log(alimentospc), xlab = "Alimentos", ylab = "", main = "Figura 17. Consumo de alimentos"),
  autoplot(diff(log(alimentospc)), xlab = "Diferencia de alimentos", ylab = "", main = ""),
  ggAcf(log(alimentospc), xlab = "", ylab = "FAC", main = ""),
  ggAcf(diff(log(alimentospc)), xlab = "", ylab = "FAC", main = ""),
  nrow = 2
)
```

### Identificación {-}

Para identificar los valores de $p$ y $q$ veremos que nos sugiere `auto.arima` :

```{r} 
auto.arima(alimentospc)
```

La identificación automática sugiere un proceso AR(1) con constante y ambos coeficientes parecen significativos.

Vamos a ver la gráfica de los residuos de este proceso para identificar rápidamente si hay valores extremos (figura 18).


```{r}
arima100 <- Arima(alimentospc, 
                  order = c(1, 0, 0))

error <- residuals(arima100)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 18. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1987, 2018, 3)) 
```

Ningún residuo supera las 2.5 desviaciones típicas así que consideraremos que $alimentospc_t \sim ARIMA(1,0,0)$.

### Coeficientes significativos {-}

Tanto $\phi_1$ como $\mu$ (la constante del modelo) son significativos.

```{r}
wald.test(b = coef(arima100), Sigma = vcov(arima100), Terms = 1)
wald.test(b = coef(arima100), Sigma = vcov(arima100), Terms = 2)
```

### Medidas de error {-}

El error medio es `r round(accuracy(arima100)[2],0)` Kg per cápita (RMSE) y el error porcentual medio (MAPE) es `r round(accuracy(arima100)[5],2)`%.

```{r, eval=FALSE}
accuracy(arima100)
```

```{r,echo=FALSE}
round(accuracy(arima100),2)
```

### Interpretación del modelo {-}

El __modelo teórico__ identificado es $alimentospc_t \sim ARIMA(1,0,0)$ + constante:
$$(1 - \phi_1 L) alimentospc_t = c + \varepsilon_t,$$

que desarrollando queda:
$$alimentospc_t = c + \phi_1 alimentospc_{t-1}+ \varepsilon_t.$$

Finalmente. el __modelo estimado__ es:
$$\widehat{alimentospc}_t = 225.21 + 0.65 \cdot alimentospc_{t-1}$$

__Observación__: El término contante $\mu$ que estima R no el valor de "c" que hemos visto en la teoría. Para convertir la contante estimada por R en "c" hemos de multiplicarla por el polinomio autoregresivo. En este caso,
$$c = \mu \cdot (1 - \phi_1) = 639.2512\cdot(1 - 0.6477) = 225.2082.$$

Cada año el consumo de alimentos per cápita en el hogar es 225 kilos más un 65% del consumo del año pasado.

### Predicciones de la serie {-}


```{r}
parima100 <- forecast(arima100, h = 5, level = 95)
parima100
```

Puedes comprobar que cada valor de la predicción se ha obtenido a partir del modelo estimado, donde $alimentospc_{t-1}$ se sustituye por la predicción del año precedente.

```{r}
autoplot(parima100, 
     ylab = "Kilos per cápita",
     main = "Figura 19. Consumo de alimentos y predicción") +
  scale_x_continuous(breaks= seq(1987, 2023, 4)) 
```

\

## Comparación con alisado exponencial

Veamos una comparativa, para los tres ejemplos vistos, entre los resultados obtenidos con ARIMA y con Alisado exponencial.
  
* Libros:
    + MAPE ARIMA: $7.02\%$ - ARIMA(0,1,0) sin deriva
    
    + MAPE ETS:   $7.05\%$ - ETS(M,N,N), $\alpha=1$
    
    + Ambos métodos han estimado el mismo modelo.

\vspace{.5cm}

* Aforo (log):
    + MAPE ARIMA: $5.24\%$ - ARIMA(1,2,0) sin deriva, con intervención
    
    + MAPE ETS:   $5.73\%$ - ETS(A,Ad,N), $\alpha=0.57$, $\beta=0.57$, $\phi = 0.85$
    
    + Cada método estima un modelo diferente
    
    + ARIMA tiene menor error a costa de incluir cuatro variables de intervención

\vspace{.5cm}

* Alimentos per cápita:
    + MAPE ARIMA: $1.37$ - ARIMA(1,0,0) con constante
    
    + MAPE ETS:   $1.43$ - ETS(M,N,N), $\alpha = 1$
    
    + Cada método ha estimado un modelo diferente, pero con una bondad de ajuste similar.


\
\

# Resumen de los comandos utilizados


|Función        |Paquete         |Descripción                                             |
|:--------------|:-------------------|:-------------------------------------------------------|
|`Arima`        |forecast  |estima un proceso ARIMA |
|`auto.arima`   |forecast  |identificación automática de un modelo ARIMA          |
|`wald.test`    |aod    |contrasta la significatividad conjunta de varios parámetros|
|`forecast`     |forecast  |realiza una predicción de un modelo|

\
\

# Referencias

* Box, G. E.P. y Jenkins, G. (1976). _Time Series Analysis: Forecasting and Control_  Editado por Holden-Day, San Francisco, CA

\
\
\
\


