---
title: "Técnicas de Alisado Exponencial"
subtitle: "Previsión con Datos Temporales (GBIA)"
author: "Iván Arribas (Depto. Análisis Económico. Universitat de València)"
output: 
  html_document:
    code_download: yes
    df_print: kable
    fig_caption: no
    highlight: pygments
    number_sections: yes
    self_contained: yes
    theme: cerulean
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
---

```{r chunk_setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      comment = "",
                      fig.align = "center", 
                      fig.show = "hold",
                      fig.height = 4,
                      fig.width = 8,
                      out.width = "80%") 
```

```{r options_setup, echo = FALSE}
options(scipen = 999) #- para quitar la notacion cientifica
```

```{r librerias, echo = FALSE}
library(forecast)
library(ggplot2); theme_set(theme_bw())
library(gridExtra)
library(grid)
```

# Introducción

Los métodos de alisado exponencial aparecen en los años 50 de la mano de Brown, Holt y Winters y han sido la raíz de uno de los métodos de predicción más sencillos y eficaces. La idea básica es predecir usando una media ponderada de los datos pasados, donde los más recientes tienen un peso mayor y este decae exponencialmente conforme usamos observaciones más antiguas.

El alisado exponencial es una familia de métodos de ajuste y previsión que ofrece muy buenos resultados para predicciones a corto plazo o para predecir series con pocos datos o _sencillas_ (sin mucho _ruido_).

Suponen un grado de modelización mayor que los métodos sencillos vistos previamente, pero sin alcanzar la complejidad de otras metodologías (modelos ARIMA).  

En origen, son métodos descriptivos con el único objetivo de producir __predicciones puntuales__. Sin embargo, su enfoque como modelos de _espacio de estados_ posibilita un marco teórico para obtener __intervalos de predicción__.

\
\

# Componentes de una serie en el contexto del alisado exponencial

Para obtener una predicción en el periodo $t+h$ con datos hasta el periodo $t$ necesitamos tres componentes:

* La estimación del nivel de la serie en el periodo $t$: $l_t$
* La estimación de la pendiente de la serie en el periodo $t$: $b_t$
* La estimación de la estacionalidad en el mes correspondiente al periodo $t+h$ con datos hasta $t$: $s_{t + h - m(k+1)}$ (recuerda, $m$ es el orden estacional y $k=\lfloor(h - 1)/m\rfloor$)

A partir de estas componentes, obtenidas en el periodo $t$ y para un esquema aditivo, se tendría que la predicción en el periodo $t+h$ es:
$$\widehat{y}_{t+h} = l_t+hb_t+s_{t+h-m(k+1)}.$$
En general, las componentes pueden __existir o no__ y se pueden combinar entre ellas __aditiva o multiplicativamente__. Veamos algunos casos:

* Existen todas y son multiplicativas:
$$\widehat{y}_{t+h}=l_t \cdot b_t^h \cdot s_{t + h - m(k+1)}$$
* Existen todas, nivel y pendiente aditivas, y estacionalidad multiplicativa:
$$\widehat{y}_{t+h}=(l_t+hb_t)s_{t + h - m(k+1)}$$
* No hay pendiente y la estacionalidad es aditiva:
$$\widehat{y}_{t+h}=l_t+s_{t + h - m(k+1)}$$

¿Como obtenemos los valores de $l_t$, $b_t$ y $s_{t + h - m(k+1)}$? Mediante __expresiones recursivas__, donde cada componente se calcula a partir de los valores hasta $t$ de la serie y de las componentes:
$$
\begin{aligned}
l_t& = f_l(y_t,y_{t-1}\ldots, l_{t-1},l_{t-2}\ldots,b_{t-1},b_{t-2}\ldots,s_{t-1},s_{t-2}\ldots) \\
b_t& = f_b(y_t,y_{t-1}\ldots, l_{t},l_{t-1}\ldots,b_{t-1},b_{t-2}\ldots,s_{t-1},s_{t-2}\ldots) \\
s_t& = f_s(y_t,y_{t-1}\ldots, l_{t},l_{t-1}\ldots,b_{t},b_{t-1}\ldots,s_{t-1},s_{t-2}\ldots)
\end{aligned}
$$
Por ejemplo, el _método ingenuo I_ se puede interpretar dentro de este contexto como un método de alisado donde $l_t = y_t$ y no hay ni pendiente ni estacionalidad. Por tanto, $\widehat{y}_{T+h} = l_{T} = y_{T}$.

De la misma forma, el _método ingenuo II_ se puede interpretar como un método de alisado donde $l_t = y_t$, $b_t = y_t - y_{t-1}$ y no hay estacionalidad. Entonces, $\widehat{y}_{T+h}=l_T + hb_T = y_T + h(y_T - y_{T-1})$.
    
El concepto de componentes aquí visto no coincide con el definido en el Tema 1. Sin embargo, podemos asimilar la tendencia de una serie como la suma (multiplicación) del nivel y la pendiente $T_{t+1} = l_t + b_t$ ($T_{t+1} = l_t \cdot b_t$) y de esta forma ambas definiciones de componentes de una serie se hacen compatibles.

\
\

# Casos posibles

Todas las series tiene nivel, pero dependiendo del tipo de pendiente y estacionalidad hay 15 casos posibles, mostrados en la tabla siguiente.

|    Tendencia          |           | Estacionalidad |                  |
|:----------------------|:---------:|:--------------:|:----------------:|
|                       | Ninguna (N) | Aditiva (A) | Multiplicativa (M)        |
| Ninguna (N)           |__N, N__   |       N, A     |        N, M      |
| Aditiva (A)           |__A, N__   |   __A, A__     |    __A, M__      |
| Aditiva Amortiguada (Ad)  |__Ad, N__  |      Ad, A     |       Ad, M      |
| Multiplicativa (M)    |    M, N   |       M, A     |        M, M      |
| Multiplicativa Amortiguada (Md) |    Md, N  |      Md, A     |       Md, M      |


Cada caso difiere en las componentes que se observan y su esquema, dando lugar a un conjunto diferente de ecuaciones recursivas de actualización.

Si se añade que el error puede ser aditivo o multiplicativo, da 30 posibilidades. El tipo de error (aditivo o multiplicativo) no afecta ni a la estimación ni a la previsión puntual, sólo es relevante en el cálculo del intervalo de confianza de las predicciones.

Los modelos más usuales son:

* (N, N):   Alisado exponencial simple
* (A, N):   Alisado de Holt
* (Ad, N):  Alisado con tendencia amortiguada (d de _damped_)
* (A, A):   Alisado de Holt-Winters aditivo
* (A, M):   Alisado de Holt-Winters multiplicativo

Acude al artículo de [Rob J. Hyndman y Yeasmin Khandakar (2008)](http://www.jstatsoft.org/v27/i03/paper) para saber más de cada modelo, o al libro _Forecasting with Exponential Smoothing: the State Space Approach_ (2008) de Hyndman y otros autores.

\
\

# Alisado exponencial simple (N, N)

\

## Definición

El alisado exponencial simple es adecuado para una serie estacionaria y sin estacionalidad. Es decir, una serie que se mueve alrededor de un nivel constante.

La ecuación de la __predicción intra-muestral__ es 
$$\widehat{y}_{t+1} = \alpha y_t + \alpha (1-\alpha) y_{t-1} + \alpha (1-\alpha)^2 y_{t-2} + \alpha (1-\alpha)^3 y_{t-3} + \ldots =  \alpha y_t + (1-\alpha)\widehat{y}_{t},$$
donde $0 \leq \alpha \leq 1$ es el parámetro de suavizado. La primera __predicción extra-muestral__ queda 
$$\widehat{y}_{T+1}=\alpha y_T + (1-\alpha)\widehat{y}_{T}$$
y para las restantes
$$\widehat{y}_{T+h} = \widehat{y}_{T+1}.$$
\

## Formulas interactivas de sus componentes

En el alisado exponencial simple solo hay una componente, el nivel $l_t$.

* La __ecuación recursiva__ de suavizado es $l_t=\alpha y_t + (1-\alpha)l_{t-1}$
* La ecuación de __predicción intra-muestral__ es $\widehat{y}_{t+1} = l_t$
* La ecuación de __predicción extra-muestral__ es $\widehat{y}_{T+h} = \widehat{y}_{T+1} = l_T$

Dos estimaciones razonables de $l_t$, el nivel de la serie en el periodo $t$, son el valor observado para la serie en ese periodo $y_t$ y el nivel del periodo previo $l_{t-1}$. La estimación final de $l_t$ es una media ponderada de ambas y esta estimación final es la previsión de la serie para el periodo siguiente. 

\

## Estimación de los parámetros del modelo
    
Dado el proceso iterativo para el cálculo de $l_t$ se necesita un __valor inicial__ de arranque $l_0$. Cada programa estadístico usa su propio método para obtener $l_0$.
    
Respecto de $\alpha$, usualmente se estima el valor __optimo__ según un criterio de precisión de la predicción. El parámetro $\alpha$ __se puede interpretar__ como:

* Si $\alpha = 1$ se tiene el _método ingenuo I_ ($\widehat{y}_{t+1}=y_t$), óptimo cuando el nivel de la serie varía constantemente en el tiempo.
* Si $\alpha = 0$ se tiene $\widehat{y}_{t} =l_0$, óptimo cuando el nivel permanece constante en el tiempo.

\

## Ejemplo

Vamos a usar el método de alisado exponencial simple para predecir la serie Libros. Usaremos para ello la función `ses` (_simple exponential smoothing_) con una previsión a 5 años vista (`h = 5`). Esta función estima los valores de $l_0$ y $\alpha$ que maximizan la función de verosimilitud, pero se pueden elegir otros criterios con el parámetro `opt.crit`.

```{r}
libros <- read.csv2("./series/libros.csv", header = TRUE)
libros <- ts(libros[, 2], start = 1993, frequency  = 1)


librosf <- ses(libros, h = 5, level = 95)
summary(librosf)
```

Veamos la salida en detalle:

* El valor de $\alpha$ que optimiza el criterio usado para medir la calidad del ajuste es $\alpha =$ `r round(librosf$model$par[1],2)`, un valor muy cercano a 1. Esto es un indicativo de que: i) la serie Libros cambia de nivel de forma constante, un rasgo en los procesos puramente estocásticos como el paseo aleatorio; y ii) el método de alisado exponencial simple se aproxima al método Ingenuo I. 
* El valor de arranque $l_0$ óptimo es `r as.integer(librosf$model$par[2])`. 
* _sigma_ es la desviación típica del error (aditivo) de predicción. Se diferencia de RMSE en el denominador, dado que para _sigma_ hay una corrección según los grados de libertad.
* La calidad del ajuste es razonable, como evidencia el error porcentual medio del 7%.
* Las predicciones son las mismas para los 5 años, como cabe esperar (recuerda que $\widehat{y}_{T+h} = l_T$).

La figura 1 muestra la serie Libros y las previsiones extra-muestrales que son constantes. 
 
```{r}
autoplot(librosf,
         xlab = "",
         ylab = "Títulos",
         main = "Figura 1. Libros y predicción con alisado simple")
```

\
\

# Alisado exponencial de Holt (A, N)

El alisado exponencial de Holt es adecuado para una serie no estacionaria y sin estacionalidad.

\

## Formulas interactivas de sus componentes

Las __ecuaciones recursivas__ son
$$
\begin{aligned}
l_t & =\alpha y_t + (1-\alpha)(l_{t-1}+b_{t-1}) \\
b_t & =\beta (l_t - l_{t-1}) + (1-\beta)b_{t-1} 
\end{aligned}
$$
La ecuación de la __predicción intra-muestral__ a un periodo vista es
$$\widehat{y}_{t+1} = l_t + b_t,$$
\noindent de forma que la ecuación de __predicción extra-muestral__ es 
$$\widehat{y}_{T+h}=l_T + h b_T.$$ 
 
Dos estimaciones razonables del nivel de la serie en el periodo $t$ son el valor observado para la serie en ese periodo $y_t$, y una estimación del nivel del periodo $t$ realizada desde el periodo $t-1$: $l_{t-1} + b_{t-1}$. Por otro lado, dos estimaciones razonables de la pendiente de la serie en el periodo $t$ son el cambio de nivel de $t-1$ a $t$ (el último observado) $l_t-l_{t-1}$, y el valor de la pendiente en el periodo previo, $b_{t-1}$. En ambos casos, nivel y pendiente, la estimación final es una media ponderada, parametrizada por $0 \leq \alpha, \: \beta \leq 1$.

\

## Estimación de los parámetros del modelo

Para aplicar este método es necesario estimar unos valores iniciales $l_0$ y $b_0$ de las ecuaciones recursivas e identificar los valores más adecuados de los parámetros $\alpha$ y $\beta$. Los __valores óptimos__ de estos cuatro parámetros se obtienen optimizando una medida de precisión de las predicciones.

La interpretación del parámetro $\alpha$ es similar al caso del alisado exponencial simple.

__Interpretación del parámetro $\beta$__:

* Si $\beta = 1$, $b_t  = l_t - l_{t-1}$, la pendiente se actualiza constantemente porque varía periodo a periodo Puede ser un indicador de mal ajuste (tendencia no lineal o pendiente no aditiva).
* Si $\beta = 0$, $b_t = b_{t-1}= \ldots = b_0$, la pendiente se mantiene constante en el tiempo.

El _método ingenuo II_ es un caso concreto de Alisado de Holt. Si hacemos $\alpha=\beta = 1$, queda $l_t=y_t$ y $b_t=y_t-y_{t-1}$, por tanto
$$\widehat{y}_{T+h}=l_T + h \cdot b_T = y_T + h(y_T - y_{T-1}).$$

\

## Ejemplo

Vamos a usar el método de alisado de Holt para predecir de nuevo la serie Libros. Usaremos para ello la función `holt` con una previsión a 5 años vista (`h = 5`).

```{r}
librosf <- holt(libros, h = 5, level = 95)
summary(librosf)
```

Los valores óptimos de los cuatro parámetros son $\alpha=$ `r round(librosf$model$par[1],2)`, $\beta=$ `r round(librosf$model$par[2],2)`, $l_0 =$ `r as.integer(librosf$model$par[3])` y $b_0 =$ `r as.integer(librosf$model$par[4])`. Observa que $\alpha$ es prácticamente 1 y que $\beta$ es cero. Si aplicamos estos valores de los parámetros a las ecuaciones recursivas y la predicción extra-muestral, obtenemos $y_{T+h}=y_T + hb_0$: la predicción es el último valor observado más $h$ veces la primera pendiente estimada.

La calidad de las predicciones es razonable, con un error porcentual medio del 6.7%, y se ha mejorado respecto del alisado exponencial simple.

```{r}
tail(librosf$model$states, n = 1)
```

En el objeto `librosf` la matriz `librosf$model$states` guarda todos los valores del nivel y de la pendiente obtenidos con las ecuaciones recursivas, incluidos los valores de arranque, así que es una matriz con $T+1$ filas. Puedes ver los valores de $l_{2018}$ y $b_{2018}$ en su última fila, que valen respectivamente `r formatC(librosf$model$states[27,], format = "f", digits = 2)`. Así, la predicción para $2019$ es $\widehat{y}_{2019}=l_{2018} + b_{2018}=$ `r formatC(librosf$model$states[27, 1], format = "f", digits = 2)` $+$ `r formatC(librosf$model$states[27, 2], format = "f", digits = 2)` $=$ `r formatC(sum(librosf$model$states[27,]), format = "f", digits = 2)`. Igualmente $\widehat{y}_{2020}=l_{2018} + 2\cdot b_{2018}=$ `r formatC(librosf$model$states[27,1] + 2* librosf$model$states[27,2], format = "f", digits = 2)`. Es decir, el incremento entre previsiones es contante e igual a $b_{2018}$ que, por ser $\beta$ prácticamente nulo, coincide con $b_0$.

La figura 2 muestra la serie Libros y las previsiones extra-muestrales que muestran una ligera tendencia creciente.
 
```{r}
autoplot(librosf,
         xlab = "",
         ylab = "Títulos",
         main = "Figura 2. Libros y predicción con alisado de Holt")
```

\
\

# Alisado exponencial con pendiente amortiguada (Ad, N)

Las previsiones con el método de Holt presentan siempre una pendiente constante. En previsiones a corto plazo esto no es un problema, pero para previsiones a largo plazo la experiencia indica que suele aparecer un sesgo de previsión. El alisado exponencial con pendiente amortiguada trata de corregir esta limitación. El mecanismo, propuesto por Gardner y McKenzie en 1985, es introducir un nuevo parámetro $0 \leq \phi \leq 1$ que _amortigua_ la tendencia hasta hacerla plana en el largo plazo.

\

## Formulas interactivas de sus componentes

Las __ecuaciones recursivas__ son
$$
\begin{aligned}
l_t & =\alpha y_t + (1-\alpha)(l_{t-1}+\phi b_{t-1}) \\
b_t & =\beta (l_t - l_{t-1}) + (1-\beta)\phi b_{t-1} 
\end{aligned}
$$
La ecuación de la __predicción intra-muestral__ a un periodo vista es
$$\widehat{y}_{t+1} = l_t + \phi b_t,$$
\noindent de forma que la ecuación de __predicción extra-muestral__ es 
$$\widehat{y}_{T+h}=l_T + (\phi + \phi^2 + \ldots + \phi^h) b_T.$$ 
 
Si $\phi = 1$ se tiene el alisado de Holt y si $\phi = 0$ se tiene el alisado simple. Para valores entre $0$ y $1$ en el corto plazo las predicciones tienen pendiente y en el largo plazo se hacen constantes e iguales a $l_T + \phi b_T/(1 - \phi)$.

\

## Ejemplo

Vamos a usar el método de alisado con amortiguamiento para predecir, una vez más, la serie Libros añadiendo a la función `holt` el argumento `damped = TRUE`. Por razones prácticas el rango de búsqueda de $\phi$ queda en el intervalo $[0.8, 0.98]$. En este caso, para ver el efecto del _amortiguamiento_ vamos a fijar el valor de $\phi$ a $0.9$ y vamos a pedir un horizonte temporal más largo.

```{r}
librosfd <- holt(libros, damped = TRUE, h = 15, phi = 0.9)
summary(librosfd)
```

La figura 3 muestra la serie Libros, su estimación (intra-muestral) y las predicciones a 15 años vista. Observa que la pendiente de las previsiones se _amortigua_ en el tiempo, de forma que al principio las previsiones crecen más rápidamente que en los últimos años.
 
```{r}
autoplot(librosfd,
         xlab = "",
         ylab = "Títulos",
         main = "Figura 3. Libros y predicción con alisado exponencial con amortiguamiento",
         PI = FALSE)
```

\
\

# Alisado de Holt-Winters aditivo (A, A) y multiplicativo (A, M)

El método de alisado exponencial de Holt-Winters es adecuado para una serie con tendencia y con estacionalidad. Existen dos versiones según que el esquema sea aditivo o multiplicativo.

\

## Alisado de Holt-Winters aditivo (A, A)

Las __ecuaciones recursivas__ de actualización son:
$$
\begin{aligned}
l_t & =\alpha (y_t - s_{t-m} ) + (1-\alpha)(l_{t-1}+b_{t-1}) \\
b_t & =\beta (l_t - l_{t-1}) + (1-\beta)b_{t-1} \\
s_t & =\gamma (y_t - l_{t-1} - b_{t-1}) + (1 - \gamma)s_{t-m}
\end{aligned}
$$
con $0 \leq \alpha, \beta, \gamma \leq 1$.

La ecuación de la __predicción intra-muestral__ a un periodo vista es 
$$\widehat{y}_{t+1}  = l_t + b_t + s_{t+1-m},$$
\noindent de forma que la ecuación de __predicción extra-muestral es__:
$$\widehat{y}_{T+h}=l_T + h b_T + s_{T+h - m(k+1)},$$ 
\noindent con $k = \lfloor(h-1)/m\rfloor$.

\

## Alisado de Holt-Winters multiplicativo (A, M)

Las __ecuaciones recursivas__ de actualización son:
$$
\begin{aligned}
l_t & =\alpha \frac{y_t}{s_{t-m}} + (1-\alpha)(l_{t-1}+b_{t-1}) \\
b_t & =\beta (l_t - l_{t-1}) + (1-\beta)b_{t-1} \\
s_t & =\gamma \frac{y_t}{l_{t-1} + b_{t-1}} + (1 - \gamma)s_{t-m}
\end{aligned}
$$

La ecuación de la __predicción intra-muestral__ a un periodo vista es 
$$\widehat{y}_{t+1}  = (l_t + b_t)s_{t+1-m},$$
de forma que la ecuación de __predicción extra-muestral es__:
$$\widehat{y}_{T+h}=(l_T + h b_T)s_{T+h - m(k+1)}.$$ 

\

## Ejemplo

Vamos a usar el método de Holt-Winters para predecir la serie Nacimientos, que presentaba un esquema multiplicativo. Para ello usaremos la función `hw` con el argumento `seasonal = "multiplicative"` (que sería `seasonal = "additive"` en caso de esquema aditivo). Vamos a considerar la serie Nacimientos desde enero de 2000 y pedir una previsión a dos años vista.

```{r}
nacimientos <- read.csv2("./series/nacimientos.csv", header = TRUE)
nacimientos <- ts(nacimientos[, 2],
                  start = c(1975, 1),
                  frequency = 12)

nacimientosb <- window(nacimientos, start = 2000)
nacimientosbf <- hw(nacimientosb, seasonal = "mult", h = 24)
summary(nacimientosbf)
```

Los valores óptimos de los parámetros son $\alpha=$ `r round(nacimientosbf$model$par[1],2)`, $\beta=$ `r round(nacimientosbf$model$par[2],2)` y $\gamma=$ `r round(nacimientosbf$model$par[3],2)`. Los valores tan bajos para $\beta$ y $\gamma$ indican que ambas, la pendiente y la estacionalidad, modifican su valor muy lentamente. Es decir, hay pendiente y hay efecto estacional, pero varían muy lentamente en el tiempo.

La calidad de las predicciones es notable, con un error porcentual medio del 1.8%. Recuerda que con el método ingenuo con estacionalidad el error era del 3.6%.

Los últimos valores de las componentes son 
```{r, eval = FALSE}
TT <- nrow(nacimientosbf$model$states)
nacimientosbf$model$states[TT,]
```

```{r, echo = FALSE}
TT <- nrow(nacimientosbf$model$states)
round(nacimientosbf$model$states[TT,], 3)
```

Como el último dato de la serie es diciembre de 2018, los valores del nivel $l$ y la pendiente $b$ mostrados corresponden a ese periodo. Sin embargo, la componente estacional tiene un orden muy peculiar: s1 es el valor estacional para diciembre (mes del último dato), s2 el de noviembre, s3 de octubre, hasta s11 que sería febrero y s12 que es enero. Podemos reproducir las predicciones para los primeros 12 meses de enero a febrero con (ojo, el etiquetado de la salida no es correcto):

```{r}
(nacimientosbf$model$states[TT, 1] + (1:12)*nacimientosbf$model$states[TT, 2]) * 
  nacimientosbf$model$states[TT, 14:3]
```

La figura 4 muestra la serie Nacimientos y las previsiones extra-muestrales.
 
```{r}
autoplot(nacimientosbf,
         xlab = "",
         ylab = "Nacimientos",
         main = "Figura 4. Nacimientos y predicción con alisado de Holt-Winters multiplicativo",
         PI = FALSE)
```

\
\

# Ejemplo con transformación logarítmica

Una alternativa a predecir la serie Nacimientos, que tiene esquema multiplicativo, es predecir la transformación logarítmica de la serie, que tendrá un esquema aditivo. Después, se aplica la transformación inversa y se obtienen las predicciones de la serie original.

Este proceso se puede realizar de forma sencilla y transparente con cualquiera de las funciones de alisado exponencial que hemos visto a partir de los argumentos `lambda` y `biasadj`.

* `lambda = 0` indica a la función de alisado que se ha de realizar la transformación logarítmica de la serie. Es un parámetro de la transformación Box-Cox que veremos en detalle en el tema 6. 
* `biasadj = TRUE` es necesario si tras una transformación de la serie original, queremos que las predicciones sean insesgadas. Sea $y_t$ la serie original y $z_t=log(y_t)$ su transformación logarítmica. Si obtenemos una predicción $\widehat{y}_t$ de la serie original, esta será insesgada $E[\widehat{y}_t]=y_t$. Ahora bien, si obtenemos una predicción $\widehat{z}_t$ de la serie transformada, podemos pensar que $e^{\widehat{z}_t}$ es una predicción insesgada de la serie original, pero resulta que $E[e^{\widehat{z}_t}] \neq y_t$. Es decir, la exponencial de la predicción de la serie transformada logarítmicamente no es insesgada. Si el argumento `biasadj` es fijado a FALSE, las predicciones se calcularan de forma directa deshaciendo la transformación y serán sesgadas; si es fijado a TRUE, las predicciones se calcularan por medio de una fórmula alternativa y serán insesgadas. En ambos casos las estimaciones son consistentes, así que para series largas no debería observarse mucha diferencia entre las dos alternativas.

Vamos a practicar el uso de estos argumentos con la serie Nacimientos. Como se va a predecir el logaritmo de la serie, se debe indicar a la función `hw` que use el modelo Holt-Winters aditivo.

```{r}
nacimientosbfl <- hw(nacimientosb, 
                     seasonal = "addit", 
                     h = 24, 
                     lambda = 0, 
                     biasadj = TRUE)
summary(nacimientosbfl)
```

Observa que en este caso la calidad de las predicciones (MAPE = 1.9%) es inferior a la obtenida con la serie sin transformar.

La figura 5 muestra la serie Nacimientos y las previsiones extra-muestrales obtenidas con y sin la transformación logarítmica.
 
```{r, echo = FALSE}
autoplot(nacimientosb,
         xlab = "",
         ylab = "Nacimientos",
         main = "Figura 5. Nacimientos y dos predicciones con alisado de Holt-Winters") + 
  autolayer(nacimientosbf, series = "Nacimientos", PI = FALSE) + 
  autolayer(nacimientosbfl, series = "Nacimientos (log)", PI = FALSE) + 
  guides(colour = guide_legend(title = "Predicción")) + 
  theme(legend.position=c(0.98,0.98), legend.justification=c(1,1)) 
```

La siguiente tabla muestra las predicciones de Nacimientos obtenidas sin transformar la serie, con transformación logarítmica y predicciones insesgadas (`biasadj = TRUE`), y con transformación logarítmica y predicciones sesgadas (`biasadj = FALSE`).

```{r, echo = FALSE}
nacimientosbfl2 <- hw(nacimientosb, seasonal = "addit", h=24, lambda = 0, biasadj = FALSE)
datos <- cbind(
  `Sin transformar` = nacimientosbf$mean,
  `log(Nac) insesgadas` = nacimientosbfl$mean,
  `log(Nac) sesgadas` = nacimientosbfl2$mean
  )
head(datos, 12)
```

Observa que las predicciones sesgadas son menores que las insesgadas. Esto siempre es así. La diferencia depende fundamentalmente de la varianza del error, _sigma_ al cuadrado en la salida de los métodos de alisado exponencial. Cuanto mayor es la varianza del error, mayores son las diferencias.

Por otro lado, las predicciones obtenidas sin y con la transformación logarítmica no guardan ninguna relación.

\
\

# Casos generales de alisado exponencial

En los epígrafes previos hemos visto cinco de los casos expuestos en la taxonomía del epígrafe 3, y las funciones de `R` asociadas. Veamos ahora como estimar cualquiera de los quince modelos que surgen según las diferentes posibilidades de la tendencia (N, A, Ad, M y Md) y de la estacionalidad (N, A y M). 

Recordemos que al añadir el error, aditivo o multiplicativo, estos quince modelos se convierten en treinta. Sin embargo, el tipo de error no influye en el cálculo de las previsiones, solo influye en el cálculo del intervalo de confianza de estas.

\

## La función `ets`

Podemos estimar cualquiera de los treinta modelos usando la función `ets` del paquete `forecast`. A diferencia de las funciones previas `ses`, `holt` y `hw`, la función `ets` solo estima los modelos, pero no produce predicciones. Para ello habrá que usar la función `forecast` sobre un modelo estimado con `ets`. Mira la ayuda para ver una explicación detallada de los argumentos de estas funciones.

* El tipo de modelo en `ets` se especifica con el argumento `model`, un código de tres letras indicando el tipo de Error, Tendencia y eStacionalidad (ETS). Por ejemplo, `model = "ANN"` indica un modelo con error aditivo, sin tendencia ni estacionalidad, es decir, el alisado exponencial simple; `model = "AAN"` indica un modelo con error aditivo, pendiente aditiva, pero sin estacionalidad, el alisado exponencial de Holt. El alisado exponencial de Holt-Winters multiplicativo sería `model = "AAM"`.
* Si se desea incluir amortiguamiento, hay que incluir el argumento `damped = TRUE`. 
* Por defecto `ets` no considera modelos con tendencia multiplicativa (últimas dos líneas de la taxonomía del epígrafe 4.3). Debes fijar el parámetro `allow.multiplicative.trend=TRUE` para contemplar esta opción.
* Además, se sigue disponiendo de los argumentos `lambda` y `biasadj`.

__Criterios de optimización__

Fijado un modelo, `ets` estima por defecto sus parámetros maximizando la función de verosimilitud. Esta búsqueda esta restringida a $0 < \beta < \alpha < 1$, $0 < \gamma < 1 - \alpha$ y $0.8 < \phi < 0.98$. Es decir, los tres primeros parámetros nunca pueden ser 0 o 1, y en la práctica sus valores límite son 0.0001 y 0.9999.

Puedes cambiar el criterio de optimización con el argumento `opt.crit`. Por defecto vale "lik", pero si lo fijas a `opt.crit = "mse"` se estiman los parámetros que minimizan el error cuadrático medio. Otra opción interesante es `opt.crit = "amse"` que minimiza la media de los error cuadráticos medios obtenido sobre las previsiones a _nmse_ periodos vista. En este caso usa el argumento `nmse` para fijar el valor numérico del horizonte temporal.

__Selección de modelos__

Lo más habitual es no saber cual es el mejor modelo, entendiendo como tal, el que mejor se ajusta a la serie temporal. De hecho, si lo que buscamos es predecir bien, más que entender la naturaleza del proceso generador de datos, el mejor modelo será el que mejor prediga.

Si en una de las tres letras del código del modelo se indica "Z", la función `ets` selecciona de entre los modelos posibles el que mejor se ajusta. Por ejemplo, `model = "AAZ"` indica un modelo con error y pendiente aditivos y dejaría a `ets` la búsqueda de la mejor opción para la estacionalidad (aditiva o multiplicativa). Si se especifica `model = "ZZZ` junto con `damped = NULL` (opciones por defecto) se dejaría a la función total libertad para buscar entre todos los modelos (excepto aquellos con pendiente multiplicativa). Si se desea restringir la búsqueda a modelos sin amortiguamiento basta indicar `damped = FALSE` y si se desea restringir la búsqueda solo a modelos aditivos se puede usar el argumento `additive.only = TRUE`.

Queda pendiente saber que criterio se usa para seleccionar el modelo cuando se ofrece esta opción. Esto se hace a partir de un criterio de información entre Akaike (aic), Akaike corregido para pequeñas muestras (aicc) y el Bayesiano (bic). Sus fórmulas son:
$$aic = -2log(L) + 2k$$
$$aicc = aic + \frac{k(k+1)}{T-k-1}$$
$$bic=aic + k(log(T) - 2)$$
\noindent donde $L$ es la verosimilitud, $T$ el número de datos y $k$ el de parámetros (incluidos los puntos iniciales de arranque y la varianza residual).

Por defecto se usa Akaike corregido para pequeñas muestras, pero el argumento `ic` permite cambiar de criterio. 

\

## Una reflexión sobre los métodos automáticos de selección de modelos

Con el comando `forecast(ets(nacimientos), h = 24)` obtenemos una predicción mensual a dos años vista del número de nacimientos en España. Así de simple, solo 31 caracteres. Todo esto gracias a que un algoritmo interno ha estimado los parámetros de múltiples modelos, elegido el mejor modelo de todos y lo ha usado para obtener las predicciones. Podemos afirmar que tenemos las mejores predicciones. Un momento, ¿podemos?

Parémonos a reflexionar sobre lo que hemos hecho o, más bien, lo que el algoritmo ha hecho y a contrastarlo con lo que nosotros queríamos. Por un lado, el algoritmo estima los parámetros de un menú fijo de modelos y para ello usa un criterio de optimización, que por defecto es maximizar la función de verosimilitud; cuando ya tiene estimados todos los modelos, elije el mejor usando el criterio de información de Akaike corregido para muestras pequeñas; y finalmente, nosotros medimos la capacidad predictiva del modelo seleccionado usando el error absoluto porcentual medio. Vaya, resulta que en los procesos de identificación y estimación del mejor modelo se usan dos criterios diferentes, que además no coinciden con nuestro criterio de calidad de las predicciones.

Si consideramos que la calidad de un modelo viene dada por el error absoluto porcentual medio en las predicciones intra-muestrales a un periodo vista (lo que hemos decidido llamar MAPE), ¿no deberíamos estimar los parámetros del modelo usando como criterio la minimización del MAPE?, ¿no deberíamos elegir entre varios modelos aquel que presenta un MAPE menor? De esta forma, en todos los pasos del proceso se usa el mismo criterio, que es, además, el criterio que hemos considerado adecuado para valorar la calidad de las predicciones.

Pero no es esto lo que hacemos. 

Nada nos garantiza que el modelo estimado y seleccionado por el algoritmo estime las mejores predicciones posibles. Y por _mejores_ quiero decir que de entre todos los posibles modelos del menú y todos los posibles valores de sus parámetros, el seleccionado sea que el minimiza nuestro criterio de calidad de las predicciones.

Ahora ya podemos dar respuesta a la pregunta del primer párrafo: no, no podemos afirmar que nuestras predicciones sean las mejores.

Alguien dirá que casi seguro entre las predicciones sub-óptimas obtenidas por el algoritmo con su extraña mezcla de criterios y las predicciones óptimas de verdad no habrá mucha diferencia. Total, que más da una función de verosimilitud que un criterio de información que una medida del error medio. Pero lo cierto es que no lo sabemos, no tenemos ni idea de la distancia que hay entre lo óptimo y lo sub-óptimo, y si el coste de equivocarme en las predicciones es alto, puede que incluso una pequeña diferencia sea relevante.

Esta reflexión realizada en el contexto de series temporales y para la función `ets` es aplicable a todos los casos donde dejamos que un algoritmo ya programado elija el mejor modelo, y se basa en el hecho de que rara vez los criterios de estimación y elección que usan los algoritmos coinciden con el concepto de calidad de ajuste que estamos interesados

A pesar de lo aquí expuesto, como es más cómodo (y rápido) tirar de rutinas ya programadas que escribir tu propio código, seguiremos trabajando con modelos sub-óptimos y obteniendo estimaciones sub-óptimas, pero diciendo que son las mejores.

\

## Residuo aditivo versus residuo multiplicativo

En los modelos de alisado estimados por la función `ets` la fórmula para el cálculo del residuo estimado depende de su naturaleza aditiva o multiplicativa.

Si el __residuo es aditivo__, entonces el modelo es $y_t = \widehat{y}_t + \widehat{\varepsilon}_t$ y el residuo se define de la forma usual 
$$\widehat{\varepsilon}_t = y_t - \widehat{y}_t.$$
Ahora bien, si el __residuo es multiplicativo__, entonces el modelo es $y_t = \widehat{y}_t \cdot (1 + \widehat{\varepsilon}_t)$, y no $y_t = \widehat{y}_t \cdot \widehat{\varepsilon}_t$, como se podría esperar. Por tanto, el residuo multiplicativo se define como 
$$\varepsilon_t = (y_t - \widehat{y}_t)/\widehat{y}_t.$$
De esta forma en ambos casos el residuo evoluciona alrededor del valor 0 y se le pueden imponer las hipótesis usuales de ruido blanco. 

La función `residual` permite extraer de un objeto `ets` el residuo del modelo. Si el modelo tiene residuo multiplicativo y se desea obtener el residuo aditivo, se debe usar con el argumento `type = "response"`.

\
\

# Ejemplos de aplicación

\

## Libros

### Identificación y estimación del mejor modelo {-}

Si estimamos el mejor modelo de alisado exponencial para la serie Libros sin ningún tipo de restricción, nos encontramos que la función `ets` elije el modelo "MNN", es decir un modelo sin pendiente.

```{r}
librosEts <- ets(libros)
summary(librosEts) 
```

El modelo estimado es ETS(M,N,N), un modelo de alisado exponencial simple con error multiplicativo, es decir, $y_{t+1} = l_t \cdot (1 + \varepsilon_{t+1})$. 

El valor de $\alpha$ técnicamente es 1, indicando que el nivel de la serie varia en el tiempo y que realmente estamos usando el método ingenuo I.

Respecto de la calidad del modelo, el valor de MAPE= $`r round(accuracy(librosEts)[5],1)`$% evidencia que estamos ante un modelo que ajusta razonablemente bien a los datos, y MASE= $`r round(accuracy(librosEts)[6],2)`$ indica que el modelo de alisado exponencial simple reduce en solo un $`r 100 - round(100*accuracy(librosEts)[6],0)`$% el error del método ingenuo I.


### Predicción {-}

Mediante la función `forecast` podemos predecir los casos de Libros. Por tratarse de un modelo de alisado exponencial simple, la predicción es constante en el tiempo (véase figura 6).

```{r}
librosEtsPre <- forecast(librosEts, h = 5)
librosEtsPre
autoplot(librosEtsPre,
         xlab = "",
         ylab = "Títulos",
         main = "Figura 6. Libros y predicción a 5 años vista")
```

### Análisis del error {-}

El error de un modelo de alisado _contiene_ la componente de __Intervención__ y el propio término de __Error__. Ver numérica o gráficamente el error permite identificar fácilmente la presencia de valores atípicos (intervención).

El modelo de alisado tiene error multiplicativo así que debemos usar el argumento `type = "response"` para obtener el error aditivo con `residuals`.

```{r}
error <- residuals(librosEts, type = "response")
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "Periodo",
         ylab = "Error",
         main = "Figura 7: Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, 
             colour = c("red", "blue", "blue", "red"), lty = 2) + 
  scale_x_continuous(breaks= seq(1993, 2019, 2)) 
```

La figura 7 muestra que aunque algún error supera las dos desviaciones típicas, ninguno puede ser considerado como claramente atípico.

### Validación: error extra-muestral a varios periodos vista {-}

Vamos a mejorar la estimación de la calidad de las predicciones obteniendo el MAPE para __previsiones extra-muestrales a varios periodos vista__. Para ello vamos a reservar, por ejemplo, las últimas 6 observaciones de la serie Libros y ajustar el modelo con las restantes. Después usaremos este modelo para calcular las predicciones a 6 periodos vista y compararlas con los valores reales de la serie Libros. 

Recuerda, este método para valorar la calidad de las predicciones usa la filosofía del método _training set/test set_: el periodo de datos usado en la estimación no se usa como periodo de datos para la validación. Sin embargo, tiene el problema de que el error obtenido es una mezcla de errores de predicción a corto, medio y largo plazo difícil de valorar. Además, los resultados dependen tremendamente del punto de corte temporal seleccionado.


```{r}
# Definimos las observaciones intra- y extra-muestrales
librosIntra <- subset(libros, end = length(libros) - 6)
librosExtra <- subset(libros, start = length(libros) - 5)

# Estimamos el modelo con todos los datos menos los 6 ultimos
librosIntraEts <- ets(librosIntra, model = "MNN", damped = FALSE)

# Predecimos los 6 años que hemos quitado de la serie y 
# vemos la calidad del ajuste.
librosExtraPre <- forecast(librosIntraEts, h = 6)
accuracy(librosExtraPre, librosExtra)
```

```{r, echo=FALSE}
error.muestral.1 <- round(accuracy(librosExtraPre, librosExtra)[1,5], 1)
error.extramuestral.n <- round(accuracy(librosExtraPre, librosExtra)[2,5],1)
```
Atendiendo al MAPE se tiene que el error de __previsión a un periodo vista__ en el __periodo intra-muestral__ de __1993 a 2012__ es del `r error.muestral.1`%; mientras que el error de __previsión a largo plazo__ en el __periodo extra-muestral__ de __2013 a 2018__ es del `r error.extramuestral.n`%. Ademas, para el periodo extra-muestral el error medio (ME) es negativo y muy elevado, un indicativo de que las previsiones están segadas. En resumen, la calidad del modelo se deteriora muy rápidamente en cuanto nos salimos de las condiciones óptimas. 

Un gráfico puede ayudar a entender este proceso de validación. En la figura 10:

* La línea de puntos vertical separa el periodo muestral (1993-2012) usado para estimar el modelo, del periodo extra-muestral (2013-2018) usado sólo para hacer las previsiones.
* La serie Libros aparece como una línea sólida en negro, desde 1993 hasta 2018.
* La previsión _intra_-muestral (a un periodo vista) de la serie Libros aparece como una línea azul.
* La línea en rojo es la previsión _extra_-muestral a largo plazo: $\hat{y}_{T+h}=l_T$, donde $T=2012$. Observa que todas las previsiones están por encima del valor real de la serie.
* Al lado de cada previsión se ha indicado el error estimado (MAPE).

Claramente estos resultados dependen del punto de corte seleccionado.

```{r,echo=FALSE}
autoplot(libros, series = "Libros",
         main="Figura 8. Libros, predicción intra- y extra-muestral",
         xlab="", 
         ylab="Títulos"
         ) +
  autolayer(fitted(librosIntraEts), series = "Libros (ajustada)") + 
  autolayer(librosExtraPre$mean, series = "Predicción") + 
  geom_vline(xintercept = 2012.5, lty = 2, col = "black") +
  scale_colour_manual(values=c("Libros"="black",
                               "Libros (ajustada)"="blue", 
                               "Predicción" = "red")) +
  guides(colour = guide_legend(title = "Series")) +
  annotate("text", x=1999, y=65000, label="7.2%", colour = "blue") +
  annotate("text", x=2016, y=72000, label="17.8%", colour = "red") +
  theme(legend.position=c(0.02,0.98), legend.justification=c(0,1)) 
```

__Nota:__ La presencia de tendencia, primero creciente y luego decreciente, en la serie Libros puede hacernos pensar que un modelo más adecuado para su ajuste y predicción sería ETS(M,A,N), forzando a que haya pendiente. De hecho, el error de estimación de este modelo es del 6%, frente al 7% para el modelo ETS(M,N,N). Sin embargo, el error de previsión extra-muestral a largo plazo para el modelo ETS(M,A,N) es del 31.5%, frente al 17.8% para el modelo ETS(M,N,N). De nuevo, incidir en que claramente estos resultados dependen del punto de corte seleccionado.

\

## Nacimientos

Veamos un segundo ejemplo con la serie Nacimientos (desde el año 2000).

### Identificación y estimación del mejor modelo {-}

Si damos total libertad al proceso de selección del mejor modelo, el óptimo tiene tendencia amortiguada con un parámetro $\phi = 0.98$, su máximo valor permitido. Este resultado aconseja repetir el proceso de selección del modelo restringido a aquellos sin amortiguamiento.

```{r}
nacimientosEts <- ets(nacimientosb, damped = FALSE)
summary(nacimientosEts) 
```

El modelo estimado es ETS(M,A,A), es decir, $y_{t+1} = (l_t + b_t +  s_{t+1-m}) \cdot (1+ \varepsilon_{t+1})$.

El bajo valor de $\beta$ y $\gamma$ indican que ambas, la pendiente y la estacionalidad, varían muy lentamente en el tiempo (véase la figura 9). 

```{r}
autoplot(nacimientosEts,
         xlab = "Periodo",
         main = "Figura 9. Componentes del modelo óptimo para Nacimientos")
```

Respecto de la calidad del modelo, el MAPE de `r round(accuracy(nacimientosEts)[5],1)`% indica que estamos ante un modelo que se ajusta muy bien a los datos; y el valor de MASE igual a `r round(summary(nacimientosEts)[6],2)` indica que este modelo reduce en un `r 100 - round(100*summary(nacimientosEts)[6],0)`% el error del método ingenuo con estacionalidad, el más sencillo posible, que ya utilizamos en el epígrafe 3 (aunque para la serie nacimientos completa).
      
Podemos ver los últimos valores estimados del nivel, la pendiente y la estacionalidad para interpretarlos.

```{r, eval = FALSE}
TT <- nrow(nacimientosEts$states)
nacimientosEts$states[TT,]
```

```{r, echo = FALSE}
TT <- nrow(nacimientosEts$states)
round(nacimientosEts$states[TT,],2)[1:7]
round(nacimientosEts$states[TT,],2)[8:14]
```

También podemos usarlos para predecir un año:
```{r, eval = FALSE}
nacimientosEts$states[TT, 1] + (1:12) * nacimientosEts$states[TT, 2] + 
  nacimientosEts$states[TT, 14:3]
```

```{r, echo = FALSE}
nacimientosEts$states[TT, 1] + (1:12) * nacimientosEts$states[TT, 2] + nacimientosEts$states[TT, 14:3]
```

Febrero es el mes con menor número de nacimientos: nacen `r abs(trunc(nacimientosEts$states[TT,13]))` bebés menos, respecto de la media anual. En octubre es cuando más niños nacen: `r trunc(nacimientosEts$states[TT,5])` más que la media anual.

Nuestra predicción para enero de 2019 es de `r as.integer((nacimientosEts$states[TT, 1] +  nacimientosEts$states[TT, 2]) + nacimientosEts$states[TT, 14])` bebés y para diciembre de 2019 de `r as.integer((nacimientosEts$states[TT, 1] + 12 * nacimientosEts$states[TT, 2]) + nacimientosEts$states[TT, 3])` bebés.

### Predicción {-}

Si pedimos los valores de predicción tenemos (sólo se muestran los primeros meses):
```{r, eval = FALSE}
nacimientosEtsPre <- forecast(nacimientosEts, h = 24, level = 95)
nacimientosEtsPre
```

```{r, echo = FALSE}
nacimientosEtsPre <- forecast(nacimientosEts, h = 24, level = 95)
forecast(nacimientosEts, h = 5, level = 95)
```

Gráficamente,
```{r}
autoplot(nacimientosEtsPre,
         xlab = "",
         ylab = "Bebés",
         main = "Figura 10. Nacimientos y predicción")
```

### Análisis del error {-}

El modelo de alisado tiene error multiplicativo así que debemos usar el argumento `type = "response"` para obtener el error aditivo con `residuals`.

```{r}
error <- residuals(nacimientosEts, type = "response")
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "Periodo",
         ylab = "Error",
         main = "Figura 11: Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, 
             colour = c("red", "blue", "blue", "red"), lty = 2) + 
  scale_x_continuous(breaks= seq(2000, 2019, 2)) 
```

El valor más atípico corresponde a enero de 2011, que supera las 4 desviaciones típicas. Abril de 2008 es otro candidato a intervención por casi alcanzar las 3 desviaciones típicas.

### Validación: error extra-muestral según horizonte temporal {-}

En este ejemplo calcularemos el error extra-muestral según el horizonte temporal de previsión, una metodología que ya hemos visto anteriormente.

```{r}  
k <- 120                 #Minimo numero de datos para estimar
h <- 12                  #Horizonte de las predicicones
TT <- length(nacimientosb)  #Longitud serie
s <- TT - k - h          #Total de estimaciones

mapeAlisado <- matrix(NA, s + 1, h)
for (i in 0:s) {
  train.set <- subset(nacimientosb, start = i + 1, end = i + k)
  test.set <-  subset(nacimientosb, start = i + k + 1, end = i + k + h)
  
  fit <- ets(train.set, model = "MAA", damped = FALSE)
  fcast<-forecast(fit, h = h)
  mapeAlisado[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set
}

errorAlisado <- colMeans(mapeAlisado)
errorAlisado

ggplot() +
  geom_line(aes(x = 1:12, y = errorAlisado)) +
  ggtitle("Figura 12. Error de predicción según horizonte temporal") +
  xlab("Horizonte temporal de predicción") +
  ylab("MAPE") +
  scale_x_continuous(breaks= 1:12)
```


La figura 12 muestra el error de previsión extra-muestral según el horizonte temporal. El error extra-muestral a un periodo vista es comparable al error intra-muestral (1.9% frente a 1.8%). Aunque el error de previsión aumenta conforme lo hace el horizonte temporal, siempre se mantiene muy bajo. Por ejemplo, en las previsiones a 12 meses vista el error es del 3.5%. 

\

## Otras alternativas para predecir Nacimientos

A la hora de predecir hay que ser un poco imaginativos, dedicarle tiempo y probar cosas. Por ejemplo, podríamos predecir los nacimientos a partir del ajuste de la transformación logarítmica. O podemos probar a cambiar el criterio de estimación de los parámetros o el de selección del modelo óptimo.

Yendo un poco más lejos, y dado que el número de nacimientos depende forzosamente del número de días del mes, podemos predecir los nacimientos medios por día (cociente entre los nacimientos de cada mes y el número de días del mes). Esta serie tendrá una componente estacional más suave, elimina el efecto de los meses de febrero bisiestos y tendrá, previsiblemente, un mejor ajuste.

También podemos mezclar varios de los enfoques previos o ser aún más imaginativos.

El siguiente código muestra el MAPE (para previsiones intra-muestrales a un periodo vista) para varias de estas opciones. Puedes deducir que se está haciendo en cada caso a partir del código. Sería más adecuado usar otro criterio de validación diferente, pero el objetivo de este epígrafe es recalcar que no hay que quedarse con lo inmediato (predecir Nacimientos con las opciones por defecto de las funciones), sino probar y probar.

```{r}
# Serie Nacimientos
accuracy(ets(nacimientos, 
             damped = TRUE))[5]
accuracy(ets(nacimientos, 
             damped = TRUE, 
             opt.crit = "mse"))[5]
accuracy(ets(nacimientos, 
             damped = TRUE, 
             allow.multiplicative.trend = TRUE))[5]
accuracy(ets(nacimientos, 
             damped = TRUE, 
             allow.multiplicative.trend = TRUE, 
             opt.crit = "mse"))[5]

# Transformación logarítmica
accuracy(ets(nacimientos, 
             lambda = 0, 
             damped = TRUE))[5]
accuracy(ets(nacimientos, 
             lambda = 0, 
             damped = TRUE, 
             opt.crit = "mse"))[5]

# Nacimientos por dia
accuracy(ets(nacimientos/monthdays(nacimientos), 
             damped = TRUE))[5]
accuracy(ets(nacimientos/monthdays(nacimientos), 
             damped = TRUE, 
             opt.crit = "mse"))[5]
accuracy(ets(nacimientos/monthdays(nacimientos), 
             damped = TRUE, 
             allow.multiplicative.trend = TRUE))[5]
accuracy(ets(nacimientos/monthdays(nacimientos), 
             damped = TRUE, 
             allow.multiplicative.trend = TRUE, 
             opt.crit = "mse"))[5]
```

La principal conclusión en este caso es que salirse de la estimación directa sobre la serie original no reduce el error significativamente. Sin embargo, cabe destacar que:

* El error de estimación de los nacimientos por día es menor, al tratarse de una serie con mejor comportamiento, aunque la ganancia no es mucha. (Véanse los cuatro últimos modelos respecto de los cuatro primeros)
* Usar la transformación logarítmica no mejora la capacidad predictiva del modelo. (Véanse los modelos 5 y 6 respecto de los modelos 1 y 2)
* Permitir una pendiente multiplicativa mejora la calidad del ajuste. (Modelos 1 vs. 3, 2 vs.4, 7 vs. 9 y 8 vs. 10)
* El mejor modelo estima los nacimientos por día, tiene pendiente multiplicativa amortiguada y estima los parámetros minimizando el error cuadrático medio ("mse"). Todo menos lo _directo_.

\
\

# Resumen de los comandos utilizados


|Función  |Paquete | Descripción                                           |
|:--------------|:-----------|:-----------------------------------------------------|
|`ses`          |forecast  |Estimación del modelo de alisado exponencial simple|
|`holt`         |forecast   |Estimación del modelo de alisado exponencial de Holt|
|`hw`           |forecast |Estimación del modelo de alisado exponencial de Holt-Winters|
|`ets`          |forecast  |Estimación de una amplia familia de métodos de alisado exponencial|
|`residuals`    |stats   |Obtiene el residuo de un modelo estimado|

\
\

# Referencias

* Brown, R. G. (1959). _Statistical forecasting for inventory control_. Ed. McGraw/Hill.

* Gardner, Jr, E. S. y McKenzie, E. (1985) _Forecasting trends in time series_, Management Science, 31(10), pp. 1237–1246. doi:10.1287/mnsc.31.10.1237

* Holt, C. E. (1957). _Forecasting seasonals and trends by exponentially weighted averages_ O.N.R. Memorandum No. 52. Carnegie Institute of Technology, Pittsburgh USA. doi:10.1016/j.ijforecast.2003.09.015

* Hyndman, R. J. y Khandakar, Y. (2008) _Automatic Time Series Forecasting: The forecast Package for R_. Journal of Statistical Software, 27(3), pp. 1-22. doi:10.18637/jss.v027.i03

* Hyndman, R. J., Koehler, A., B., Ord, J. K. y Snyder, R. D. (2008) _Forecasting with Exponential Smoothing: the State Space Approach_. Ed. Springer.

* Winters, P. R. (1960). _Forecasting sales by exponentially weighted moving averages_. Management Science, 6, pp. 324–342.  doi:10.1287/mnsc.6.3.324

\
\
\
\
