[
  {
    "objectID": "03-02-Tema2.html#definición",
    "href": "03-02-Tema2.html#definición",
    "title": "Series Temporales. Definición y componentes",
    "section": "1.1 Definición",
    "text": "1.1 Definición\nUna serie temporal es una variable medida secuencialmente en el tiempo a intervalos equiespaciados.\nLa representaremos por,\n\\[\\{y_t\\}_{t=1}^T=\\{y_1,y_2,\\ldots,y_T\\}.\\]\nLa serie aparece indexada por su fechado \\(t\\) y el subíndice \\(T\\) hará siempre referencia a la fecha del último dato.\nEl fechado varía en su frecuencia, que puede ser anual (baja frecuencia), trimestral, mensual, semanal, diario (alta frecuencia) o disponer casi de un continuo de datos."
  },
  {
    "objectID": "03-02-Tema2.html#proceso-generador-de-datos",
    "href": "03-02-Tema2.html#proceso-generador-de-datos",
    "title": "Series Temporales. Definición y componentes",
    "section": "1.2 Proceso generador de datos",
    "text": "1.2 Proceso generador de datos\nEl proceso generador de los datos de una serie temporal es en general desconocido, pero se puede aproximar por un modelo estadístico. Estos modelos se pueden clasificar en tres grandes familias según su naturaleza: deterministas, estocásticos y ambos.\nEn ocasiones las series temporales pueden ser modeladas de forma determinista ajustando los datos a funciones matemáticas: \\[y_t=f(t)+\\varepsilon_t.\\]\nEn los procesos estocásticos las observaciones cercanas en el tiempo tienden a estar (cor)relacionadas y se puede aprovechar esta dependencia para entender la serie y predecirla: \\[y_t=f(y_{t-1},y_{t-2},\\ldots)+\\varepsilon_t.\\]\nA veces, ambas situaciones se dan simultáneamente: \\[y_t=f(t,y_{t-1},y_{t-2},\\ldots)+\\varepsilon_t.\\]\nLa Figura 1 muestra un ejemplo gráfico de estos tres procesos generadores.\n\n\n\n\n\nFigura 1: Ejemplos de procesos generadores\n\n\n\n\nEn este curso se asumirá en todo momento que la serie temporal tiene una componente estocástica. Para series deterministas puedes usar los modelos de regresión que has visto en Previsión con Datos Transversales."
  },
  {
    "objectID": "03-02-Tema2.html#lectura-de-datos-y-representación-gráfica",
    "href": "03-02-Tema2.html#lectura-de-datos-y-representación-gráfica",
    "title": "Series Temporales. Definición y componentes",
    "section": "1.3 Lectura de datos y representación gráfica",
    "text": "1.3 Lectura de datos y representación gráfica\nAntes de continuar vamos a importar en R tres de las series que usaremos de ejemplo en este tema: número de títulos (libros y folletos) publicados anualmente en España, los nacimientos mensuales en España y la demanda eléctrica diaria (GWh) en España. Servirán de ejemplo para el análisis de series con diferente fechado, anual, mensual y diario, respectivamente.\n\nTítulos publicados en España\nLibros es una serie anual de 1993 a 2019 (fuente Instituto Nacional de Estadística). Los datos están disponibles en el fichero libros.csv. La primera columna tiene el año de la serie y la segunda contiene el número de títulos publicados. En la primera fila aparece el nombre de cada columna.\n\nlibros <- read.csv2(\"./series/libros.csv\", \n                    header = TRUE)\n\nlibros <- ts(libros[, 2], \n             start = 1993, \n             frequency  = 1)\n\nUsamos para leer los datos read.csv2, indicando que la primera línea tiene el nombre de las variables. Esta función asume que el separador decimal es la coma “,” y que el separador entre variables es el punto y coma “;”. Si el separador decimal es el punto “.” y el separador de variables es la coma “,”, debes usar read.csv. En cualquiera de estas funciones puedes modificar el separador decimal por medio del argumento dec; también puedes usar el argumento sep para indicar el carácter usado como separador de variables.\nLa función ts, de la librería stats, convierte un objeto (vector o matriz) a la clase serie temporal. En este caso seleccionamos solo la segunda columna, la que contiene el número de títulos publicados.\n\nCon start indicamos el fechado del primer dato.\nCon frequency indicamos la frecuencia, que en este caso es un dato por año.\n\nUsa help(ts) para obtener más información y str(libros) para ver qué contiene un objeto serie temporal.\n\n\n\n\n\n\nNombre del fichero y nombre de la serie\n\n\n\n\n\nPor comodidad (o pereza), muchas veces el nombre del fichero con los datos, el nombre del data.frame con su contenido en R y el nombre de la serie es el mismo. No tiene por que ser así, aunque en estos apuntes es lo más habitual.\nEn este caso, observa que primero creamos “libros”, una base de datos (data.frame) con el contenido del fichero “libros.csv” que tiene dos variables (fechado y valores de la serie). Luego creamos la serie temporal “libros” extrayendo de la base de datos la segunda columna (valores de la serie) y fechándola. Por tanto, estamos machacando la base de datos al poner a la serie el mismo nombre.\n\n\n\nPodemos dibujar la serie Libros con la función plot o mejor con autoplot. Esta última está en el paquete forecast.\nEn general, las funciones gráficas que vamos a usar pertenecen a la libraría forecast, pero en ocasiones las ampliaremos con funciones de la librería ggplot2. Te recomiendo cargar estas dos librerías desde el inicio. En casi todos los casos existe una versión de la función gráfica usada en la librería stats.\n\nlibrary(forecast)\nlibrary(ggplot2)\n\n\nautoplot(libros,\n         xlab = \"\",\n         ylab = \"Títulos\",\n         main = \"\")\n\n\n\n\nFigura 2: Títulos publicados (libros y folletos)\n\n\n\n\n\n\nNacimientos en España\nNacimientos es una serie mensual de enero de 1975 a diciembre de 2019 (fuente: Instituto Nacional de Estadística). Los datos están disponibles en el fichero nacimientos.csv. La primera columna tiene la fecha y la segunda la serie propiamente. En la primera fila aparece el nombre de cada columna. De nuevo seleccionamos solo la columna con los datos de nacimientos.\n\nnacimientos <- read.csv2(\"./series/nacimientos.csv\", \n                         header = TRUE)\n\nnacimientos <- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  frequency = 12)\n\n\n\n\n\n\n\n¿Sobrías responder a estas preguntas?\n\n\n\n¿Por qué se usa el argumento “header = TRUE”?\n¿Por qué la serie corresponde a la segunda columna de la base de datos “nacimientos”?\n\n\nEn este caso:\n\nCon start indicamos que el primer dato es enero de 1975. También sería correcto start = 1975. Si el primer dato fuera, por ejemplo, marzo de 1975, deberíamos poner start = c(1975, 3) o start = 1975 + 2/12.\nCon frequency indicamos que se tienen 12 datos (meses) por año. Si la serie fuera trimestral pondríamos frequency = 4.\n\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\")\n\n\n\n\nFigura 3: Nacimientos mensuales\n\n\n\n\n\n\nDemanda eléctrica\nDemanda eléctrica es una serie diaria desde el 1 enero de 2021 hasta el 31 diciembre de 2021 (fuente: Red Electrica de España). Los datos están disponibles en el fichero Consumo electrico.csv. La primera columna tiene la fecha y la segunda, la seleccionada para el fechado, la serie propiamente. En la primera fila aparece el nombre de cada columna.\n\nelectricidad <- read.csv2(\"./series/Consumo electrico.csv\",\n                          header = TRUE)\n\nelectricidad <- ts(electricidad[, 2],\n                   start = c(1, 5),\n                   frequency = 7)\n\nEn este caso:\n\nCon start indicamos que el primer dato es un viernes (día 5 de la semana) de la primera semana. Como ves no hay referencia al año. También sería correcto start = 1 + 4/7.\nCon frequency indicamos que se tienen 7 datos (días) por semana. Si la serie fuera de lunes a viernes pondríamos frequency = 5.\n\n\nautoplot(electricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\nFigura 4: Consumo diario de electricidad (2021)"
  },
  {
    "objectID": "03-02-Tema2.html#funciones-útiles-para-objetos-de-clase-ts",
    "href": "03-02-Tema2.html#funciones-útiles-para-objetos-de-clase-ts",
    "title": "Series Temporales. Definición y componentes",
    "section": "1.4 Funciones útiles para objetos de clase ts",
    "text": "1.4 Funciones útiles para objetos de clase ts\nOtras funciones relacionadas con los objetos de clase serie temporal que pueden ser útiles son:\n\nstart da el fechado del primer dato, y end da el fechado del último dato.\n\n\nstart(nacimientos); end(nacimientos)\n\n[1] 1975    1\n\n\n[1] 2019   12\n\nstart(electricidad); end(electricidad)\n\n[1] 1 5\n\n\n[1] 53  5\n\n\n\nfrequency da la frecuencia de los datos.\n\n\nfrequency(nacimientos)\n\n[1] 12\n\nfrequency(electricidad)\n\n[1] 7\n\n\n\ntime crea un vector con el fechado de una serie. Observa como guarda internamente R el fechado de una serie temporal.\n\n\nhead(time(nacimientos), n = 48)  #Mostramos sólo los 4 primeros años\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1975 1975.000 1975.083 1975.167 1975.250 1975.333 1975.417 1975.500 1975.583\n1976 1976.000 1976.083 1976.167 1976.250 1976.333 1976.417 1976.500 1976.583\n1977 1977.000 1977.083 1977.167 1977.250 1977.333 1977.417 1977.500 1977.583\n1978 1978.000 1978.083 1978.167 1978.250 1978.333 1978.417 1978.500 1978.583\n          Sep      Oct      Nov      Dec\n1975 1975.667 1975.750 1975.833 1975.917\n1976 1976.667 1976.750 1976.833 1976.917\n1977 1977.667 1977.750 1977.833 1977.917\n1978 1978.667 1978.750 1978.833 1978.917\n\nhead(time(electricidad), n = 28)  #Mostramos sólo las 4 primeras semanas\n\nTime Series:\nStart = c(1, 5) \nEnd = c(5, 4) \nFrequency = 7 \n [1] 1.571429 1.714286 1.857143 2.000000 2.142857 2.285714 2.428571 2.571429\n [9] 2.714286 2.857143 3.000000 3.142857 3.285714 3.428571 3.571429 3.714286\n[17] 3.857143 4.000000 4.142857 4.285714 4.428571 4.571429 4.714286 4.857143\n[25] 5.000000 5.142857 5.285714 5.428571\n\n\n\n\n\n\n\n\nFechado de la serie\n\n\n\n¿Tienes claro cómo se guarda internamente el fechado de una serie? ¿Qué indican los valores decimales?\n\n\n\ncycle, crea un vector con la posición en el ciclo de cada observación. Para una serie mensual sus valores van de 1 a 12, para una serie diaria sus valores irían 1 a 7.\n\n\nhead(cycle(nacimientos), n = 48) #Mostramos sólo los 4 primeros años\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1975   1   2   3   4   5   6   7   8   9  10  11  12\n1976   1   2   3   4   5   6   7   8   9  10  11  12\n1977   1   2   3   4   5   6   7   8   9  10  11  12\n1978   1   2   3   4   5   6   7   8   9  10  11  12\n\nhead(cycle(electricidad), n = 28) #Mostramos sólo las 4 primeras semanas\n\nTime Series:\nStart = c(1, 5) \nEnd = c(5, 4) \nFrequency = 7 \n [1] 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4"
  },
  {
    "objectID": "03-02-Tema2.html#tendencia-t_t",
    "href": "03-02-Tema2.html#tendencia-t_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.1 Tendencia, \\(T_t\\)",
    "text": "2.1 Tendencia, \\(T_t\\)\nDefinición: la tendencia de una serie es su comportamiento a largo plazo. Describe los cambios sistemáticos de la serie temporal que no aparentan ser periódicos.\nRespecto a la dirección del movimiento la tendencia puede ser:\n\nCreciente: a largo plazo la serie aumenta su valor\nDecreciente: a largo plazo la serie disminuye su valor\nEstacionaria: a largo plazo la serie mantiene su valor\n\nRespecto del proceso generador de la tendencia, puede ser:\n\nDeterminista: \\(T_t = f(t)\\)\nEstocástica: \\(T_t = f(T_{t-1}, T_{t-2},\\ldots)\\)\nAmbas: \\(T_t = f(t,T_{t-1}, T_{t-2},\\ldots)\\)\n\nEn la Figura 5 se muestran ejemplos de series temporales según dirección del movimiento y pendiente de la tendencia.\n\n\n\n\n\n\n\n\n\n(a) Estacionaria\n\n\n\n\n\n\n\n\n\n(b) Creciente (lineal)\n\n\n\n\n\n\n\n(c) Creciente (exponencial)\n\n\n\n\n\n\n\n(d) Creciente (logarítmica)\n\n\n\n\n\n\n\n\n\n(e) Decreciente (lineal)\n\n\n\n\n\n\n\n(f) Dereciente (exponencial)\n\n\n\n\n\n\n\n(g) Decreciente (logarítmica)\n\n\n\n\nFigura 5: Ejemplos de tendencia\n\n\n\n\nSi la serie temporal es suficientemente larga es posible observar cambios en la dirección del movimiento de la tendencia que definen los ciclos."
  },
  {
    "objectID": "03-02-Tema2.html#ciclo-c_t",
    "href": "03-02-Tema2.html#ciclo-c_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.2 Ciclo, \\(C_t\\)",
    "text": "2.2 Ciclo, \\(C_t\\)\nDefinición: Son patrones sin periodicidad fija que abarcan varios años.\nPor ejemplo, los ciclos económicos, los cambios climáticos asociados al fenómeno El Niño, o las manchas solares.\n\n\n\nFigura 6: Ciclos solares (imagen tomada de Courtillot, Lopes, and Mouël (2021))\n\n\n\n\nLa serie de Nacimientos es lo suficientemente larga como para observarse un ciclo completo, que queda identificado por dos cambios de tendencia consecutivos de signo opuesto (véase Figura 7):\n\nA finales de la década de los 90 la tendencia decreciente en los nacimientos pasa a creciente por la llegada de inmigrantes con una mayor tasa de natalidad.\nA finales de la primera década del 2000 la tendencia creciente pasa a decreciente porque la Gran Recesión provoca el regreso a sus países de origen de muchos de estos inmigrantes.\n\nDe esta forma, observamos un ciclo completo desde 1975 hasta poco antes de 2010 (periodo entre dos cambios de tendencia), y el inicio del siguiente ciclo, en el que aun estamos.\n\n\n\n\n\nFigura 7: Ciclos en la serie Nacimientos"
  },
  {
    "objectID": "03-02-Tema2.html#estacionalidad-s_t",
    "href": "03-02-Tema2.html#estacionalidad-s_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.3 Estacionalidad, \\(S_t\\)",
    "text": "2.3 Estacionalidad, \\(S_t\\)\nDefinición: Son patrones repetitivos de periodicidad fija e inferior al año.\nEl orden de la periodicidad lo denominaremos \\(m\\), por tanto el patrón estacional se repite cada \\(m\\) periodos. Lógicamente, \\(m\\) toma el valor 12 para datos mensuales, el valor 4 para datos trimestrales, 7 para datos diarios de lunes a domingo, etc.\nLa componente estacional surge por factores climatológicos, institucionales o sociales.\nEn ocasiones no es fácil determinar la existencia de estacionalidad o su orden. En este caso, se puede usar el análisis espectral, que no veremos en este curso, para analizar esta componente. La librería forecast dispone de la función findfrequency que devuelve la frecuencia dominante de una serie usando el análisis espectral.\nLa serie Nacimientos tiene una estacionalidad de orden 12, causada principalmente por el número de días del mes. Los valles en la Figura 8 corresponden a febrero, que por tener 28 días (o 29 en años bisiestos) presenta menor número de nacimientos.\n\n\n\n\n\nFigura 8: Nacimientos\n\n\n\n\n\n\nLa Figura 9 muestra la demanda eléctrica diariamente para las cuatro semanas de febrero de 2021, desde el lunes 1 hasta el domingo 28 (semanas 6 a 9 del año). Tiene, por tanto, una estacionalidad de orden 7. En el eje OX aparecen los lunes de cada semana que permiten identificar el domingo como el día de menor demanda, seguido del sábado.\n\n\n\n\n\nFigura 9: Demanda diaria de electricidad\n\n\n\n\n\n\nSi el fechado de la serie es de muy alta frecuencia, puede ocurrir que se superponga más de una componente estacional. La Figura 10 muestra la serie corresponde a la demanda eléctrica (GW) recogida cada hora durante el mes de febrero de 2021. El eje OX señala el consumo la primera hora de cada día del mes (desde media noche hasta la una de la madrugada). Se aprecia una estacionalidad diaria de orden \\(24\\), otra semanal de orden \\(7 \\times 24\\) y si mostráramos varios años de consumo, también se observaría otra mensual.\n\n\n\n\n\nFigura 10: Estacionalidad múltiple para el consumo de electricidad por hora\n\n\n\n\n\n\n\n\n\n\nSeries con más de una componente estacional\n\n\n\nEn este curso no analizaremos series con más de una componente estacional. Si quieres tener una primera aproximación de como se hace, puedes ir a la píldora Múltiples componentes estacionales."
  },
  {
    "objectID": "03-02-Tema2.html#intervención-i_t",
    "href": "03-02-Tema2.html#intervención-i_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.4 Intervención, \\(I_t\\)",
    "text": "2.4 Intervención, \\(I_t\\)\nDefinición: Es un factor sistemático no periódico, o irregular, que vendría determinado por fenómenos ocasionales que provocan observaciones anómalas y valores atípicos en la serie temporal.\nPor su relación con fechas concretas, podemos distinguir dos tipos:\n\nEfectos calendario: festivos en series diarias; Semana Santa, días laborales y febrero bisiesto en series mensuales.\nOtros efectos no sujetos a calendario: catástrofes, huelgas, caída del sistema eléctrico o de los servidores de una red social, etc.\n\nEn la serie mensual Nacimientos, los meses de febrero bisiestos (puntos rojos) presentan un número de nacimientos mayor que los meses de febrero no bisiestos. Para algunos años este hecho es mas claro (véase Figura 11 ).\n\n\n\n\n\nFigura 11: Efecto de los febreros bisiestos en Nacimientos\n\n\n\n\nEn la serie diaria Electricidad cuando un día entre semana es festivo, el consumo se reduce notablemente apareciendo un efecto calendario. Si se trata de un día aislado, el efecto es muy fácil de identificar y analizar. Un ejemplo, observable en la Figura 12, es el lunes 1 de noviembre de 2021, día de Todos los Santos, identificado con un punto rojo en la figura. Se aprecia que el consumo fue muy inferior al observado el lunes precedente y posterior (puntos verdes). Si los festivos abarcan varios días, el efecto sigue siendo perfectamente identificable, pero es más complejo de analizar. Un ejemplo es la última semana de diciembre donde el consumo es muy inferior al observado en las semanas previas debido, por un lado, al efecto del día festivo de Navidad (punto azul) y, por otro lado, al periodo semi-vacacional que estas festividades supone en España.\n\n\n\n\n\nFigura 12: Efectos calendario en Electricidad\n\n\n\n\nPor su naturaleza, podemos distinguir tres tipos básicos de intervención (aunque hay más):\n\nPulso (Additive Outlier, AO)\n\nEn un periodo aislado la serie toma un valor anómalo. Por ejemplo, un día entre semana es festivo y la demanda eléctrica es inferior a la usual. Véase Figura 13, panel derecho.\n\nCambio transitorio (Transitory Change, TC)\n\nEn un periodo un shock genera un valor anómalo en la serie y el efecto del shock va desapareciendo poco a poco. Por ejemplo, las redes sociales ponen de moda un producto que temporalmente aumenta sus ventas, pero conforme pasa el tiempo los consumidores se olvidan del producto y sus ventas vuelven poco a poco a su nivel natural. Véase Figura 13, panel central.\n\nCambio permanente (Level Shift, LS)\n\nEn un periodo la serie cambia de nivel y permanece de forma permanente en este nuevo nivel. Por ejemplo, enfrente de un supermercado abre la competencia, de forma que sus ventas descienden bruscamente de forma permanente. Véase Figura 13, panel izquierdo.\n\n\n\n\n\n\n\nFigura 13: Tipos de intervención"
  },
  {
    "objectID": "03-02-Tema2.html#residuo-r_t",
    "href": "03-02-Tema2.html#residuo-r_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.5 Residuo, \\(R_t\\)",
    "text": "2.5 Residuo, \\(R_t\\)\nDefinición: No presenta un comportamiento sistemático a corto, medio o largo plazo por lo que no se puede predecir de modo alguno. Es la parte de la serie que se debe a puro azar.\nAunque inicialmente no se hará ningún supuesto sobre el residuo, se espera que sea ruido blanco (media cero, incorrelado y homocedástico), es decir \\(R_t \\sim iid(0, \\sigma^2\\))."
  },
  {
    "objectID": "03-02-Tema2.html#esquema-aditivo-y-multiplicativo",
    "href": "03-02-Tema2.html#esquema-aditivo-y-multiplicativo",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.6 Esquema aditivo y multiplicativo",
    "text": "2.6 Esquema aditivo y multiplicativo\nUna serie temporal siempre tiene tendencia y residuo. La presencia de estacionalidad, ciclo e intervención depende de la naturaleza de la serie. Por ejemplo, una serie anual no tendrá nunca estacionalidad y en una serie corta no se podrá observar el ciclo.\nLas componentes de una serie temporal se pueden combinar de múltiples formas.\nEn el esquema aditivo cada componente suma su efecto sobre las demás, \\(y_t = T_t + S_t + C_t + I_t + R_t\\). La demanda diaria de electricidad es un ejemplo de este tipo de esquema (Figura 14). El gráfico superior muestra la serie en el tiempo e identificamos el esquema aditivo porque la amplitud estacional (para cada semana la diferencia entre el día de más consumo y el de menos consumo) se mantiene constante en el tiempo. En el gráfico inferior cada punto corresponde a una semana, la coordenada X es el consumo de esa semana y la coordenada Y la desviación típica del consumo para los días de esa semana. Este segundo gráfico revela el esquema aditivo de la serie porque no se observa un patrón creciente en los puntos: más consumo no implica una mayor desviación típica.\n\n\n\n\n\n\n\n(a) Consumo electrico diario\n\n\n\n\n\n\n\n\n\n(b) Gráfica media-desviación típica semanal\n\n\n\n\nFigura 14: Ejemplo de esquema aditivo\n\n\nEn el esquema multiplicativo cada componente supone un incremento porcentual respecto de las demás, \\(y_t = T_t \\cdot S_t \\cdot C_t \\cdot I_t \\cdot R_t\\). La serie Nacimientos es un ejemplo de esquema multiplicativo: en el gráfico superior de la Figura 15, que muestra la serie, se observa que según decrece el número de nacimientos, también decrece la amplitud estacional. Además, en el gráfico inferior cada punto corresponde a un año, en el eje X los nacimientos anuales y en el eje Y la desviación típica de los nacimientos mensuales. En este gráfico se observa un patrón creciente en los puntos, revelando el esquema multiplicativo de la serie.\n\n\n\n\n\n\n\n(a) Nacimientos mensuales\n\n\n\n\n\n\n\n\n\n(b) Gráfica media-desviación anual\n\n\n\n\nFigura 15: Ejemplo de esquema multiplicativo\n\n\nSi una serie presenta un esquema multiplicativo, su logaritmo lo presentará aditivo. A lo largo del curso se verán otras razones por las que puede ser aconsejable analizar el logaritmo de una serie temporal.\nEn principio, cualquier combinación entre las componentes es posible (véase Tema 5):\n\n\\(y_t = (T_t + C_t) \\cdot S_t + I_t + R_t\\)\n\\(y_t = (T_t + S_t + C_t + I_t)R_t\\)\n\\(y_t = T_t \\cdot S_t \\cdot C_t \\cdot I_t + R_t\\)\n…"
  },
  {
    "objectID": "03-02-Tema2.html#idea-general",
    "href": "03-02-Tema2.html#idea-general",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.1 Idea general",
    "text": "3.1 Idea general\nPodemos manipular una serie temporal con diferentes fines:\n\nextraer la tendencia (eliminando la estacionalidad), por ejemplo, pasando de una serie mensual a una anual, o de una serie diaria a una semanal.\nextraer la estacionalidad (eliminando la tendencia) de forma sencilla, aunque no muy precisa.\nrecortar una serie para obtener una submuestra.\nextraer una subserie correspondiente a un único periodo estacional. Por ejemplo, los nacimientos en febrero o el consumo de electricidad de los domingos.\n\nEstas operaciones nos permitirán mejorar nuestra capacidad descriptiva de la serie, identificar mejor el tipo de esquema entre las componentes, facilitar la estimación del proceso generador o ampliar las herramientas de análisis y predicción de una serie temporal."
  },
  {
    "objectID": "03-02-Tema2.html#extracción-de-la-tendencia",
    "href": "03-02-Tema2.html#extracción-de-la-tendencia",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.2 Extracción de la tendencia",
    "text": "3.2 Extracción de la tendencia\nSi tenemos una serie con estacionalidad y agregamos la serie –obteniendo un dato por año, si la serie es mensual, o un dato por semana, si es diaria– obtenemos una nueva serie sin estacionalidad, solo con tendencia.\nDependiendo de la naturaleza de la serie, convendrá agregar sumando los datos (consumo eléctrico, títulos publicados, viajeros transportados, nacimientos) o sacando la media (temperatura, número de parados, ocupación hotelera).\nVeamos como extraer la tendencia de la serie Nacimientos usando la función aggregate con el argumento FUN = sum.\n\nnacimientosAnual <- aggregate(nacimientos, FUN = sum)\nautoplot(nacimientosAnual/1000,\n         xlab = \"\",\n         ylab = \"Nacimientos (miles)\",\n         main = \"\")\n\n\n\n\nFigura 16: Nacimientos por año\n\n\n\n\nAhora para la demanda de electricidad:\n\nelectricidadSemanal <- aggregate(electricidad, FUN = sum)\nautoplot(electricidadSemanal,\n       xlab = \"\",\n       ylab = \"GWh\",\n       main = \"\")\n\n\n\n\nFigura 17: Consumo de electricidad por semana\n\n\n\n\nLa función aggregate aplicada a una serie temporal agrega los datos de cada periodo estacional completo aplicando la función especificada en FUN.\n\nUna serie trimestral o mensual la transforma en anual, una serie diaria en semanal.\nLa función a usar dependerá de la naturaleza de los datos y del objetivo perseguido (FUN=sum, FUN=mean, FUN=sd…)\nEsta función tiene un uso más amplio en R. Usa la función help para aprenderlo.\n\n\n\n\n\n\n\nAtención\n\n\n\nLa agregación no tiene por que empezar en el primera componente estacional. Por ejemplo, la serie Demanda eléctrica se inicia el viernes 1 de enero, así que el primer dato de la serie agregada irá del viernes 1 de enero al jueves 7 de enero, un periodo estacional completo; el segundo dato de la serie agregada ira del viernes 8 al jueves 14 de enero; y así sucesivamente. El último dato de la serie agregada será del viernes 24 al jueves 30 de diciembre. El 31 de diciembre no formará parte de la serie agregada."
  },
  {
    "objectID": "03-02-Tema2.html#extracción-de-la-estacionalidad",
    "href": "03-02-Tema2.html#extracción-de-la-estacionalidad",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.3 Extracción de la estacionalidad",
    "text": "3.3 Extracción de la estacionalidad\nTenemos varias alternativas gráficas y numéricas para analizar la estacionalidad de una serie. Veamos un ejemplo para la serie Nacimientos.\nPodemos hacer una gráfico de la serie contra cada periodo estacional. Este gráfico permite identificar el efecto de la estacionalidad en la serie y su evolución en el tiempo. Existen varias opciones para este tipo de gráficos, veremos dos de ellas: gráfico de subseries y gráfico de líneas. Para facilitar la interpretación vamos a trabajar con la serie desde el año 2000 que denominaremos nacimientosb.\n\nnacimientosb <- window(nacimientos, start = 2000)\n\nVeamos primero un ejemplo de gráfico de subseries (Figura 18) que muestra para cada periodo estacional la subserie de valores de ese periodo y el valor medio de la subserie. Para las series con tendencia y esquema multiplicativo, el valor medio de las subseries (líneas horizontales) puede llevarnos a una interpretación incorrecta de la estacionalidad. (La función monthplot de stats realiza un gráfico similar.)\n\nggsubseriesplot(nacimientosb) +\n  ylab(\"Nacimientos\") +\n  xlab(\"\") +\n  ggtitle(\"\")\n\n\n\n\nFigura 18: Gráfico estacional de subseries\n\n\n\n\nLa Figura 19 muestra el gráfico de líneas. Si se incluye el argumento polar=TRUE, se obtiene una versión tipo tela de araña de este gráfico.\n\nggseasonplot(nacimientosb, \n             year.labels=TRUE, \n             xlab = \"\",\n             ylab = \"Nacimientos\",\n             main = \"\")\n\n\n\n\nFigura 19: Gráfico estacional de lineas\n\n\n\n\nLas gráficas ayudan a describir y entender un poco mejor la componente estacional. Sin embargo, si deseamos estimar dicha componente, debemos proceder de otra forma.\nLa siguiente sintaxis usa la función tapply para estimar numéricamente la componente estacional bajo un esquema multiplicativo. Básicamente, calcula para cada mes (argumento cycle(nacimientosb)) la media (FUN = mean) del cociente nacimientosb / mean(nacimientosb).\n\ncomponenteEstacional <- tapply(nacimientosb/mean(nacimientosb), \n                               cycle(nacimientosb), \n                               FUN = mean)\nround(componenteEstacional, 2)\n\n   1    2    3    4    5    6    7    8    9   10   11   12 \n1.00 0.91 1.00 0.97 1.01 0.97 1.03 1.02 1.04 1.05 1.00 1.00 \n\n\nLos valores de la tabla previa indican que, por ejemplo, en febrero nacen un 9% menos de bebés respecto de la media anual y que en octubre nacen un 5% más que la media anual.\nLos cambios necesarios para estimar la componente estacional bajo un esquema aditivo son mínimos. Veámoslo para la serie Demanda eléctrica.\n\ncomponenteEstacional <- tapply(electricidad - mean(electricidad), \n                               cycle(electricidad), \n                               FUN = mean)\nround(componenteEstacional, 2)\n\n     1      2      3      4      5      6      7 \n 12.58  30.28  30.32  32.09  21.34 -42.59 -84.43 \n\n\nEn domingo la demanda de electricidad cae en 84 GWh respecto de la media semanal. De lunes a viernes la demanda eléctrica es mayor que la media semanal."
  },
  {
    "objectID": "03-02-Tema2.html#extracción-de-una-subserie",
    "href": "03-02-Tema2.html#extracción-de-una-subserie",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.4 Extracción de una subserie",
    "text": "3.4 Extracción de una subserie\nR proporciona varias funciones que permiten extraer una submuestra de la serie original. Podemos:\n\nseleccionar una submuestra especificando los puntos temporales de inicio y fin.\nseleccionar una submuestra seleccionando un periodo estacional determinado.\nquitar fácilmente un conjunto de datos usando índices.\n\nVeamos algunos ejemplos de extracción con la serie Nacimientos y las funciones window y subset:\nFunción window\n\nwindow(nacimientos, start = c(2000, 1), end = c(2009, 12)) selecciona de la serie original los datos desde enero de 2000 a diciembre de 2009.\nwindow(nacimientos, start = c(2010, 3)) selecciona de la serie original los datos desde marzo de 2010 hasta el último dato (diciembre de 2019).\nwindow(nacimientos, end = c(1999, 12)) selecciona de la serie original los datos desde el primero (enero de 1975) hasta diciembre de 1999.\nwindow(nacimientos, start = c(2000, 3), freq = TRUE) selecciona de la serie original solo los meses de marzo desde 2000.\n\nFunción subset\n\nsubset(nacimientos, start = 10, end = 34) selecciona de la serie las observaciones que van desde la 10 a la 34, ambas inclusive.\nsubset(nacimientos, start = 121) selecciona de la serie las observaciones que van desde la 121 hasta la última.\nsubset(nacimientos, start = length(nacimientos) - 47) selecciona de la serie los últimos 4 años (2016 a 2019).\nsubset(nacimientos, end = length(nacimientos) - 48) selecciona de la serie todo menos los últimos 4 años. Es decir, el último dato es diciembre de 2015.\nsubset(nacimientos, season  = 5) selecciona de la serie todos los meses de mayo.\n\n\n\n\n\n\n\nwindow y subset\n\n\n\nDurante el curso haremos un uso contante de la funciones window y subset. Practícalas para familiarizarte con su uso.\n\n\nAdemás, puedes usar las funciones head y tail para extraer las primeras o las últimas observaciones."
  },
  {
    "objectID": "03-02-Tema2.html#concepto",
    "href": "03-02-Tema2.html#concepto",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.1 Concepto",
    "text": "4.1 Concepto\nLos métodos que hemos visto para la descripción de la tendencia y la componente estacional son muy sencillos, pero no son ni rigurosos ni precisos. Veamos métodos más adecuados para extraer de una serie sus componentes: tendencia-ciclo, estacionalidad, e intervención-residuo.\nSi la serie es demasiado corta para poder extraer el ciclo, entonces el ciclo queda recogido dentro de la tendencia. Por otro lado, las técnicas de identificación de la intervención son complejas por lo que esta componente queda incorporada al residuo. Por tanto, asumiremos que una serie tiene sólo Tendencia, Estacionalidad y Residuo:\n\nEsquema aditivo \\(y_t = T_t + C_t + S_t + I_t + R_t = (T_t + C_t) + S_t +(I_t + R_t) = T'_t + S_t + R'_t\\)\nEsquema multiplicativo \\(y_t = T_t \\cdot C_t \\cdot S_t \\cdot I_t \\cdot R_t = (T_t \\cdot C_t) \\cdot S_t \\cdot (I_t \\cdot R_t) = T'_t \\cdot S_t \\cdot R'_t\\)\n\nVeremos a continuación como extraer estas tres componentes a partir de una serie original. Este proceso se denomina descomposición.\nHay múltiples formas de realizar una descomposición. Aquí veremos dos de ellas, la más sencilla, basada en el concepto de medias móviles (decompose), y otra más versátil y compleja a partir de regresiones locales ponderadas (stl).\nAdemás, R proporciona (a través de paquetes específicos) el método de descomposición que utiliza el US Census Bureau and Statistics Canada, denominado X11, y el método que utiliza el Banco de España, denominado SEATS (Seasonal Extraction in ARIMA Time Series), aunque estos métodos solo son válidos para series mensuales y trimestrales.\nEn origen los métodos de descomposición no sirven para realizar predicciones, pero actualmente se usan también con este fin (véase las funciones stlm y stlf del paquete forecast)."
  },
  {
    "objectID": "03-02-Tema2.html#descomposición-por-medias-móviles",
    "href": "03-02-Tema2.html#descomposición-por-medias-móviles",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.2 Descomposición por medias móviles",
    "text": "4.2 Descomposición por medias móviles\n\nIdeas generales\nLa función decompose estima las componentes de tendencia, estacionalidad y residuo usando el método de medias móviles (que veremos con más detalle en el Tema 4). En concreto decompose sigue los siguientes pasos para obtener cada componente:\nPaso 1: Se estima la tendencia de una serie a partir de una media móvil centrada. Si el orden estacional es par, la media móvil es ponderada de orden \\(m + 1\\); y si el orden estacional es impar, la media móvil es de orden \\(m\\). En concreto,\n\nSi \\(m=2k\\): \\(\\hat{T}_t = \\frac{\\frac{1}{2}y_{t-k} + y_{t-k+1} + \\ldots + y_t + \\ldots + y_{t+k-1} + \\frac{1}{2} y_{t+k}}{m}\\),\nSi \\(m=2k+1\\): \\(\\hat{T}_t = \\frac{y_{t-k} + y_{t-k+1} + \\ldots + y_t + \\ldots + y_{t+k-1} + y_{t+k}}{m}\\).\n\nPaso 2: Para un modelo con esquema aditivo calculamos la serie sin tendencia como \\(y_t - \\hat{T}_t\\) y para un esquema multiplicativo como \\(y_t/ \\hat{T}_t\\).\nPaso 3: Para estimar la componente estacional para cada periodo estacional, calculamos el valor medio de la serie sin tendencia (paso 2) de forma independiente para los datos de cada estación. Así, obtenemos un vector con la estimación de las \\(m\\) componentes estacionales.\nDespués estos valores se ajustan para que sumen 0 (esquema aditivo) o para que sumen \\(m\\) (esquema multiplicativo). La componente estacional se obtiene repitiendo el vector de \\(m\\) componentes ajustadas hasta alcanzar la longitud de la serie original. Esto da \\(\\hat{S}_t\\)\nPaso 4: El residuo se obtiene como \\(\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t\\) (esquema aditivo) o \\(\\hat{R}_t = y_t / (\\hat{T}_t \\cdot \\hat{S}_t)\\) (esquema multiplicativo)\n\n\nLa Tabla 1 muestra un ejemplo de descomposición aditiva por medias móviles para una serie simulada de orden estacional 5.\n\nLa dos primeras columnas indican la estación de cada dato y el valor de la serie, para un total de 25 datos. La columna Ten ha sido obtenida siguiendo el paso 1 como una media móvil de orden 5: \\[Ten_t = (Serie_{t-2} + Serie_{t-1} + Serie_{t} + Serie_{t+1} + Serie_{t+2})/5.\\]\nLa serie sin tendencia, columna Est + Res, se obtiene restando a la columna Serie la columna Ten, tal y como se indica en el paso 2.\nPara el cálculo de la columna Est, que repite de forma periódica la primera estimación de las 5 componentes estacionales, se sigue el paso 3. Para cada estación se promedian los valores de la columna Est + Res correspondientes a dicha estación. La suma de los cinco valores de la componente estacional así obtenidos vale 1.1.\nPara ajustar la componente estacional para que sume 0 a cada valor de la componente estacional se le resta su suma actual 1.1 dividida por 5, el número de estaciones. El resultado de este ajuste aparece en la columna Est corregida que será la componente estacional final.\nSiguiendo el paso 4, la columna Res se calcula restando a la serie original (columna Serie) la suma de la tendencia y la estacionalidad (columnas Ten y Est corregida).\n\nObserva que en el proceso de descomposición se han perdido 4 datos para la tendencia y el residuo, dos al inicio de la serie y dos al final.\n\n\n\n\n\n\n\nTabla 1: Ejemplo de descomposición por medias móviles\n\n\nEstacion\nSerie\nTen\nEst + Res\nEst\nEst corregida\nRes\n\n\n\n\n1\n17.00\nNA\nNA\n9.14\n8.92\nNA\n\n\n2\n6.72\nNA\nNA\n-10.89\n-11.11\nNA\n\n\n3\n5.08\n20.62\n-15.54\n-10.06\n-10.28\n-5.48\n\n\n4\n8.79\n27.89\n-19.10\n-9.46\n-9.68\n-9.64\n\n\n5\n65.53\n28.00\n37.53\n22.37\n22.15\n15.16\n\n\n1\n53.31\n28.58\n24.73\n9.14\n8.92\n15.59\n\n\n2\n7.28\n29.79\n-22.51\n-10.89\n-11.11\n-11.62\n\n\n3\n8.00\n25.62\n-17.62\n-10.06\n-10.28\n-7.56\n\n\n4\n14.84\n19.13\n-4.29\n-9.46\n-9.68\n5.17\n\n\n5\n44.67\n20.08\n24.59\n22.37\n22.15\n2.22\n\n\n1\n20.85\n21.78\n-0.93\n9.14\n8.92\n-10.07\n\n\n2\n12.02\n21.64\n-9.62\n-10.89\n-11.11\n1.27\n\n\n3\n16.51\n18.97\n-2.46\n-10.06\n-10.28\n7.60\n\n\n4\n14.14\n22.07\n-7.93\n-9.46\n-9.68\n1.53\n\n\n5\n31.31\n23.08\n8.23\n22.37\n22.15\n-14.14\n\n\n1\n36.37\n24.68\n11.69\n9.14\n8.92\n2.55\n\n\n2\n17.06\n26.66\n-9.60\n-10.89\n-11.11\n1.29\n\n\n3\n24.53\n30.92\n-6.39\n-10.06\n-10.28\n3.67\n\n\n4\n24.02\n30.56\n-6.54\n-9.46\n-9.68\n2.92\n\n\n5\n52.62\n33.48\n19.14\n22.37\n22.15\n-3.23\n\n\n1\n34.59\n33.51\n1.08\n9.14\n8.92\n-8.06\n\n\n2\n31.66\n33.51\n-1.85\n-10.89\n-11.11\n9.04\n\n\n3\n24.65\n32.95\n-8.30\n-10.06\n-10.28\n1.76\n\n\n4\n24.01\nNA\nNA\n-9.46\n-9.68\nNA\n\n\n5\n49.86\nNA\nNA\n22.37\n22.15\nNA\n\n\n\n\n\n\n\n\n\n\n\n\nMedia movil centrada y no centrada\n\n\n\n\n\nCon esta técnica de descomposición hemos usamos la media móvil centrada para extraer la tendencia de la serie. En el tema 4 veremos la media móvil no centrada para predecir la serie.\n\n\n\n\n\nLos principales inconvenientes de este método de descomposición son que se perderán datos al inicio y final de la serie –por ejemplo, si la serie es mensual se perderán seis datos al inicio y seis al final–, y que asume que la componente estacional no ha variado en el tiempo. Sin embargo, sabemos que para muchas series sociales y de consumo la componente estacional se ha suavizando con el tiempo.\nPor el contrario, una se las ventajas de este método, además de su sencillez de cálculo, es que se puede usar tanto para esquemas aditivos (type=\"addi\") como multiplicativos (type=\"multi\").\nLa función decompose genera un objeto con las siguientes componentes:\n\n$x para la serie original,\n$trend para la tendencia,\n$seasonal para la estacionalidad,\n$random para el residuo, y\n$figure que contiene las estimaciones de los m efectos estacionales ajustados. Es una extracción para un único año o semana de $seasonal.\n\nSiempre que generes nuevos objetos en R a partir de funciones te recomiendo que con names y str mires que hay en su interior.\nEn los métodos de descomposición que vamos a ver, para obtener las componentes individualmente puedes usar la función seasonal para la componente estacional, trendcycle para el componente de tendencia, y remainder para el residuo.\n\n\nEjemplo de esquema aditivo\nVamos a descomponer la serie Demanda eléctrica asumiendo un esquema aditivo (type = \"addi).\n\neleDesAdi <- decompose(electricidad, \n                       type = \"addi\")\n\nautoplot(eleDesAdi,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 20: Descomposición aditiva de la Demanda eléctrica por medias móviles\n\n\n\n\nEs fácil verificar que si se suma para cada fecha la tendencia, la estacionalidad y el residuo se obtiene exactamente el valor de la serie:\n\ntmp <- trendcycle(eleDesAdi) + seasonal(eleDesAdi) + remainder(eleDesAdi)\nsummary(electricidad - tmp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0       0       0       0       0       0       6 \n\n\nA continuación, tienes un ejemplo del manejo de las componentes extraídas para hacer una gráfica.\n\nautoplot(electricidad, \n         series=\"Demanda eléctrica\",\n         xlab = \"\",\n         ylab = \"MWh\",\n         main = \"\") +\n  autolayer(trendcycle(eleDesAdi), \n            series=\"Tendencia\") +\n  scale_colour_manual(values=c(\"Demanda eléctrica\"=\"black\",\"Tendencia\"=\"red\"),\n                      breaks=c(\"Demanda eléctrica\",\"Tendencia\"))\n\n\n\n\nFigura 21: Demanda eléctrica: serie y tendencia\n\n\n\n\nTambién podemos ver las componentes estacionales y verificar que suman 0. Ojo, como la serie empieza un viernes, la primera componente que se muestra es la del viernes y la última la del jueves.\n\neleDesAdi$figure\n\n[1]  24.91077 -43.49535 -86.11384  12.17731  29.87723  30.23754  32.40634\n\nsum(eleDesAdi$figure)\n\n[1] 0.000000000000003552714\n\n\nPor último, podemos realizar una gráfica de la componente estacional.\n\ncompEstacional <- eleDesAdi$figure[c(4:7, 1:3)]\nggplot() +\n  geom_line(aes(x = 1:7, y = compEstacional)) + \n  geom_hline(yintercept = 0, colour = \"blue\", lty = 2) +\n  ggtitle(\"\") +\n  xlab(\"\") +\n  ylab(\"GWh\") +\n  scale_x_continuous(breaks= 1:7, \n                     labels = c(\"Lunes\", \"Martes\", \"Miércoles\", \"Jueves\", \n                                \"Viernes\", \"Sábado\", \"Domingo\")) \n\n\n\n\nFigura 22: Componente estacional de Electricidad (esquema aditivo)\n\n\n\n\n\n\nEjemplo de Esquema Multiplicativo\nVeamos ahora la descomposición de Nacimientos bajo un esquema multiplicativo (type = \"mult\").\n\nnacDesMul <- decompose(nacimientos, \n                       type = \"mult\")\n\nautoplot(nacDesMul,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 23: Descomposición multiplicativa de Nacimientos por medias móviles\n\n\n\n\nObserva que por tratarse de un esquema multiplicativo en la Figura 23 la componente estacional se mueve alrededor del valor 1 y debe interpretarse como una variación porcentual. Igualmente, el residuo también gira en torno al valor 1.\nLas componentes estacionales se deben interpretar como incrementos porcentuales: en febrero nacen un 9.1% menos de niños y en octubre un 3.3% más, respecto de la media anual. Además, la suma de las componentes estacionales será 12.\n\nnacDesMul$figure\n\n [1] 0.9982724 0.9090410 1.0063202 0.9841434 1.0342330 0.9821019 1.0350563\n [8] 1.0165354 1.0300831 1.0335096 0.9780025 0.9927012\n\nsum(nacDesMul$figure)\n\n[1] 12"
  },
  {
    "objectID": "03-02-Tema2.html#descomposición-por-regresiones-locales-ponderadas",
    "href": "03-02-Tema2.html#descomposición-por-regresiones-locales-ponderadas",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.3 Descomposición por regresiones locales ponderadas",
    "text": "4.3 Descomposición por regresiones locales ponderadas\n\nIdeas generales\nLa función stl estima las componentes de tendencia y estacionalidad a partir de regresiones locales ponderadas (técnica conocida como loess)\nSus ventajas son:\n\nNo se perderán datos al inicio o al final de la serie.\nAsume que tanto la tendencia como la estacionalidad pueden cambiar con el tiempo y posibilita controlar este cambio a partir de parámetros.\nEs bastante robusta frente a valores atípicos.\n\nSu principal desventaja es que esta técnica de descomposición solo es válida para esquemas aditivos. Es posible obtener con stl una descomposición multiplicativa descomponiendo primero el logaritmo de la serie, para después calcular la exponencial de las componentes.\nLa función stl genera un objeto con la componente $time.series que contiene en columna tres series temporales: seasonal, trend y remainder (de nuevo usa names y str para aprender más).\nLos dos parámetros principales que deben elegirse cuando se utiliza stl son la ventana de tendencia (t.window) y la ventana estacional (s.window). Estos parámetros controlan la rapidez con la que pueden cambiar los componentes de tendencia y estacional con el tiempo. Valores pequeños permiten cambios más rápidos, valores grandes implican que no hay cambios. Ambos parámetros deben ser números impares:\n\nt.window es el número de observaciones consecutivas que se deben utilizar al estimar la tendencia. Consulta la ayuda para ver el valor por defecto.\ns.window está relacionado con el número observaciones que se deben utilizar al estimar cada valor de la componente estacional. No hay ningún valor por defecto para este parámetro. Establecerlo como periodic equivale a que la componente estacional sea periódica (es decir, idéntica a lo largo de los años). Si es un valor numérico, debe ser impar y mayor o igual a 7.\n\n\n\nEjemplo\nVeamos un ejemplo de su uso para la serie Demanda eléctrica (Figura 24). Se ha usado el valor por defecto para t.window y se ha indicado que la estacionalidad es constante en el tiempo (s.window = \"periodic\"). Además, se ha especificado que se tenga en cuenta la posible existencia de valores atípicos (robust = TRUE).\n\neleStl <- stl(electricidad, \n              s.window = \"periodic\",\n              robust = TRUE)\nhead(eleStl$time.series)\n\nTime Series:\nStart = c(1, 5) \nEnd = c(2, 3) \nFrequency = 7 \n          seasonal    trend    remainder\n1.571429  24.60393 757.7300 -173.6180276\n1.714286 -45.25864 763.9929  -38.3018792\n1.857143 -89.85937 770.2558    4.0927667\n2.000000  16.30943 777.5211   -0.2045157\n2.142857  30.74631 784.7863   -8.9444823\n2.285714  31.92108 792.5055 -124.2819712\n\n\n\nautoplot(eleStl,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 24: Descomposición de Electricidad por regresores locales ponderados\n\n\n\n\nPodemos ver numéricamente las componentes estacionales, que de nuevo deben sumar cero.\n\nhead(seasonal(eleStl), 7)\n\nTime Series:\nStart = c(1, 5) \nEnd = c(2, 4) \nFrequency = 7 \n[1]  24.60393 -45.25864 -89.85937  16.30943  30.74631  31.92108  31.53726\n\nsum(head(seasonal(eleStl), 7))\n\n[1] -0.0000001507404\n\n\n\n\nPara Demanda eléctrica hemos obtenido tres estimaciones de la componente estacional: la primera obtenida con tapply, la segunda obtenida con decompose y la tercera con stl. Se puede observar que las tres estimaciones son muy similares, pero no coincidentes, y aunque los métodos de descomposición son preferibles a tapply, ninguna estimación es a priori mejor que otra.\n\n# tapply\nround(as.numeric(componenteEstacional), 2)\n# decompose\nround(seasonal(eleDesAdi)[c(4:7, 1:3)], 2)\n# stl\nround(seasonal(eleStl)[c(4:7, 1:3)], 2)\n\n\n\n           [,1]  [,2]  [,3]  [,4]  [,5]   [,6]   [,7]\ntapply    12.58 30.28 30.32 32.09 21.34 -42.59 -84.43\ndecompose 12.18 29.88 30.24 32.41 24.91 -43.50 -86.11\nstl       16.31 30.75 31.92 31.54 24.60 -45.26 -89.86\n\n\n\n\nSi en lugar de periodic, fijamos el parámetro s.window a, por ejemplo, 11 (siempre un valor impar), estaremos permitiendo que la estacionalidad cambien en el tiempo. La Figura 25 muestra la componente estacional estimada previamente (bajo el supuesto de componente estacional constante) y la que se obtiene con el argumento s.window = 11. Para el periodo mostrado se observa que la componente estacional ha variando con el tiempo. Cuanto mayor es el valor (impar) de s.window más constante en el tiempo es la componente estacional.\n\n\n\n\n\nFigura 25: Componente estacional para Demanda eléctrica"
  },
  {
    "objectID": "01-Guia-curso.html#introducción",
    "href": "01-Guia-curso.html#introducción",
    "title": "Guía del Curso",
    "section": "Introducción",
    "text": "Introducción\nLa asignatura de Predicción con Datos Temporales pertenece al Grado en Inteligencia y Analítica de Negocios y se configura como un módulo impartido en el segundo cuatrimestre de segundo curso con 6 créditos ECTS.\nEl objetivo general es aprender a manejarse con datos de corte temporal, Series Temporales. Es decir, ser capaces de aplicar los métodos para describir, analizar, modelizar y predecir series de datos que evolucionan en el tiempo. Además, hay que saber evaluar la calidad de las predicciones en el contexto de la estadística tradicional y con técnicas de Machine Learning.\nPara ello, veremos la teoría necesaria y, sobre todo, practicaremos utilizando el programa estadístico R y el lenguaje Markdown (o Quarto) a través de RStudio. A estas alturas del grado ya debes estar familiarizado con este programa."
  },
  {
    "objectID": "01-Guia-curso.html#contenidos",
    "href": "01-Guia-curso.html#contenidos",
    "title": "Guía del Curso",
    "section": "Contenidos",
    "text": "Contenidos\nEl programa de la asignatura contiene 7 temas distribuidos entre sesiones teóricas de una hora y prácticas de tres horas:\n\nTema 1. Introducción\nTema 2. Definición y componentes\nTema 3: Métodos sencillos de predicción. Evaluación de predicciones.\nTema 4: Métodos de medias móviles.\nTema 5: Técnicas de alisado exponencial.\nTema 6. Procesos ARIMA\nTema 7. Procesos ARIMA con estacionalidad\n\nPasar de un tema al siguiente supone incrementar la complejidad de las técnicas empleadas para el análisis de una serie temporal, de forma que podréis valorar con datos reales la relación entre complejidad–tiempo–resultados y optar por la metodología más adecuada."
  },
  {
    "objectID": "01-Guia-curso.html#metodología",
    "href": "01-Guia-curso.html#metodología",
    "title": "Guía del Curso",
    "section": "Metodología",
    "text": "Metodología\nDado el carácter eminentemente práctico del curso, la mayoría de las clases se impartirán haciendo uso constante del ordenador y alternaremos entre teoría y práctica según convenga:\n\nTiempo de teoría: durante las clases teóricas, de una hora de duración, veremos conceptos nucleares de cada tema de forma precisa y rigurosa, en lenguaje natural, gráfico y formal. El material teórico lo podéis encontrar en la sección Diapos.\nTambién aprovecharemos algunas clases de teoría para ampliar en algunos conceptos que no forman parte del núcleo del curso, pero pueden serte útiles en tu futuro profesional. El material teórico lo podéis encontrar en Píldoras.\nTiempo de práctica: durante las clases prácticas también veremos algo de teoría (material en Diapos), pero inmediatamente la practicaremos a partir del código y de los ficheros de datos que podéis encontrar en la sección Recursos de la asignatura y aprenderemos el manejo de R para el análisis de series temporales.\nTiempo de evaluación: tras cada tema realizaréis una prueba tipo test para valorar si habéis adquirido los conocimientos teóricos y las habilidades practicas básicas. Sin embargo, el grueso de la evaluación consistirá en dos trabajos prácticos. Estas pruebas serán la evaluación continua del curso.\n\n\n\nOs recomiendo fuertemente que vengáis a clase con vuestros ordenadores.\nTodo el material necesario para este curso lo tenéis disponible en esta página web así como en Aula Virtual. También podéis encontrar los detalles en la sección Logística de la web."
  },
  {
    "objectID": "01-Guia-curso.html#evaluación",
    "href": "01-Guia-curso.html#evaluación",
    "title": "Guía del Curso",
    "section": "Evaluación",
    "text": "Evaluación\nLa evaluación continua supondrá un 80% de la nota de la asignatura.\n\nTras cada una de las unidades temáticas se realizará una prueba tipo test con preguntas de respuesta múltiple, numérica, verdadero/falso, etc. (40%)\nRealizaréis dos trabajos prácticos extensos que cubrirán todos los conceptos vistos en clase. (40%)\nDe la evaluación continua solo son recuperables los trabajos prácticos de cara a la segunda convocatoria, pero no lo serán las pruebas tipo test.\nTienes más información en Evaluación Continua\n\nEl examen final de la asignatura supondrá el restante 20% de la nota de la asignatura."
  },
  {
    "objectID": "03-04-Tema4.html#definición",
    "href": "03-04-Tema4.html#definición",
    "title": "Método de medias móviles",
    "section": "2.1 Definición",
    "text": "2.1 Definición\nEl método de la media móvil simple es adecuado para una serie estacionaria y sin estacionalidad. Es decir, una serie que se mueve alrededor de un nivel constante. En este caso, la predicción para un periodo es la media de las \\(r\\) observaciones previas. El valor \\(r\\), denominado orden de la media móvil, es un parámetro que debe fijar el investigador.\nDefinimos primero la media móvil hacia atrás (backward) de orden \\(r\\) para el periodo \\(t\\) como la media de las \\(r\\) últimas observaciones \\[mm_t = \\frac{1}{r}\\sum_{i=0}^{r-1} y_{t-i} = \\frac{y_t + y_{t-1} + \\ldots + y_{t-r+1}}{r}\\] Observa que la media móvil hacia atrás se define de forma diferente a la media móvil centrada que vimos en la descomposición por medias móviles.\nA partir de \\(mm_t\\) se define la ecuación de la predicción intramuestral como \\[\\widehat{y}_{t+1} = mm_t.\\] Podemos entender que \\(mm_t\\) es una estimación del nivel de la serie en el periodo \\(t\\) y como la serie es estacionaria (no cambia de nivel), este valor se usa como predicción para el periodo siguiente.\nEs decir, la media móvil simple de orden \\(r\\) usa para predecir el promedio los últimos \\(r\\) datos. Es un punto medio entre el método Ingenuo I, que para predecir usaba solo el ultimo dato, y la media simple, que promediaba todos los datos.\nLa primera predicción extramuestral es \\(\\widehat{y}_{T+1} = mm_T\\) y las restantes \\(\\widehat{y}_{T+h} = \\widehat{y}_{T+1}.\\)\nObserva que:\n\nPara poder aplicar la media móvil de orden \\(r\\) se necesitan al menos \\(r\\) datos.\nPara los \\(r-1\\) periodos iniciales no es posible calcular la media móvil y, por tanto, para los \\(r\\) periodos iniciales no es posible obtener una predicción intramuestral.\nEl valor de \\(r\\) debe fijarlo el investigador. Cuanto mayor es su valor, más suave es la linea de predicciones obtenida y menos efecto tiene el valor de una observación individual sobre la predicción (véase la Figura 1). En el caso límite \\(r=1\\) se tiene el método ingenuo I y para \\(r=T\\) el método de la media.\n\nVeamos un sencillo ejemplo numérico. La Tabla 1 muestra la aplicación del método de las medias móviles para los 10 últimos datos de la serie Libros. La primera columna indica el año y la segunda el valor de la serie Libros. La tercera columna, mm, es la media móvil de orden 5 y la siguiente columna, Yhat, la predicción de Libros, es decir la columna mm desfasada un periodo. La columna Res (error de predicción) se calcula restando a la serie original (columna Libros) la predicción intramuestral (columna Yhat). Observa que en el proceso se han perdido 5 datos para la predicción y el residuo al inicio de la serie.\n\n\n\n\n\n\n\nTabla 1: Ejemplo de descomposición por medias móviles\n\n\nFecha\nLibros\nmm\nYhat\nRes\n\n\n\n\n2010\n76206\nNA\nNA\nNA\n\n\n2011\n74244\nNA\nNA\nNA\n\n\n2012\n69668\nNA\nNA\nNA\n\n\n2013\n56435\nNA\nNA\nNA\n\n\n2014\n56030\n66516.6\nNA\nNA\n\n\n2015\n61008\n63477.0\n66516.6\n-5508.6\n\n\n2016\n59567\n60541.6\n63477.0\n-3910.0\n\n\n2017\n60185\n58645.0\n60541.6\n-356.6\n\n\n2018\n62180\n59794.0\n58645.0\n3535.0\n\n\n2019\n64154\n61418.8\n59794.0\n4360.0\n\n\n\n\n\n\nTu mismo puedes comprobar que\n\\[mm_{2014} = \\frac{76206 + 74244 + 69668 + 56435 + 56030}{5} = 66516.6\\] y la primera predicción extramuestral sería la media de las 5 últimas observaciones\n\\[\\widehat{Libros}_{2020} = mm_{2019} =  61418.8.\\]\n\n\n\n\n\n\n¿Por que no es la media móvil un método sencillo?\n\n\n\nDate cuenta de que es la primera vez que en un método hay que elegir el valor de un parámetro.\n¿Cómo elegirlo? No hay ninguna base teórica que permita saber el valor más adecuado para el orden de la media móvil, hay que fijarlo según criterios empíricos."
  },
  {
    "objectID": "03-04-Tema4.html#ejemplo-de-aplicación-con-demanda-eléctrica",
    "href": "03-04-Tema4.html#ejemplo-de-aplicación-con-demanda-eléctrica",
    "title": "Método de medias móviles",
    "section": "2.2 Ejemplo de aplicación con Demanda Eléctrica",
    "text": "2.2 Ejemplo de aplicación con Demanda Eléctrica\nVamos a usar el método de medias móviles para predecir la serie Demanda de electricidad semanal. La Figura 2 muestra que es una serie estacionaria alrededor de un valor medio.\n\nelectricidad <- read.csv2(\"./series/Consumo electrico.csv\", header = TRUE)\n\nelectricidad <- ts(electricidad[, 2],\n                   start = c(1, 5),\n                   frequency = 7)\n\n#Creamos la serie de consumo semanal\nelectricidad <- aggregate(electricidad, FUN = sum) \n\n\nautoplot(electricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\",\n         ylim= c(0, 6000))\n\n\n\n\nFigura 2: Consumo semanal de electricidad (2021)\n\n\n\n\nR proporciona varias funciones que permiten calcular la media móvil simple: filter del paquete stats, ts_ma de TSstudio, sma de smooth, ma de forecast o movavg de pracma. Sin embargo, ninguna de estas funciones es tan completa como las vistas en el tema previo y permiten obtener la media móvil, la estimación, la predicción y el error con una sola función.\nPara resolver este inconveniente vamos a programar nuestra propia función mmf\n\nmmf <- function(x, r = 3, h = 5) {\n  z <- NULL\n  z$x <- x\n  z$orden = r\n  \n  TT <- length(x)\n  inicio <- start(x)\n  frecuencia <-frequency(x)\n  \n  z$mm <- stats::filter(x, rep(1/r, r), \n                        side = 1)\n  \n  z$fitted <- ts(c(NA, z$mm[-TT]), \n                 start = inicio, \n                 freq = frecuencia)\n  \n  z$mean <- ts(rep(z$mm[TT], h), \n               start = time(x)[TT] + 1/frecuencia, \n               freq = frecuencia)\n  \n  z$residuals <- x - z$fitted\n  \n  class(z) <- \"forecast\"\n  z\n}\n\nLa función mmf precisa de tres argumentos, una serie temporal, el orden de la media móvil \\(r\\) y el horizonte de previsión \\(h\\). Los dos últimos argumentos están fijados a 3 y 5 como opciones por defecto. El cálculo de la media móvil se hace mediante la función filter (librería stats), que calcula la media móvil simple. Realmente la función filter calcula una media ponderada, tiene como primer argumento la serie y el segundo argumento son los pesos de la media móvil, es decir \\(1/r\\) repetido \\(r\\) veces o rep(1/r, r). El tercer argumento indica si la media es centrada side = 2 (como la usada en el método de descomposición visto en el tema 2) o hacia atrás side = 1.\nLa función mmf devuelve un objeto que contiene la serie original (x), el orden de la media móvil (orden), la media móvil (mm), la serie ajustada o predicción intramuestral a un periodo vista (fitted), la predicción (mean) y el error de estimación (residuals).\n\nmmelectricidad <- mmf(electricidad, r = 5, h = 5)\n\nLa Figura 3 muestra la serie Demanda eléctrica (negro), las previsiones intramuestrales (rojo) y las previsiones extramuestrales (azul), que son constantes.\n\nautoplot(mmelectricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\") +\n  autolayer(mmelectricidad$fitted) + \n  theme(legend.position=\"none\")\n\n\n\n\nFigura 3: Consumo eléctrico y predicción con media móvil de orden 5\n\n\n\n\nPodemos ahora obtener los diferentes indicadores de bondad de ajuste del modelo (error de predicción intramuestral) con la función accuracy.\n\naccuracy(mmelectricidad)\n\n                    ME    RMSE      MAE        MPE     MAPE     MASE      ACF1\nTraining set -25.32394 244.639 179.1574 -0.6787638 3.702664 1.093756 0.4808701\n\n\nEl MAPE de 3.7% (o 245 GWh de RMSE) evidencia que las predicciones son bastante adecuadas.\n\nIdentificación del mejor modelo\nPodemos aplicar el método de la media móvil para diferentes órdenes \\(r\\) y comprobar para cual de ellos se obtiene el menor valor del criterio de bondad de ajuste deseado. Realizaremos este ejercicio considerando como indicador de bondad de ajuste el MAPE pero en dos escenarios: previsiones intramuestrales a un periodo vista y origen de predicción móvil.\nSelección usando previsiones intramuestrales a un periodo vista\nLa siguiente rutina calcula el MAPE para el método de la media móvil con órdenes desde 1 a 4.\n\nfor(r in 1:4) {\n  error <- accuracy(mmf(electricidad, r = r))[5]\n  cat(\"\\nPara un orden de\", \n      r, \n      \"el error es\", \n      formatC(error, format = \"f\", digits = 2),\n      \" %\")\n}\n\n\nPara un orden de 1 el error es 3.33  %\nPara un orden de 2 el error es 3.40  %\nPara un orden de 3 el error es 3.40  %\nPara un orden de 4 el error es 3.56  %\n\n\nPodemos concluir que el orden óptimo es 1, donde se alcanza el menor MAPE, es decir el método Ingenuo I. Sin embargo, para órdenes superiores de la media móvil, el error se mantiene igual de reducido.\nSelección usando origen de predicción móvil\nDado que nuestro interés está en la calidad de las previsiones, vamos a obtener el MAPE para el método de la media móvil usando origen de predicción móvil. Asumiremos un horizonte de predicción de hasta cuatro semanas y 20 como el mínimo número de semanas necesarias para hacer las estimaciones. Consideremos órdenes desde 1 hasta 4.\n\nk <- 20              \nh <- 4               \nTT <- length(electricidad) \ns <- TT - k - h      \n\nfor(r in 1:4){\n  \n  mapemm <- matrix(NA, s + 1, h)\n  for (i in 0:s) {\n    \n    train.set <- subset(electricidad, start = i + 1, end = i + k)\n    test.set <-  subset(electricidad, start = i + k + 1, end = i + k + h)\n    \n    mmElectricidad <- mmf(train.set, r = r, h = h)\n    mapemm[i + 1, ] <- 100*abs(test.set - mmElectricidad$mean)/test.set\n  }\n  mapemm <- colMeans(mapemm)\n  \n  cat(\"\\nPara un orden de\", \n      r, \n      \"los errores son\", \n      formatC(mapemm, format = \"f\", digits = 2)) \n  \n}\n\n\nPara un orden de 1 los errores son 2.80 3.80 4.21 5.91\nPara un orden de 2 los errores son 3.04 3.54 4.56 6.07\nPara un orden de 3 los errores son 3.09 3.86 4.84 6.14\nPara un orden de 4 los errores son 3.35 4.29 5.05 6.28\n\n\nPodemos aceptar que con independencia del horizonte temporal de previsión, el orden \\(r = 1\\) (Ingenuo I) podría ser el más adecuado. Sólo para una previsión a dos semanas vista el orden 2 parece ser superior. En todo caso, en este ejemplo la calidad de las previsiones es relativamente independiente del orden de la media móvil.\n\n\n\n\n\n\nImportancia del criterio de calidad elegido\n\n\n\nPrueba a repetir los ejercicios de identificación del mejor modelo esta vez usando como criterio el RMSE. ¿Qué concluyes? ¿Coinciden tus conclusiones con las obtenidas usando el MAPE?"
  },
  {
    "objectID": "03-09-Ejemplo3.html#identificación-del-método-sencillo-con-mejor-ajuste",
    "href": "03-09-Ejemplo3.html#identificación-del-método-sencillo-con-mejor-ajuste",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "2.1 Identificación del método sencillo con mejor ajuste",
    "text": "2.1 Identificación del método sencillo con mejor ajuste\nPara series sin estacionalidad tenemos tres aproximaciones por métodos sencillos: media simple, método ingenuo I y método de la deriva. Veamos en primer lugar cual de ellos ajusta mejor a los datos, es decir, cual ofrece las mejores predicciones intramuestrales a un periodo vista.\n\nmediaPernoctaciones <- meanf(PernoctacionesAnual, h = 5)\nnaivePernoctaciones <- naive(PernoctacionesAnual, h = 5)\nderivaPernoctaciones <- rwf(PernoctacionesAnual,  h = 5, drift = TRUE)\n\nautoplot(PernoctacionesAnual, series = \"Pernoctaciones\",\n                xlab = \"\",\n                ylab = \"Noches (millones)\",\n                main = \"\") +\n  autolayer(mediaPernoctaciones, series=\"Media\", PI = FALSE) +\n  autolayer(naivePernoctaciones, series=\"Ingenuo\", PI = FALSE) +\n  autolayer(derivaPernoctaciones, series=\"Deriva\", PI = FALSE) +\n  scale_colour_discrete(limits=c(\"Pernoctaciones\", \"Media\", \"Ingenuo\", \"Deriva\")) +\n  labs(colour=\"Métodos\") + \n  theme(legend.position=c(0.15,0.7))\n\n\n\n\nFigura 3: Pernoctaciones anuales y predicción por métodos sencillos\n\n\n\n\nLa Figura 3 muestra lo inadecuado del método de la media, y no deja claro entre los métodos Ingenuo y de la Deriva cuál es más adecuado. El primero ofrece una previsión constante –acorde con el comportamiento de la serie en los dos últimos años– y el segundo previsiones crecientes –acordes con la tendencia general de la serie. Veamos la capacidad de ajuste de cada método\n\ntmp <- rbind(\n  accuracy(mediaPernoctaciones),\n  accuracy(naivePernoctaciones),\n  accuracy(derivaPernoctaciones)\n)\ntmp <- round(tmp,2)\nrownames(tmp) <- c(\"Media\",\"Ingenuo I\",\"Deriva\")\ntmp\n\n            ME  RMSE   MAE   MPE  MAPE MASE ACF1\nMedia     0.00 32.54 27.37 -1.67 11.01 2.92 0.86\nIngenuo I 3.43 12.10  9.36  1.16  3.89 1.00 0.26\nDeriva    0.00 11.60  9.28 -0.26  3.92 0.99 0.26\n\n\nTanto el método ingenuo I, como el método de la deriva resultan igualmente buenos respecto a su ajuste a los datos (MAPE de 3.9%). El primero, porque repetir el último dato siempre es una buena estrategia para predicciones a cortísimo plazo. El segundo, porque considerar la pendiente media pasada es también una buena estrategia, que solo fallará en los periodos de cambio de tendencia. Sin embargo, en ningún caso el intervalo de confianza de las predicciones será de utilidad."
  },
  {
    "objectID": "03-09-Ejemplo3.html#identificación-del-método-sencillo-con-mejor-predicción-extramuestral",
    "href": "03-09-Ejemplo3.html#identificación-del-método-sencillo-con-mejor-predicción-extramuestral",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "2.2 Identificación del método sencillo con mejor predicción extramuestral",
    "text": "2.2 Identificación del método sencillo con mejor predicción extramuestral\nYa hemos visto que para los dos mejores métodos el error es próximo al 3.9%. Este valor es la estimación del error en la previsión intramuestral y a un periodo vista. A fin de poder estimar mejor la capacidad predictiva de los dos métodos vamos a aplicar el método de origen de predicción móvil para obtener los errores extramuestrales según el horizonte de previsión.\nAsumiremos que necesitamos 10 años para obtener un buena estimación. La siguiente rutina permite obtener el MAPE para previsiones con un horizonte temporal desde 1 a 5 años.\n\nk <- 10                  \nh <- 5                   \nTT <- length(PernoctacionesAnual) \ns <- TT - k - h          \n\nmapeNaiveI <- matrix(NA, s + 1, h)\nmapeDeriva <- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set <- subset(PernoctacionesAnual, start = i + 1, end = i + k)\n  test.set <-  subset(PernoctacionesAnual, start = i + k + 1, end = i + k + h)\n  \n  fcast <- naive(train.set, h = h)\n  mapeNaiveI[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n  fcast <- rwf(train.set, h = h,  drift = TRUE)\n  mapeDeriva[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n}\n\nmapeNaiveI <- colMeans(mapeNaiveI)\nmapeDeriva <- colMeans(mapeDeriva)\n\nmapeNaiveI\n\n[1]  4.741687  9.586099 13.225053 16.253007 18.579674\n\nmapeDeriva\n\n[1]  4.204332  8.547111 11.779166 14.371077 16.222312\n\n\nLa conclusión es clara, con independencia del horizonte temporal, el método de la Deriva es el que ofrece las mejores predicciones.\nPara el método de la Deriva, el error de previsión extramuestral a un periodo (4.2%) es algo mayor que el error de ajuste (3.9%). Por otro lado, no parece aconsejable utilizar este método para predicciones más allá de dos años vista porque el error porcentual crece rápidamente."
  },
  {
    "objectID": "03-09-Ejemplo3.html#predicciones",
    "href": "03-09-Ejemplo3.html#predicciones",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "2.3 Predicciones",
    "text": "2.3 Predicciones\nUna vez identificado el mejor método sencillo de ajuste para la serie anual, mostramos las predicciones numéricamente.1\n\nderivaPernoctaciones\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2020       302.5235 287.2448 317.8022 279.1567 325.8902\n2021       305.9548 283.7862 328.1235 272.0508 339.8589\n2022       309.3862 281.5647 337.2076 266.8369 351.9354\n2023       312.8175 279.9361 345.6990 262.5297 363.1054\n2024       316.2489 278.6601 353.8377 258.7617 373.7360"
  },
  {
    "objectID": "03-09-Ejemplo3.html#validación",
    "href": "03-09-Ejemplo3.html#validación",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.1 Validación",
    "text": "3.1 Validación\nVamos de nuevo a aplicar el método de origen de predicción móvil para obtener los errores extramuestrales según el horizonte de previsión.\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k=120\\), y que el horizonte temporal es un año, \\(h = 12\\) meses.\n\nk <- 120                 \nh <- 12                  \nTT <- length(Pernoctaciones)  \ns <- TT - k - h          \n\nmapeSnaive <- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set <- subset(Pernoctaciones, start = i + 1, end = i + k)\n  test.set <-  subset(Pernoctaciones, start = i + k + 1, end = i + k + h)\n  \n  fit <- snaive(train.set, h = h)\n  mapeSnaive[i + 1,] <- 100*abs(test.set - fit$mean)/test.set\n}\n\nmapeSnaive <- colMeans(mapeSnaive)\nmapeSnaive\n\n [1] 5.082764 5.067379 5.049382 5.040556 5.058523 5.014313 4.980572 4.908294\n [9] 4.816117 4.738261 4.651920 4.587700\n\nggplot() +\n  geom_line(aes(x = 1:12, y = mapeSnaive)) +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"MAPE\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\nFigura 5: Error de predicción según horizonte temporal\n\n\n\n\nLa Figura 5 muestra el error de previsión extramuestral según el horizonte de previsión. Cuidado con la interpretación. El error prácticamente no varía con el horizonte temporal y se mueve en una estrecha franja de 0.5 puntos porcentuales, entre el 5.1% y el 4.6%. La serie Pernoctaciones es tan sencilla que la mejor predicción a corto plazo es repetir la última observación del mismo mes."
  },
  {
    "objectID": "06-Evaluacion_Continua.html",
    "href": "06-Evaluacion_Continua.html",
    "title": "Evaluación continua",
    "section": "",
    "text": "Durante todo el curso realizarás una serie de pruebas de evaluación continua que aplicarán las técnicas vistas en cada tema. Hay dos tipos de pruebas\n\n\n\nPruebas tipo test\n\nEn la mayoría de los casos la prueba la recibirás en tu correo de la universidad (os avisaré con antelación) y consistirá en un documento PDF con preguntas (unas 10) tipo test o de respuestas numérica.\nLa prueba contendrá una plantilla de respuestas que deberás cumplimentar y subir a Aulavirtual como fichero PDF. Para cada prueba se abrirá una tarea específica.\nEl plazo de entrega se indicará en el correo con la prueba, pero será unos dos o tres días después de que recibas el correo.\nAlguna prueba se realizará en clase, en este caso sin aviso previo, y la plantilla de respuestas se deberá entregar en mano.\n\n\n\n\n\nTrabajos prácticos\n\nDurante el curso realizarás dos trabajos que aplicarán las técnicas vistas.\nPara empezar, la primera tarea es descargarte tu serie temporal. Descárgate aqui el fichero .pdf con la descripción de las series y localiza en él el nombre de tu serie. En este fichero encontrarás también una descripción de la serie que vas a analizar: nombre, definición, unidades, fuente, fechado…\nPor otro lado, descárgate aquí el fichero comprimido con todas las series, descomprímelo y localiza el fichero ‘’csv’’ con tu serie.\nLa fecha máxima de entrega de cada trabajo se indicará con tiempo. Procura no retrasarte.\nEn Aulavirtual, en la tarea “Practica evaluación n” deberás subir un fichero .pdf o .html con los resultados de la práctica n-ésima. Sólo se admite un único fichero.\nPiensa que además de los contenidos también su ordenación, estructura, sintaxis, comentarios, etc. son parte de la evaluación.\nIncluye siempre todo el código en R utilizado en el trabajo. Dado que utilizas Rmarkdown, haz que el código sea visible.\n\n\n\nLa media ponderada de las notas de estos test y trabajos será el 80% de la nota de la asignatura, el restante 20% se obtendrá a partir del examen."
  },
  {
    "objectID": "02-Logistica.html",
    "href": "02-Logistica.html",
    "title": "Logística",
    "section": "",
    "text": "El grueso del material del curso lo podéis encontrar en esta página web. Sin embargo, hay muuuucho material adicional disponible en formato papel y online. A continuación os describo brevemente dónde podéis encontrar parte de este material.\n\n\n\n\n\n\nContenidos del curso\n\nPágina web del curso dónde encontrarás todo el material teórico y práctico necesario para seguir el curso y realizar las prácticas de evaluación.\nAula Virtual donde encontrarás una copia de todo el material del curso en un único fichero comprimido. Además, Aulavirtual será el medio de comunicación oficial entre nosotros y dónde subiréis las respuestas a las pruebas tipo test y las prácticas de evaluación.\n\n\n\n\n\n\n\n\nMateriales\n\nContenidos teóricos: los encontrarás en la sección Diapos. Las primeras entradas corresponden a los contenidos teóricos del curso y las restantes a un ejemplo práctico de aplicación para cada tema. Este ejemplo no necesariamente lo veremos durante el curso, pero puede serte de ayuda para terminar de comprender los conceptos teóricos vistos y su aplicación práctica con R.\nEn Píldoras encontrarás extensiones a conceptos del curso, algunos de los cuales iremos vendo en clase de teoría.\nRecursos de la asignatura: contiene los ficheros de datos de los ejemplos utilizados durante el curso (ficheros .csv) así como el código de R que usaremos en las clases (ficheros .R).\n\n\n\n\n\n\nMás recursos\nLa última sección de la página web Más es un pequeño baúl de recursos que te pueden ser útiles:\n\nR: enlaces a páginas con recursos sobre R y RStudio\nMarkdown: enlaces a páginas con recursos sobre (R)Markdown y Quatro\nOtros: enlace a R-bloggers, un blog sobre R que te puede ser de gran ayuda y a libros online sobre R, análisis de datos, inferencia…\n\n\n\n\n\n\n\n\nBibliografía\nEl material de este curso debe mucho a\n\nForecasting: Principles and Practice: un libro online (realizado con bookdown) de Rob J. Hyndman y George Athanasopoulos. Durante el curso usaremos fundamentalmente la librería forecast de Hyndman, Athanasopoulos y otros (aquí).\n\n\n\nHay un montón de bibliografía sobre Predicción con Datos Temporales, tanto de libros en papel como de recursos de todo tipo online. Es imposible ser exhaustivos, así que aquí va simplemente una muestra.\nSentíos libres de hurgar en la web en busca de tutoriales, videos, libros… que os puedan ser útiles. Si algún material que localizáis creéis que es realmente útil, por favor, compartirlo con vuestros compañeros y conmigo.\n\n\nGrandes clásicos de Series Temporales\n\nAbraham, B. y Ledolter, J. (1983) Statistical methods for forecasting. Wiley\nBox, G.E.P. y Jenkins, G.M. (1976) Time series analysis, forecasting and control. Holden-Day\nBox, G.E.P., Jenkins, G.M. y Reinsel, G.C. (1994) Time series análisis. Prentice-Hall\nBrockwell, P.J. y Davis, R.A. (1996) Introduction to time series and forecasting. Springer-Verlag\nChatfield, C. (1989) The analysis of time series. An introduction. Chapman & Hall\nGreene, W. (1998) Análisis econométrico. Prentice Hall\nHolden, K., Peel D.A. y Thompson, J. L. (1990) Economic Forecasting: an introduction. Cambridge University Preess: Cambridge\nHolton, J. y Keating, B. (1996) Previsión en los negocios. IRWIN\nMarkridakis, S., Weelwright, S.C. y Hyndman, R.J. (1998) Forecasting: Methods and Applications. Willey\nPeña, D. (1999) Estadística: modelos y métodos 2 (Modelos lineales y Series Temporales.) Alianza Universidad Textos.\nPulido, A. y López, A.M. (1999) Predicción y simulación aplicada al a economía y gestión de empresas. Pirámide.\nUriel, E. (2005) Introducción al análisis de series temporales. Paraninfo.\n\n\n\nBibliografía de Series Temporales con R\n\nCowpertwait, P. S. P. y Metcalfe, A. V. (2009) Introductory Time Series with R. Springer (Collection Use R!)\nPfaff, B. (2008) Analysis of Integrated and Cointegrated Time Series with R. Springer (Collection Use R!)\nCryer, J. D., Chan, Kung-Sik. (2008) Time Series Analysis. With Applications in R. Springer\n\n\n\nDirecciones de internet\n\nQuick-R: Time Series and Forecasting: un vistazo muy pero que muy rápido al análisis de series temporales con R.\nTres manuales online de series temporales al estilo del comentado al inicio de la bibliografía se pueden encontrar aquí, aquí y aquí\nA First Course on Time Series Analysis with SAS: es un libro de análisis de series temporales con SAS online\nJournal of Time Series Analysis: revista científica sobre series temporales\n\nInternational Journal of Forecasting: revista científica sobre predicción\n\n\n\nSoftware gratuito\n\nR-project: creo que ya lo conocemos todos.\nJDemetra+: ajuste automático de series con modelos ARIMA aplicando dos métodos: TRAMO-SEATS+ y X-12ARIMA/X-13ARIMA-SEATS.\nGretl: programa de modelización gratuito: regresión múltiple, modelos de elección binaria, series temporales, etc."
  },
  {
    "objectID": "04-05-Valores_perdidos_Outliers.html",
    "href": "04-05-Valores_perdidos_Outliers.html",
    "title": "Valores perdidos y valores atípicos",
    "section": "",
    "text": "Introducción\nEn la Figura 1 se muestran dos series de datos: en el panel superior las exportaciones de España a la Unión Europea de productos químicos, una serie mensual en millones de euros; en el panel inferior la extracción de dinero de un cajero ubicado en el centro de Valencia, una serie diaria en euros.\nEn la serie exportaciones destaca el valor de marzo de 2007, inusualmente elevado. En concreto ese mes se exportaron 1900 millones de euros, mientras que la media del marzo anterior y posterior fue de 1130 millones de euros. Es posible que este valor atípico (outlier) se deba a un error en algún punto del proceso que va desde la toma de datos en frontera hasta su registro en Eurostat (fuente de los datos). O quizás es que ese mes hubo una exportación excepcional de productos químicos por parte de España a algún país de la Unión Europea. En cualquier caso, este dado dificultará el ajuste de la serie a un modelo para su posterior predicción, especialmente si el modelo no admite intervención. ¿No sería más adecuado trabajar con una serie sin valores atípicos?\nLa serie de extracción de dinero de un cajero presenta dos valores ausentes o perdidos (missing values), ambos en jueves, debido a que el cajero se estropeó y fue imposible durante esos dos días extraer dinero. La presencia de valores perdidos dificulta o imposibilita la aplicación de muchas técnicas de ajuste y previsión de series temporales1. Claramente no podemos eliminar estos datos porque se rompería la estacionalidad, habría dos semanas de seis días sin jueves. Lo que se debe hacer es rellenar estos dos huecos.\n\n\n\n\n\n\n\n(a) Exportaciones de España a la UE de productos químicos\n\n\n\n\n\n\n\n\n\n(b) Extracción de dinero de un cajero\n\n\n\n\nFigura 1: Ejemplos de series con valores atípicos o perdidos\n\n\nTanto si se observan valores atípicos como datos perdidos, la solución más fácil para poder ajustar y predecir la serie usando las técnicas usuales es poner un valor adecuado que sustituya el dato atípico o el valor perdido. Al proceso de sustituir el dato de una observación por otro se le denomina imputación. El análisis de la serie imputada será más fácil que el de la serie original.\nAsí, tanto para valores perdidos como atípicos tendremos que aprender a imputar. Además, para los valores atípicos también tenderemos que aprender a identificarlos. ¿Cuándo un valor se sale lo suficiente de lo normal como para considerarlo atípico?\n\n\nImputación de valores perdidos\nNo existe una mejor forma de imputar. El proceso de imputación más adecuado dependerá, entre otros factores, de la naturaleza de la base de datos (corte transversal, longitudinal o panel), de la distribución variable (estacionaria, unimodal…) y del propio concepto que recoge la variable (exportaciones, temperatura, frecuencia…).\nComo regla general, las técnicas sencillas de imputación (poca manipulación de datos y poco tiempo de computación) suelen ser tan eficaces como las más complejas, que requerirán mucho más tiempo de computación. Además, el tiempo y coste dedicado a la imputación debe ser acorde con la relevancia del análisis que se está realizando.\nSi en algún momento necesitas trabajar profesionalmente con series temporales con datos perdidos, te recomiendo el paquete imputeTS que puedes descargarte desde CRAN.\nAquí vamos a ser más modestos y usar la función na.interp del paquete forecast que usa una adaptación para series temporales del método de imputación denominado interpolación lineal.2 Imputar e interpolar son dos conceptos estadísticos que, sin ser del todo equivalente, usaremos como tal en este contexto.\nEl método de interpolación lineal es uno de los más sencillos que existen y consiste en sustituir un valor perdido por el valor estimado con una regresión lineal obtenida a partir de los puntos vecinos del valor perdido. Para series sin estacionalidad este es el método de imputación que se aplica.\nPara series con estacionalidad, primero se descompone la serie (aplicando la versión robusta de stl). Después, se aplica la interpolación lineal a la componente de la tendencia. Por último se vuelven a combinar todas las componentes –tendencia imputada, estacionalidad y error– para obtener la nueva serie imputada.\nVeamos como queda la serie de dinero extraído de una cajero tras imputar los valores perdidos. La Figura 2 muestra la serie original a la que se ha superpuesto un punto sobre cada dato diario. Los dos datos imputados aparecen como puntos rojos.\n\ndinero_imputado <- na.interp(dinero)\n\nautoplot(dinero, \n         xlab = \"\", \n         ylab = \"Euros\",\n         colour = \"black\") +\n  geom_point() +\n  geom_point(aes(x = time(dinero)[is.na(dinero)][1], \n                 y = dinero_imputado[is.na(dinero)][1]),\n             colour = \"red\") +\n  geom_point(aes(x = time(dinero)[is.na(dinero)][2], \n                 y = dinero_imputado[is.na(dinero)][2]),\n             colour = \"red\") +\n  labs(colour = \"Serie\")\n\n\n\n\nFigura 2: Extracción de dinero de un cajero e imputación de valores perdidos\n\n\n\n\n\n\nImputación de valores atípicos\nEl proceso de imputación de valores atípicos es idéntico al usado para valores perdidos: una vez identificado el valor atípico, se sustituye por otro valor usando un algoritmo de imputación, por ejemplo la interpolación lineal. Sin embargo, previamente hay que identificar los valores atípicos y, de nuevo, hay una amplia literatura al respecto.\nNo siempre es conveniente imputar valores atípicos. Es posible que estos valores nos den información útil con respecto a la serie y permitan mejorar las predicciones. En esta categoría entran los efectos calendario (Semana Santa, días laborables, años bisiestos…), que pueden incorporarse dentro de la metodología ARIMA para mejorar las predicciones.\nCaso diferente son los valores atípicos que surgen por mero error durante la manipulación de los datos o por efectos que no son de calendario y cabe esperar que no tengan lugar durante el periodo de predicción. Imputar estos valores atípicos puede mejorar el proceso de ajuste de la serie y la calidad de las predicciones.\nUn valor atípico es una observación que difiere sustancialmente de la mayoría de las observaciones. El concepto de atípico es subjetivo y, por tanto, su identificación dependerá no solo de la distribución de las observaciones, sino del criterio del investigador.\nAunque existen diferentes técnicas para detectar valores atípicos, una de las más ampliamente difundidas y usadas por su sencillez y por no depender de ninguna hipótesis sobre la distribución de los datos es la prueba de Tukey:\n\nSe calculan el primer y el tercer cuartil de las observaciones, que denominaremos \\(Q_1\\) y \\(Q_2\\) respectivamente. Recuerda que \\(Q_1\\) el el valor que deja por debajo un 25% de las observaciones y \\(Q_3\\) es el valor que deja por debajo el 75% de las observaciones. Es decir, entre \\(Q_1\\) y \\(Q_3\\) están el 50% de las observaciones.\nSe obtiene el rango intercuartil \\(IQR = Q_3 - Q_1\\).\nSe define como valor atípico cualquier valor menor que \\(Q_1 - 3\\cdot IQR\\) o mayor que \\(Q_3 + 3\\cdot IQR\\).\n\nEl valor de \\(3\\) rangos intercuartílicos es arbitrario. De hecho, en la literatura estadística es usual el valor de \\(1.5\\) para identificar valores atípicos (outliers) y el valor de \\(3\\) para identificar valores lejanos (far out). Aquí, aunque usaremos el valor de \\(3\\) rangos intercuartilicos, hablaremos de valores atípicos.\nSi la serie sigue una distribución normal, la prueba de Tukey es equivalente a considerar atípico cualquier valor que se aleje de la media 4.7 veces la desviación típica. Como ves, el criterio usado es muy conservador. Es decir, solo valores muy alejados de la distribución de la serie se considerarán valores atípicos.\nPara series temporales, la identificación de los valores atípicos se hace a partir del residuo que se obtiene tras la descomposición de la serie.\nVamos a usar la función tsoutliers del paquete forecast para identificar los valores atípicos y proponer un posible valor de imputación. Esta función usa la prueba de Tukey arriba definida para identificar los valores atípicos3 y usa la función na.interp para obtener los valores de imputación.\nVeamos la aplicación de esta función en la serie de exportaciones.\n\ntsoutliers(exportaciones)\n\n$index\n[1]  99 112 157\n\n$replacements\n[1] 1098.898 1124.781 1386.168\n\n\nLa función tsoutliers identifica tres valores atípicos en las posiciones 99, 112 y 157 de la serie. La observación 99 corresponde a marzo de 2007, muy visible en el gráfico, y las otras dos a febrero de 2008 y enero de 2012. Los valores originales para estas fechas eran\n\nexportaciones[c(99, 112, 157)]\n\n[1] 1893.880 1417.258 1773.067\n\n\nObserva que esta función solo identifica los valores atípicos, pero no los sustituye por los valores potenciales de reemplazamiento.\nEl paquete forecast dispone de la función tsclean que hace todo en uno: imputa los valores perdidos, e identifica e imputa los valores atípicos. Vemos su efecto en la serie de exportaciones en la Figura 3. Los puntos rojos muestran los tres valores identificados como atípicos (panel superior) y sus valores imputados (panel inferior).\n\nexportaciones_imputada <- tsclean(exportaciones)\n\nautoplot(exportaciones, main = \"\", xlab = \"\", ylab = \"Millones de euros\")\nautoplot(exportaciones_imputada, main = \"\", xlab = \"\", ylab = \"Millones de euros\")\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n\n\n(b) Serie con valores atípicos imputados\n\n\n\n\nFigura 3: Imputación de valores atípicos en la serie Exportaciones\n\n\nPara finalizar, recordemos que los modelos ARIMA pueden predecir perfectamente series con valores perdidos y pueden usar la intervención para incorporar los valores atípicos. Es decir, no es necesario limpiar la serie antes de la aplicación de estos modelos. Ahora bien, si deseamos aplicar un método de alisado para predecir, es necesario imputar los valores perdidos y conveniente hacer lo mismo con los valores atípicos. Hay que imputar los primeros porque los métodos de alisado son procesos iterativos y se falta un dato, el proceso se detiene; también hay que imputar los segundos porque los valores atípicos pueden distorsionar las predicciones.\n\n\n\n\n\nFootnotes\n\n\nLos métodos sencillos de previsión y los modelos ARIMA funcionan perfectamente con valores perdidos, pero la función ets para Alisado Exponencial o las funciones stlf y tbats, para trabajar con series con más de una componente estacional, no funcionan si hay valores perdidos.↩︎\nEl nombre de la función na.interp viene de unir na de not available (valor perdido) e interp de interpolación.↩︎\nPuedes leer más detalles sobre el proceso de identificación de valores atípicos usado por la función tsoutlier pinchando aquí↩︎"
  },
  {
    "objectID": "03-08-Ejemplo2.html#tendencia",
    "href": "03-08-Ejemplo2.html#tendencia",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.1 Tendencia",
    "text": "3.1 Tendencia\nHemos obtenido la serie anual pernoctaciones, que presentamos en la Figura 3. Se confirma la tendencia decreciente de la primera década, aunque con un comportamiento muy irregular. En el año 2000 el número de pernoctaciones alcanzaba los 234 millones al año, mientras que en 2009 toco fondo con 200 millones. En los años siguientes se produce una recuperación de las pernoctaciones (a pesar de la crisis económica), superando rápidamente los niveles de principios de siglo y alcanzado a finales de la segunda década los 300 millones de pernoctaciones, a pesar de que en los dos últimos años disponibles las pernoctaciones han vuelto a caer.\n\nautoplot(CasosAnual/1000000,\n         xlab = \"\",\n         ylab = \"Noches (millones)\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(2000, 2020, 2)) \n\n\n\n\nFigura 3: Pernoctaciones"
  },
  {
    "objectID": "03-08-Ejemplo2.html#estacionalidad",
    "href": "03-08-Ejemplo2.html#estacionalidad",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.2 Estacionalidad",
    "text": "3.2 Estacionalidad\nVeamos ahora como varían las pernoctaciones de los turistas extranjeros en España según el mes del año.\n\nggmonthplot(Pernoctaciones, \n             polar=TRUE,\n             xlab = \"\",\n             ylab = \"\",\n             main = \"\") +\n  guides(colour=FALSE)\n\n\n\n\nFigura 4: Gráfico estacional: pernoctaciones\n\n\n\n\nCada subserie en la Figura 4 vuelve a mostrar la evolución de la tendencia durante el periodo de análisis. Respecto de la estacionalidad, se aprecia que el principal determinante es la temperatura –y los movimientos vacacionales asociados a ella– puesto que el número de pernoctaciones aumenta progresivamente desde enero a agosto para luego caer bruscamente de septiembre a diciembre. También cabría esperar un efecto días del mes y observar más pernoctaciones en los meses de 31 días que en los de 30, pero el efecto de la temperatura es tan dominante que anula cualquier otro efecto."
  },
  {
    "objectID": "03-05-Tema5.html#las-funciones-ets-y-forecast",
    "href": "03-05-Tema5.html#las-funciones-ets-y-forecast",
    "title": "Técnicas de Alisado Exponencial",
    "section": "3.1 Las funciones ets y forecast",
    "text": "3.1 Las funciones ets y forecast\nPodemos estimar cualquiera de los treinta modelos usando la función ets del paquete forecast.\n\nEl tipo de modelo en ets se especifica con el argumento model, un código de tres letras indicando el tipo de Error, Tendencia y eStacionalidad (ETS). Por ejemplo, model = \"ANN\" indica un modelo con error aditivo, sin tendencia ni estacionalidad, es decir, el alisado exponencial simple; model = \"AAN\" indica un modelo con error aditivo, pendiente aditiva, pero sin estacionalidad, el alisado exponencial de Holt. El alisado exponencial de Holt-Winters multiplicativo sería model = \"AAM\".\nSi se desea incluir amortiguamiento, hay que añadir el argumento damped = TRUE.\nPor defecto ets no considera modelos con tendencia multiplicativa (últimas dos líneas de la Tabla 1). Debes fijar el parámetro allow.multiplicative.trend=TRUE para contemplar esta opción.\n\nA diferencia de las funciones vistas en el Tema 3 (naive, meanf, rwf y snaive), la función ets solo estima los modelos, pero no produce predicciones. Para ello habrá que usar la función forecast sobre un modelo estimado con ets. El principal argumento de esta función es h que especifica el horizonte temporal de predicción. También puedes usar level para fijar el nivel de confianza del intervalo de predicción.\nMira la ayuda de R para ver una explicación detallada de los argumentos de estas las funciones ets y forecast."
  },
  {
    "objectID": "03-05-Tema5.html#definición",
    "href": "03-05-Tema5.html#definición",
    "title": "Técnicas de Alisado Exponencial",
    "section": "4.1 Definición",
    "text": "4.1 Definición\nEl alisado exponencial simple es adecuado para una serie estacionaria y sin estacionalidad. Es decir, una serie que se mueve alrededor de un nivel constante.\nLa ecuación de la predicción intramuestral es\n\\[\\widehat{y}_{t+1} = \\alpha y_t + \\alpha (1-\\alpha) y_{t-1} + \\alpha (1-\\alpha)^2 y_{t-2} + \\alpha (1-\\alpha)^3 y_{t-3} + \\ldots =  \\alpha y_t + (1-\\alpha)\\widehat{y}_{t},\\] donde \\(0 \\leq \\alpha \\leq 1\\) es el parámetro de suavizado. La primera predicción extramuestral queda\n\\[\\widehat{y}_{T+1}=\\alpha y_T + (1-\\alpha)\\widehat{y}_{T}\\] y para las restantes\n\\[\\widehat{y}_{T+h} = \\widehat{y}_{T+1}.\\]"
  },
  {
    "objectID": "03-05-Tema5.html#formulas-interactivas-de-sus-componentes",
    "href": "03-05-Tema5.html#formulas-interactivas-de-sus-componentes",
    "title": "Técnicas de Alisado Exponencial",
    "section": "4.2 Formulas interactivas de sus componentes",
    "text": "4.2 Formulas interactivas de sus componentes\nEn el alisado exponencial simple solo hay una componente, el nivel \\(l_t\\).\n\nLa ecuación recursiva de suavizado es \\(l_t=\\alpha y_t + (1-\\alpha)l_{t-1}\\)\nLa ecuación de predicción intramuestral es \\(\\widehat{y}_{t+1} = l_t\\)\nLa ecuación de predicción extramuestral es \\(\\widehat{y}_{T+h} = \\widehat{y}_{T+1} = l_T\\)\n\nDos estimaciones razonables de \\(l_t\\), el nivel de la serie en el periodo \\(t\\), son el valor observado para la serie en ese periodo \\(y_t\\) y el nivel del periodo previo \\(l_{t-1}\\). La estimación final de \\(l_t\\) es una media ponderada de ambas y esta estimación final es la previsión de la serie para el periodo siguiente."
  },
  {
    "objectID": "03-05-Tema5.html#estimación-de-los-parámetros-del-modelo",
    "href": "03-05-Tema5.html#estimación-de-los-parámetros-del-modelo",
    "title": "Técnicas de Alisado Exponencial",
    "section": "4.3 Estimación de los parámetros del modelo",
    "text": "4.3 Estimación de los parámetros del modelo\nDado el proceso iterativo para el cálculo de \\(l_t\\) se necesita un valor inicial de arranque \\(l_0\\). Cada programa estadístico usa su propio método para obtener \\(l_0\\).\nRespecto de \\(\\alpha\\), usualmente se estima el valor optimo según un criterio de precisión de la predicción. El parámetro \\(\\alpha\\) se puede interpretar:\n\nSi \\(\\alpha = 1\\) se tiene el método ingenuo I (\\(\\widehat{y}_{t+1}=y_t\\)), óptimo cuando el nivel de la serie varía constantemente en el tiempo.\nSi \\(\\alpha = 0\\) se tiene \\(\\widehat{y}_{t} =l_0\\), óptimo cuando el nivel permanece constante en el tiempo.\n\nEn concreto, ets estima por defecto los parámetros \\(\\alpha\\) y \\(l_0\\) maximizando la función de verosimilitud. Esta búsqueda está restringida a \\(0 < \\alpha < 1\\). Es decir el parámetro \\(\\alpha\\) nunca puede ser 0 o 1 y en la práctica sus valores limite son 0.0001 y 0.9999."
  },
  {
    "objectID": "03-05-Tema5.html#ejemplo",
    "href": "03-05-Tema5.html#ejemplo",
    "title": "Técnicas de Alisado Exponencial",
    "section": "4.4 Ejemplo",
    "text": "4.4 Ejemplo\nVamos a usar el método de alisado exponencial simple para predecir la serie Demanda eléctrica semanal desde la primera semana de febrero (semana 6 del año) hasta la última semana de mayo (semana 22 del año), en total 17 semanas. Usaremos para ello la función ets con `model = “ANN”.\n\nelectricidad <- read.csv2(\"./series/Consumo electrico.csv\", header = TRUE)\nelectricidad <- ts(electricidad[, 2],\n                   start = c(1, 5),\n                   frequency = 7)\n\n#Nos quedamos con los meses de febrero a mayo.\nelectricidad <- window(electricidad, start = c(6, 1), end = c(22, 7)) \n#Creamos la serie de consumo semanal\nelectricidadSemanal <- aggregate(electricidad, FUN = sum) \n\nelectricidadEts <- ets(electricidadSemanal, \n                       model = \"ANN\")\nsummary(electricidadEts)\n\nETS(A,N,N) \n\nCall:\n ets(y = electricidadSemanal, model = \"ANN\") \n\n  Smoothing parameters:\n    alpha = 0.5839 \n\n  Initial states:\n    l = 5034.1607 \n\n  sigma:  158.4303\n\n     AIC     AICc      BIC \n224.2576 226.1037 226.7572 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -40.34035 148.8194 82.91992 -0.9090506 1.789911 0.8506723\n                   ACF1\nTraining set 0.01332464\n\n\nVeamos la salida en detalle:\n\nEl valor de \\(\\alpha\\) que optimiza el criterio usado para medir la calidad del ajuste es \\(\\alpha =\\) 0.58, un valor intermedio, ni cercano a 0 ni cercano a 1. Es decir, la serie Demanda eléctrica cambia de nivel poco a poco.\nEl valor de arranque \\(l_0\\) óptimo es 5034.16.\nsigma es la desviación típica del error (aditivo) de predicción. Se diferencia de RMSE en el denominador. Para calcular sigma en lugar de dividir por \\(T\\) se divide por \\(T\\) menos el número de parámetros estimados, en este caso 3: \\(l_0\\), \\(\\alpha\\) y sigma. Sí, sigma, se considerará siempre otro parámetro estimado.\nLa calidad del ajuste es bueno, como evidencia el error porcentual medio del 1.8%.\n\nEn el objeto electricidadEts la matriz electricidadEts$states guarda todos los valores del nivel obtenidos con la ecuación recursiva, incluido el valor de arranque, así que es una matriz con \\(T+1\\) filas (18 en el ejemplo). Puedes ver el valor de \\(l_{22}\\) de la última semana de mayo de 2021 (semana 22 del año) en su última fila, que vale 4633.76.\n\ntail(electricidadEts$states, 1)\n\nTime Series:\nStart = 22 \nEnd = 22 \nFrequency = 1 \n            l\n[1,] 4633.761\n\n\nAsí, la predicción para la primera semana de junio es \\(\\widehat{y}_{23}=l_{22}=\\) 4633.76. Igualmente \\(\\widehat{y}_{24}=l_{22}=\\) 4633.76. Es decir, todas las previsiones son iguales a \\(l_{22}\\).\nMediante la función forecast podemos predecir los casos de Demanda eléctrica para las próximas 5 semanas. Por tratarse de un modelo sin pendiente ni estacionalidad, la predicción es constante en el tiempo. Recuerda que \\(\\widehat{y}_{T+h} = l_T\\).\n\nelectricidadf <- forecast(electricidadEts,\n                          h = 5, \n                          level = 95)\nelectricidadf\n\n   Point Forecast    Lo 95    Hi 95\n23       4633.761 4323.243 4944.279\n24       4633.761 4274.192 4993.330\n25       4633.761 4231.072 5036.450\n26       4633.761 4192.142 5075.380\n27       4633.761 4156.376 5111.146\n\n\nLa Figura 1 muestra la serie Demanda eléctrica, las previsiones extramuestrales que son constantes y el intervalo de confianza. Conforme aumentamos el horizonte de predicción, el intervalo de confianza es más amplio como reflejo de la mayor incertidumbre en la predicción.\n\nautoplot(electricidadf,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\nFigura 1: Consumo semanal de electricidad (febrero a mayo 2021) y predicción con alisado simple"
  },
  {
    "objectID": "03-05-Tema5.html#formulas-interactivas-de-sus-componentes-1",
    "href": "03-05-Tema5.html#formulas-interactivas-de-sus-componentes-1",
    "title": "Técnicas de Alisado Exponencial",
    "section": "5.1 Formulas interactivas de sus componentes",
    "text": "5.1 Formulas interactivas de sus componentes\nLas ecuaciones recursivas son\n\\[\\begin{aligned}\nl_t & =\\alpha y_t + (1-\\alpha)(l_{t-1}+b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)b_{t-1}\n\\end{aligned}\\]\nLa ecuación de la predicción intramuestral a un periodo vista es\n\\[\\widehat{y}_{t+1} = l_t + b_t,\\] de forma que la ecuación de predicción extramuestral es \\[\\widehat{y}_{T+h}=l_T + h b_T.\\]\nDos estimaciones razonables del nivel de la serie en el periodo \\(t\\) son el valor observado para la serie en ese periodo \\(y_t\\), y una estimación del nivel del periodo \\(t\\) realizada desde el periodo \\(t-1\\): \\(l_{t-1} + b_{t-1}\\). Por otro lado, dos estimaciones razonables de la pendiente de la serie en el periodo \\(t\\) son el cambio de nivel de \\(t-1\\) a \\(t\\) (el último observado) \\(l_t-l_{t-1}\\), y el valor de la pendiente en el periodo previo, \\(b_{t-1}\\). En ambos casos, nivel y pendiente, la estimación final es una media ponderada, parametrizada por \\(0 \\leq \\alpha, \\: \\beta \\leq 1\\).\nObserva que el método ingenuo II es un caso concreto de Alisado de Holt. Si hacemos \\(\\alpha=\\beta = 1,\\) queda \\(l_t=y_t\\) y \\(b_t=y_t-y_{t-1}\\), por tanto\n\\[\\widehat{y}_{t+1}=l_t + b_t = y_t + (y_t - y_{t-1}).\\]\ny\n\\[\\widehat{y}_{T+h}=l_T + h \\cdot b_T = y_T + h(y_T - y_{T-1}).\\]"
  },
  {
    "objectID": "03-05-Tema5.html#estimación-de-los-parámetros-del-modelo-1",
    "href": "03-05-Tema5.html#estimación-de-los-parámetros-del-modelo-1",
    "title": "Técnicas de Alisado Exponencial",
    "section": "5.2 Estimación de los parámetros del modelo",
    "text": "5.2 Estimación de los parámetros del modelo\nPara aplicar este método es necesario estimar unos valores iniciales \\(l_0\\) y \\(b_0\\) de las ecuaciones recursivas e identificar los valores más adecuados de los parámetros \\(\\alpha\\) y \\(\\beta\\).\nLa función ets estima por defecto los parámetros \\(\\alpha\\), \\(\\beta\\), \\(l_0\\) y \\(b_0\\) maximizando la función de verosimilitud. En este caso la búsqueda está restringida a \\(0 < \\beta < \\alpha < 1\\). Por tanto, \\(\\alpha\\) y \\(\\beta\\) nunca pueden ser 0 o 1 y en la práctica sus valores limite son 0.0001 y 0.9999.\nLa interpretación del parámetro \\(\\alpha\\) es similar al caso del alisado exponencial simple.\nInterpretación del parámetro \\(\\beta\\):\n\nSi \\(\\beta = 1\\), \\(b_t = l_t - l_{t-1}\\), la pendiente se actualiza constantemente porque varía periodo a periodo Puede ser un indicador de mal ajuste (tendencia no lineal o pendiente no aditiva).\nSi \\(\\beta = 0\\), \\(b_t = b_{t-1}= \\ldots = b_0\\), la pendiente se mantiene constante en el tiempo."
  },
  {
    "objectID": "03-05-Tema5.html#ejemplo-1",
    "href": "03-05-Tema5.html#ejemplo-1",
    "title": "Técnicas de Alisado Exponencial",
    "section": "5.3 Ejemplo",
    "text": "5.3 Ejemplo\nVamos a usar el método de alisado de Holt para predecir la serie Libros. Usaremos para ello la función ets con los argumentos model = \"AAN\" y damped = FALSE. El segundo argumento previene el uso de tendencia amortiguada que veremos en el siguiente epígrafe.\n\nlibros <- read.csv2(\"./series/libros.csv\", header = TRUE)\nlibros <- ts(libros[, 2], \n             start = 1993, \n             frequency  = 1)\n\nlibrosEts <- ets(libros, \n                 model = \"AAN\",\n                 damped = FALSE)\nsummary(librosEts)\n\nETS(A,A,N) \n\nCall:\n ets(y = libros, model = \"AAN\", damped = FALSE) \n\n  Smoothing parameters:\n    alpha = 0.9461 \n    beta  = 0.0001 \n\n  Initial states:\n    l = 43015.3558 \n    b = 866.6661 \n\n  sigma:  6235.705\n\n     AIC     AICc      BIC \n566.5129 569.3700 572.9921 \n\nTraining set error measures:\n                    ME     RMSE     MAE        MPE    MAPE      MASE\nTraining set -91.75606 5755.295 4179.61 -0.3817069 6.68946 0.9237963\n                    ACF1\nTraining set 0.004201274\n\n\nLos valores óptimos de los cuatro parámetros son \\(\\alpha=\\) 0.95, \\(\\beta=\\) 0, \\(l_0 =\\) 43015.36 y \\(b_0 =\\) 866.67. Observa que \\(\\alpha\\) es prácticamente 1 y que \\(\\beta\\) es cero. Si aplicamos estos valores de los parámetros a las ecuaciones recursivas y la predicción extramuestral, obtenemos \\(y_{T+h}=y_T + hb_0\\): la predicción es el último valor observado más \\(h\\) veces la primera pendiente estimada. La calidad de las predicciones es razonable, con un error porcentual medio del 6.7%.\n\n\n\n\n\n\nParámetros estimados\n\n\n\n¿Cuántos parámetros se han estimado (y la respuesta no es 4)? ¿Cuál es el denominador en el cálculo de RMSE y de sigma?\n\n\nEn el objeto librosEts la matriz librosEts$states guarda todos los valores obtenidos con las ecuaciones recursivas, en este caso el nivel y la pendiente, incluidos los valores de arranque. Puedes ver los valores de \\(l_{2019}\\) y \\(b_{2019}\\) en su última fila, que valen respectivamente 64091.02, 866.42.\n\ntail(librosEts$states, 1)\n\nTime Series:\nStart = 2019 \nEnd = 2019 \nFrequency = 1 \n            l        b\n2019 64091.02 866.4181\n\n\nAsí, la predicción para \\(2020\\) es \\(\\widehat{y}_{2020}=l_{2019} + b_{2019}=\\) 64091.02 \\(+\\) 866.42 \\(=\\) 64957.43. Igualmente \\(\\widehat{y}_{2021}=l_{2019} + 2\\cdot b_{2019}=\\) 65823.85. Es decir, el incremento entre previsiones es constante e igual a \\(b_{2019}\\) que, por ser \\(\\beta\\) prácticamente nulo, coincide con \\(b_0\\).\n\nlibrosf <- forecast(librosEts,\n                    h = 5, \n                    level = 95)\nlibrosf\n\n     Point Forecast    Lo 95    Hi 95\n2020       64957.43 52735.68 77179.19\n2021       65823.85 48998.58 82649.13\n2022       66690.27 46273.69 87106.85\n2023       67556.69 44091.57 91021.81\n2024       68423.11 42261.77 94584.44\n\n\nLa Figura 2 muestra la serie Libros y las previsiones extramuestrales que muestran una ligera tendencia creciente.\n\nautoplot(librosf,\n         xlab = \"\",\n         ylab = \"Títulos\",\n         main = \"\")\n\n\n\n\nFigura 2: Libros y predicción con alisado de Holt"
  },
  {
    "objectID": "03-05-Tema5.html#formulas-interactivas-de-sus-componentes-2",
    "href": "03-05-Tema5.html#formulas-interactivas-de-sus-componentes-2",
    "title": "Técnicas de Alisado Exponencial",
    "section": "6.1 Formulas interactivas de sus componentes",
    "text": "6.1 Formulas interactivas de sus componentes\nLas ecuaciones recursivas son\n\\[\\begin{aligned}\nl_t & =\\alpha y_t + (1-\\alpha)(l_{t-1}+\\phi b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)\\phi b_{t-1}\n\\end{aligned}\\]\nLa ecuación de la predicción intramuestral a un periodo vista es \\[\\widehat{y}_{t+1} = l_t + \\phi b_t,\\] de forma que la ecuación de predicción extramuestral es \\[\\widehat{y}_{T+h}=l_T + (\\phi + \\phi^2 + \\ldots + \\phi^h) b_T.\\]\nComo observas, se ha añadido un nuevo parámetro \\(\\phi\\), que acompaña siempre a la pendiente \\(b_t\\). Si \\(\\phi = 1\\), se tiene el alisado de Holt y si \\(\\phi = 0\\), se tiene el alisado simple. Para valores entre \\(0\\) y \\(1\\) en el corto plazo las predicciones tienen pendiente y en el largo plazo se hacen constantes e iguales a \\(l_T + \\phi b_T/(1 - \\phi)\\).\nPor razones prácticas el rango de búsqueda de \\(\\phi\\) queda en el intervalo \\([0.8, 0.98]\\). Si el valor óptimo de \\(\\phi\\) fuera su valor máximo de \\(0.98\\) o muy cercano a este valor, cabría plantearse si no sería más adecuado un modelo sin amortiguamiento."
  },
  {
    "objectID": "03-05-Tema5.html#ejemplo-2",
    "href": "03-05-Tema5.html#ejemplo-2",
    "title": "Técnicas de Alisado Exponencial",
    "section": "6.2 Ejemplo",
    "text": "6.2 Ejemplo\nVamos a usar el método de alisado con amortiguamiento para predecir, una vez más, la serie Libros añadiendo a la función ets el argumento damped = TRUE. En este caso, para ver el efecto del amortiguamiento vamos a pedir un horizonte temporal más largo.\n\nlibrosEtsD <- ets(libros, \n                  model = \"AAN\", \n                  damped = TRUE)\nsummary(librosEtsD)\n\nETS(A,Ad,N) \n\nCall:\n ets(y = libros, model = \"AAN\", damped = TRUE) \n\n  Smoothing parameters:\n    alpha = 0.8971 \n    beta  = 0.0001 \n    phi   = 0.9083 \n\n  Initial states:\n    l = 41396.6242 \n    b = 2881.012 \n\n  sigma:  6239.024\n\n     AIC     AICc      BIC \n567.3414 571.5414 575.1164 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -159.4816 5631.786 3984.248 -0.6273468 6.314471 0.8806165\n                   ACF1\nTraining set 0.01091024\n\n\nEl valor óptimo del parámetro \\(\\phi\\) es \\(0.91\\) y el error porcentual 6.3%, algo inferior al obtenido con el alisado de Holt sin amortiguamiento. La inclusión de un nuevo parámetro en el modelo mejora el ajuste.\n\n\n\n\n\n\nParámetros estimados\n\n\n\n¿Cuántos parámetros se han estimado en este caso? ¿Cuáles?\n\n\nLa Figura 3 muestra la serie Libros, su estimación (intramuestral) y las predicciones a 15 años vista. Observa que la pendiente de las previsiones se amortigua en el tiempo, de forma que al principio las previsiones crecen más rápidamente que en los últimos años.\n\nlibrosfD <- forecast(librosEtsD, \n                     h = 15,\n                     level = 95)\nlibrosfD\n\n     Point Forecast    Lo 95     Hi 95\n2020       64148.96 51920.70  76377.22\n2021       64325.91 47897.02  80754.81\n2022       64486.65 44730.42  84242.88\n2023       64632.65 42033.33  87231.96\n2024       64765.27 39642.22  89888.32\n2025       64885.73 37469.99  92301.47\n2026       64995.15 35463.92  94526.37\n2027       65094.54 33589.35  96599.73\n2028       65184.82 31822.06  98547.58\n2029       65266.82 30144.45 100389.20\n2030       65341.31 28543.22 102139.40\n2031       65408.97 27008.10 103809.84\n2032       65470.43 25530.94 105409.92\n2033       65526.26 24105.17 106947.34\n2034       65576.97 22725.40 108428.54\n\nautoplot(librosfD,\n         xlab = \"\",\n         ylab = \"Títulos\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\nFigura 3: Libros y predicción con alisado exponencial con amortiguamiento"
  },
  {
    "objectID": "03-05-Tema5.html#alisado-de-holt-winters-aditivo-a-a-a",
    "href": "03-05-Tema5.html#alisado-de-holt-winters-aditivo-a-a-a",
    "title": "Técnicas de Alisado Exponencial",
    "section": "7.1 Alisado de Holt-Winters aditivo (A, A, A)",
    "text": "7.1 Alisado de Holt-Winters aditivo (A, A, A)\nLas ecuaciones recursivas de actualización son:\n\\[\\begin{aligned}\nl_t & =\\alpha (y_t - s_{t-m} ) + (1-\\alpha)(l_{t-1}+b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)b_{t-1} \\\\\ns_t & =\\gamma (y_t - l_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m}\n\\end{aligned}\\]\ncon \\(0 \\leq \\alpha, \\beta, \\gamma \\leq 1\\).\nLa ecuación de la predicción intramuestral a un periodo vista es \\[\\widehat{y}_{t+1}  = l_t + b_t + s_{t+1-m},\\] de forma que la ecuación de predicción extramuestral es: \\[\\widehat{y}_{T+h}=l_T + h b_T + s_{T+h - m(k+1)},\\] con \\(k = \\lfloor(h-1)/m\\rfloor\\)."
  },
  {
    "objectID": "03-05-Tema5.html#alisado-de-holt-winters-multiplicativo-m-a-m",
    "href": "03-05-Tema5.html#alisado-de-holt-winters-multiplicativo-m-a-m",
    "title": "Técnicas de Alisado Exponencial",
    "section": "7.2 Alisado de Holt-Winters multiplicativo (M, A, M)",
    "text": "7.2 Alisado de Holt-Winters multiplicativo (M, A, M)\nLas ecuaciones recursivas de actualización son:\n\\[\\begin{aligned}\nl_t & =\\alpha \\frac{y_t}{s_{t-m}} + (1-\\alpha)(l_{t-1}+b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)b_{t-1} \\\\\ns_t & =\\gamma \\frac{y_t}{l_{t-1} + b_{t-1}} + (1 - \\gamma)s_{t-m}\n\\end{aligned}\\]\nLa ecuación de la predicción intramuestral a un periodo vista es \\[\\widehat{y}_{t+1}  = (l_t + b_t)s_{t+1-m},\\] de forma que la ecuación de predicción extramuestral es: \\[\\widehat{y}_{T+h}=(l_T + h b_T)s_{T+h - m(k+1)}.\\]"
  },
  {
    "objectID": "03-05-Tema5.html#ejemplo-con-demanda-electrica",
    "href": "03-05-Tema5.html#ejemplo-con-demanda-electrica",
    "title": "Técnicas de Alisado Exponencial",
    "section": "7.3 Ejemplo con Demanda electrica",
    "text": "7.3 Ejemplo con Demanda electrica\nVamos a usar el método de Holt-Winters Aditivo para predecir la serie Demanda eléctrica, que presentaba un esquema aditivo. Para ello usaremos la función ets con el argumento model = \"AAA\" (y damped = FALSE). Vamos a considerar la serie desde el 1 de febrero (lunes) hasta el 30 de mayo (domingo), 17 semanas, y pedir una previsión a dos semanas vista.\nEn el proceso de estimación, el parámetro \\(\\gamma\\) que gobierna la componente estacional está restringido a \\(0 < \\gamma < 1 - \\alpha\\).\n\nelectricidad <- read.csv2(\"./series/Consumo electrico.csv\", header = TRUE)\nelectricidad <- ts(electricidad[, 2],\n                   start = c(1, 5),\n                   frequency = 7)\n#Nos quedamos con los meses de febrero a mayo.\nelectricidad <- window(electricidad, start = c(6, 1), end = c(22, 7)) \n\nelectricidadEts <- ets(electricidad, \n                       model = \"AAA\", \n                       damped = FALSE)\nsummary(electricidadEts)\n\nETS(A,A,A) \n\nCall:\n ets(y = electricidad, model = \"AAA\", damped = FALSE) \n\n  Smoothing parameters:\n    alpha = 0.8355 \n    beta  = 0.0001 \n    gamma = 0.0001 \n\n  Initial states:\n    l = 741.55 \n    b = 0.0601 \n    s = -88.686 -47.5536 19.731 30.4514 35.9361 36.6726\n           13.4485\n\n  sigma:  17.5704\n\n     AIC     AICc      BIC \n1263.333 1266.276 1296.682 \n\nTraining set error measures:\n                     ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.7940567 16.73864 10.45847 -0.1438659 1.587029 0.5252875\n                   ACF1\nTraining set 0.03820125\n\n\nLos valores óptimos de los parámetros son \\(\\alpha=\\) 0.84, \\(\\beta=\\) 0 y \\(\\gamma=\\) 0. Los valores nulos para \\(\\beta\\) y \\(\\gamma\\) indican que ambas, la pendiente y la estacionalidad, permanecen constantes en el tiempo. La calidad de las predicciones es notable, con un error porcentual medio del 1.6%.\nLos últimos valores de las componentes son\n\nTT <- nrow(electricidadEts$states)\nelectricidadEts$states[TT,]\n\n\n\n      l       b      s1      s2      s3      s4      s5      s6      s7 \n668.923   0.051 -88.685 -47.554  19.729  30.452  35.934  36.667  13.448 \n\n\nComo el último dato de la serie es domingo 30 de mayo, los valores del nivel \\(l\\) y la pendiente \\(b\\) mostrados corresponden a ese día. Sin embargo, la componente estacional tiene un orden muy peculiar: s1 es el valor estacional para domingo (día del último dato), s2 el de sábado, s3 de viernes, hasta s7 que sería lunes. Podemos reproducir las predicciones para los próximos 7 días, 31 de mayo a 6 de junio (ojo, el etiquetado de la salida no tiene sentido):\n\nelectricidadEts$states[TT, 1] + (1:7)*electricidadEts$states[TT, 2] + \n  electricidadEts$states[TT, 9:3]\n\n      s7       s6       s5       s4       s3       s2       s1 \n682.4217 705.6918 705.0094 699.5780 688.9063 621.6741 580.5928 \n\n\nO mejor usar la función forecast para obtener las predicciones y sus intervalos a dos semanas vista.\n\nelectricidadf <- forecast(electricidadEts,\n                          h = 14, \n                          level = 95)\nelectricidadf\n\n         Point Forecast    Lo 95    Hi 95\n23.00000       682.4217 647.9844 716.8591\n23.14286       705.6918 660.8134 750.5702\n23.28571       705.0094 651.6952 758.3236\n23.42857       699.5780 638.9897 760.1663\n23.57143       688.9063 621.8267 755.9859\n23.71429       621.6741 548.6768 694.6714\n23.85714       580.5928 502.1216 659.0640\n24.00000       682.7765 599.1867 766.3664\n24.14286       706.0466 617.6340 794.4592\n24.28571       705.3642 612.3776 798.3508\n24.42857       699.9328 602.5858 797.2798\n24.57143       689.2611 587.7399 790.7823\n24.71429       622.0289 516.4975 727.5603\n24.85714       580.9476 471.5520 690.3432\n\n\nLa Figura 4 muestra la serie Demanda eléctrica y las previsiones extramuestrales.\n\nautoplot(electricidadf,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\nFigura 4: Demanda eléctrica y predicción con alisado de Holt-Winters aditivo"
  },
  {
    "objectID": "03-05-Tema5.html#ejemplo-con-nacimientos",
    "href": "03-05-Tema5.html#ejemplo-con-nacimientos",
    "title": "Técnicas de Alisado Exponencial",
    "section": "7.4 Ejemplo con Nacimientos",
    "text": "7.4 Ejemplo con Nacimientos\nVamos a usar el método de Holt-Winters multiplicativo para predecir la serie Nacimientos, que presentaba un esquema multiplicativo. En este caso usaremos el argumento model = \"MAM\" 1. Vamos a considerar la serie Nacimientos desde enero de 2000 y pedir una previsión a dos años vista.\n\nnacimientos <- read.csv2(\"./series/nacimientos.csv\", header = TRUE)\nnacimientos <- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  frequency = 12)\n\nnacimientosb <- window(nacimientos, start = 2000)\nnacimientosbEts <- ets(nacimientosb, \n                       model = \"MAM\", \n                       damped = FALSE)\nsummary(nacimientosbEts)\n\nETS(M,A,M) \n\nCall:\n ets(y = nacimientosb, model = \"MAM\", damped = FALSE) \n\n  Smoothing parameters:\n    alpha = 0.3187 \n    beta  = 0.0541 \n    gamma = 0.1828 \n\n  Initial states:\n    l = 33016.8308 \n    b = 134.4179 \n    s = 0.9976 0.9806 1.0479 1.0246 1.0248 1.0326\n           0.9789 1.0242 0.9739 1.0001 0.919 0.996\n\n  sigma:  0.0236\n\n     AIC     AICc      BIC \n4575.865 4578.621 4635.035 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -13.31703 860.7476 675.2864 -0.0726126 1.823373 0.4868175\n                  ACF1\nTraining set 0.0980623\n\n\nLos valores óptimos de los parámetros son \\(\\alpha=\\) 0.32, \\(\\beta=\\) 0.05 y \\(\\gamma=\\) 0.18. Los valores tan bajos para \\(\\beta\\) y \\(\\gamma\\) indican que ambas, la pendiente y la estacionalidad, modifican su valor muy lentamente. La calidad de las predicciones es notable, con un error porcentual medio del 1.8%.\nLos últimos valores de las componentes son\n\nTT <- nrow(nacimientosbEts$states)\nnacimientosbEts$states[TT,]\n\n\n\n        l         b        s1        s2        s3        s4        s5        s6 \n29462.334   -77.537     0.995     1.000     1.066     1.042     1.046     1.049 \n       s7        s8        s9       s10       s11       s12 \n    0.978     0.999     0.949     0.980     0.896     0.994 \n\n\nComo el último dato de la serie es diciembre de 2019, los valores del nivel \\(l\\) y la pendiente \\(b\\) mostrados corresponden a ese mes. Sin embargo, recuerda que la componente estacional sigue un orden inverso: s1 es el valor estacional para diciembre (mes del último dato), s2 el de noviembre, s3 de octubre, hasta s11 que sería febrero y s12 que es enero.\nPodemos reproducir las predicciones para los primeros 12 meses de enero a diciembre (ojo, el etiquetado de la salida no es correcto) para ver que coinciden con las obtenidas con la función forecast.\n\n(nacimientosbEts$states[TT, 1] + (1:12)*nacimientosbEts$states[TT, 2]) * \n  nacimientosbEts$states[TT, 14:3]\n\n     s12      s11      s10       s9       s8       s7       s6       s5 \n29208.44 26261.08 28659.28 27669.53 29035.26 28350.73 30332.66 30170.34 \n      s4       s3       s2       s1 \n29982.89 30583.87 28610.96 28402.26 \n\n\n\nnacimientosbf <- forecast(nacimientosbEts,\n                          h = 24, \n                          level = 95)\nnacimientosbf\n\n         Point Forecast    Lo 95    Hi 95\nJan 2020       29208.44 27858.19 30558.69\nFeb 2020       26261.08 24965.01 27557.14\nMar 2020       28659.28 27134.81 30183.74\nApr 2020       27669.53 26072.36 29266.71\nMay 2020       29035.26 27208.76 30861.76\nJun 2020       28350.73 26403.10 30298.36\nJul 2020       30332.66 28056.12 32609.20\nAug 2020       30170.34 27698.52 32642.16\nSep 2020       29982.89 27305.72 32660.06\nOct 2020       30583.87 27614.07 33553.67\nNov 2020       28610.96 25597.03 31624.89\nDec 2020       28402.26 25164.92 31639.60\nJan 2021       28286.46 24710.45 31862.47\nFeb 2021       25429.93 21982.37 28877.49\nMar 2021       27749.82 23723.48 31776.16\nApr 2021       26789.14 22637.45 30940.82\nMay 2021       28108.94 23465.20 32752.67\nJun 2021       27443.82 22620.07 32267.58\nJul 2021       29359.74 23879.54 34839.95\nAug 2021       29200.02 23422.41 34977.63\nSep 2021       29015.99 22940.65 35091.33\nOct 2021       29594.92 23048.56 36141.28\nNov 2021       27683.28 21224.32 34142.25\nDec 2021       27478.84 20726.54 34231.15\n\n\nLa Figura 5 muestra la serie Nacimientos y las previsiones extramuestrales.\n\nautoplot(nacimientosbf,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\nFigura 5: Nacimientos y predicción con alisado de Holt-Winters multiplicativo"
  },
  {
    "objectID": "03-05-Tema5.html#criterios-de-optimización",
    "href": "03-05-Tema5.html#criterios-de-optimización",
    "title": "Técnicas de Alisado Exponencial",
    "section": "9.1 Criterios de optimización",
    "text": "9.1 Criterios de optimización\nFijado un modelo, ets estima por defecto sus parámetros maximizando la función de verosimilitud. Esta búsqueda esta restringida a \\(0 < \\beta < \\alpha < 1\\), \\(0 < \\gamma < 1 - \\alpha\\) y \\(0.8 < \\phi < 0.98\\). Es decir, los tres primeros parámetros nunca pueden ser 0 o 1, y en la práctica sus valores límite son 0.0001 y 0.9999.\nPuedes cambiar el criterio de optimización con el argumento opt.crit. Por defecto vale “lik” (de likelihood o verosimilitud), pero si lo fijas a opt.crit = \"mse\" se estiman los parámetros que minimizan el error cuadrático medio. Otra opción interesante es opt.crit = \"amse\" que minimiza la media de los errores cuadráticos medios obtenido sobre las previsiones hasta nmse periodos vista. En este caso usa el argumento nmse para fijar el valor numérico del horizonte temporal."
  },
  {
    "objectID": "03-05-Tema5.html#criterios-de-selección-de-modelos",
    "href": "03-05-Tema5.html#criterios-de-selección-de-modelos",
    "title": "Técnicas de Alisado Exponencial",
    "section": "9.2 Criterios de selección de modelos",
    "text": "9.2 Criterios de selección de modelos\nQueda pendiente saber que criterio se usa para seleccionar el modelo cuando se ofrece esta opción. Esto se hace a partir de un criterio de información entre Akaike (aic), Akaike corregido para pequeñas muestras (aicc) y el Bayesiano (bic). Sus fórmulas son: \\[aic = -2log(L) + 2k\\] \\[aicc = aic + \\frac{k(k+1)}{T-k-1}\\] \\[bic=aic + k(log(T) - 2)\\] donde \\(L\\) es la verosimilitud, \\(T\\) el número de datos y \\(k\\) el de parámetros estimados (incluidos los puntos iniciales de arranque y la desviación típica del error).\nCuanto menor es el criterio de información, mejor modelo. Por defecto se usa Akaike corregido para pequeñas muestras (aicc), pero el argumento ic permite cambiar de criterio.\n\n\n\n\n\n\nUna reflexión sobre los métodos automáticos de selección de modelos\n\n\n\n\n\nCon el comando forecast(ets(nacimientos),h=24) obtenemos una predicción mensual a dos años vista del número de nacimientos en España. Así de simple, solo 31 caracteres. Todo esto gracias a que un algoritmo interno ha estimado los parámetros de múltiples modelos, elegido el mejor modelo de todos y lo ha usado para obtener las predicciones. Podemos afirmar que tenemos las mejores predicciones. Un momento, ¿podemos?\nParémonos a reflexionar sobre lo que hemos hecho o, más bien, lo que el algoritmo ha hecho y a contrastarlo con lo que nosotros queríamos. Por un lado, el algoritmo estima los parámetros de un menú fijo de modelos y para ello usa un criterio de optimización, que por defecto es maximizar la función de verosimilitud; cuando ya tiene estimados todos los modelos, elije el mejor usando el criterio de información de Akaike corregido para muestras pequeñas; y finalmente, nosotros medimos la capacidad predictiva del modelo seleccionado usando el error absoluto porcentual medio. Vaya, resulta que en los procesos de identificación y estimación del mejor modelo se usan dos criterios diferentes, que además no coinciden con nuestro criterio de calidad de las predicciones.\nSi consideramos que la calidad de un modelo viene dada por el error absoluto porcentual medio en las predicciones intramuestrales a un periodo vista (lo que hemos decidido llamar MAPE), ¿no deberíamos estimar los parámetros del modelo usando como criterio la minimización del MAPE?, ¿no deberíamos elegir entre varios modelos aquel que presenta un MAPE menor? De esta forma, en todos los pasos del proceso se usa el mismo criterio, que es, además, el criterio que hemos considerado adecuado para valorar la calidad de las predicciones.\nPero no es esto lo que hacemos.\nNada nos garantiza que el modelo estimado y seleccionado por el algoritmo estime las mejores predicciones posibles. Y por mejores quiero decir que de entre todos los posibles modelos del menú y todos los posibles valores de sus parámetros, el seleccionado sea el que minimiza nuestro criterio de calidad de las predicciones.\nAhora ya podemos dar respuesta a la pregunta del primer párrafo: no, no podemos afirmar que nuestras predicciones sean las mejores.\nAlguien dirá que casi seguro entre las predicciones subóptimas obtenidas por el algoritmo con su extraña mezcla de criterios y las predicciones óptimas de verdad no habrá mucha diferencia. Total, que más da una función de verosimilitud que un criterio de información que una medida del error medio. Pero lo cierto es que no lo sabemos, no tenemos ni idea de la distancia que hay entre lo óptimo y lo sub-óptimo, y si el coste de equivocarme en las predicciones es alto, puede que incluso una pequeña diferencia sea relevante.\nEsta reflexión realizada en el contexto de series temporales y para la función ets es aplicable a todos los casos donde dejamos que un algoritmo ya programado elija el mejor modelo, y se basa en el hecho de que rara vez los criterios de estimación y elección que usan los algoritmos coinciden con el concepto de calidad de ajuste que estamos interesados.\nA pesar de lo aquí expuesto, como es más cómodo (y rápido) tirar de rutinas ya programadas que escribir nuestro propio código, seguiremos trabajando con modelos subóptimos y obteniendo estimaciones subóptimas, pero diciendo que son las mejores."
  },
  {
    "objectID": "03-05-Tema5.html#libros",
    "href": "03-05-Tema5.html#libros",
    "title": "Técnicas de Alisado Exponencial",
    "section": "11.1 Libros",
    "text": "11.1 Libros\n\nIdentificación y estimación del mejor modelo\nSi estimamos el mejor modelo de alisado exponencial para la serie Libros sin ningún tipo de restricción, nos encontramos:\n\nlibrosEts <- ets(libros)\nsummary(librosEts) \n\nETS(M,N,N) \n\nCall:\n ets(y = libros) \n\n  Smoothing parameters:\n    alpha = 0.9999 \n\n  Initial states:\n    l = 40459.0122 \n\n  sigma:  0.0902\n\n     AIC     AICc      BIC \n557.0738 558.1173 560.9613 \n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE     MAPE      MASE        ACF1\nTraining set 877.6729 5798.394 4367.896 1.318359 6.900756 0.9654123 -0.03788908\n\n\nEl modelo estimado es ETS(M,N,N) o “MNN”, un modelo sin pendiente ni estacionalidad y con error multiplicativo. Es decir, \\(y_{t+1} = l_t \\cdot (1 + \\varepsilon_{t+1})\\).\nEl valor de \\(\\alpha\\) técnicamente es 1, indicando que el nivel de la serie varia en el tiempo y que realmente estamos usando para las previsiones el método ingenuo I.\nRespecto de la calidad del modelo, el valor de MAPE= \\(6.9\\)% evidencia que estamos ante un modelo que se ajusta moderadamente bien a los datos. MASE= \\(0.97\\) indica que el modelo de alisado exponencial simple reduce en solo un \\(3\\)% el error del método ingenuo I. Además, el modelo estimado presenta un marcado sesgo positivo, las predicciones en promedio son menores que los valores reales.\n\n\nPredicción\nMediante la función forecast podemos predecir los casos de Libros. Por tratarse de un modelo sin pendiente ni estacionalidad, la predicción es constante en el tiempo (véase Figura 7).\n\nlibrosEtsPre <- forecast(librosEts, \n                         h = 5,\n                         level = 95)\nlibrosEtsPre\n\n     Point Forecast    Lo 95    Hi 95\n2020        64153.8 52808.97 75498.64\n2021        64153.8 48077.97 80229.63\n2022        64153.8 44425.26 83882.34\n2023        64153.8 41327.01 86980.59\n2024        64153.8 38580.71 89726.89\n\nautoplot(librosEtsPre,\n         xlab = \"\",\n         ylab = \"Títulos\",\n         main = \"\")\n\n\n\n\nFigura 7: Libros y predicción a 5 años vista\n\n\n\n\n\n\nAnálisis del residuo\nEl error de un modelo de alisado contiene la componente de Intervención y el propio término de Error. Ver numérica o gráficamente el error permite identificar fácilmente la presencia de valores atípicos (intervención). Obtenemos el error con la función residuals.\n\nerror <- residuals(librosEts)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"Periodo\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1993, 2019, 2)) \n\n\n\n\nFigura 8: Error + Intervención\n\n\n\n\nLa Figura 8 muestra que aunque algún error supera las dos desviaciones típicas, ninguno puede ser considerado claramente como atípico.\n\n\nValidación: error extramuestral a varios periodos vista\nVamos a mejorar la estimación de la calidad de las predicciones obteniendo el MAPE para previsiones extramuestrales a varios periodos vista. Para ello vamos a reservar, por ejemplo, las últimas 6 observaciones de la serie Libros y ajustar el modelo con las restantes. Después usaremos este modelo para calcular las predicciones a 6 periodos vista y compararlas con los valores reales de la serie Libros.\nRecuerda, este método para valorar la calidad de las predicciones usa la filosofía del método training set/test set: el periodo de datos usado en la estimación no se usa como periodo de datos para la validación. Sin embargo, tiene el problema de que el error obtenido es una mezcla de errores de predicción a corto, medio y largo plazo difícil de valorar. Además, los resultados dependen tremendamente del punto de corte temporal seleccionado.\n\n# Definimos las observaciones intra y extramuestrales\nlibrosIntra <- subset(libros, end = length(libros) - 6)\nlibrosExtra <- subset(libros, start = length(libros) - 5)\n\n# Estimamos el modelo con todos los datos menos los 6 ultimos\nlibrosIntraEts <- ets(librosIntra, model = \"MNN\")\n\n# Predecimos los 6 años que hemos quitado de la serie y \n# vemos la calidad del ajuste.\nlibrosExtraPre <- forecast(librosIntraEts, h = 6)\naccuracy(librosExtraPre, librosExtra)\n\n                    ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set  764.4434 6445.919 5075.985 1.116656 7.994644 0.9557225\nTest set     4084.3395 4786.634 4219.782 6.586626 6.828358 0.7945138\n                    ACF1 Theil's U\nTraining set -0.04292193        NA\nTest set      0.08397051  1.881683\n\n\n\n\n\nAtendiendo al MAPE se tiene que el error de previsión a un periodo vista en el periodo intramuestral de 1993 a 2013 es del 8%; y el error de previsión a largo plazo en el periodo extramuestral de 2014 a 2019 es del 6.8%. Sin embargo, para el periodo extramuestral el error medio (ME) es muy elevado, un indicativo de que las previsiones están segadas. Para el punto de corte elegido, la calidad de las previsiones no se deteriora cuanto nos salimos de las condiciones óptimas.\nUn gráfico puede ayudar a entender este proceso de validación. En la Figura 9:\n\nLa línea de puntos vertical separa el periodo muestral (1993-2013) usado para estimar el modelo, del periodo extramuestral (2014-2019) usado sólo para hacer las previsiones.\nLa serie Libros aparece como una línea sólida en negro, desde 1993 hasta 2019.\nLa previsión intra-muestral (a un periodo vista) de la serie Libros aparece como una línea azul. Observa la previsión puede ser mayor o menor que la serie, no evidenciándose sesgo.\nLa línea en rojo es la previsión extra-muestral a largo plazo: \\(\\hat{y}_{T+h}=l_T\\), donde \\(T=2013\\). Observa que todas las previsiones están por debajo del valor real de la serie: la previsión extramuestral está sesgada.\nAl lado de cada previsión se ha indicado el error estimado (MAPE). Para la previsión extramuestral, el error es la media de errores muy bajos (primeras previsiones) y errores muy elevados (últimas previsiones).\n\nClaramente estos resultados dependen del punto de corte seleccionado.\n\n\n\n\n\nFigura 9: Libros, predicción intra y extramuestral\n\n\n\n\nLa presencia de tendencia, primero creciente y luego decreciente, en la serie Libros puede hacernos pensar que un modelo más adecuado para su ajuste y predicción sería ETS(M,A,N), forzando a que haya pendiente. De hecho, el error de estimación de este modelo es del 6.1%, frente al 6.9% para el modelo ETS(M,N,N). Sin embargo, el error de previsión extramuestral a largo plazo para el modelo ETS(M,A,N) es del 12.1%, frente al 6.8% para el modelo ETS(M,N,N). Mejor ajuste no implica mejor predicción. De nuevo, incidir en que claramente estos resultados dependen del punto de corte seleccionado."
  },
  {
    "objectID": "03-05-Tema5.html#nacimientos",
    "href": "03-05-Tema5.html#nacimientos",
    "title": "Técnicas de Alisado Exponencial",
    "section": "11.2 Nacimientos",
    "text": "11.2 Nacimientos\nVeamos un segundo ejemplo con la serie Nacimientos (desde el año 2000).\n\nIdentificación y estimación del mejor modelo\nSi damos total libertad al proceso de selección del mejor modelo, el modelo estimado es ETS(M,A,A), es decir, \\(y_{t+1} = (l_t + b_t + s_{t+1-m}) \\cdot (1+ \\varepsilon_{t+1})\\).\n\nnacimientosEts <- ets(nacimientosb, damped = FALSE)\nsummary(nacimientosEts) \n\nETS(M,A,A) \n\nCall:\n ets(y = nacimientosb, damped = FALSE) \n\n  Smoothing parameters:\n    alpha = 0.4474 \n    beta  = 0.0128 \n    gamma = 0.0001 \n\n  Initial states:\n    l = 33059.0092 \n    b = 133.8769 \n    s = 182.5531 -22.7455 1980.873 1425.817 900.8348 1155.648\n           -1072.562 328.585 -1319.188 -230.6447 -3327.042 -2.1282\n\n  sigma:  0.0222\n\n     AIC     AICc      BIC \n4548.513 4551.270 4607.684 \n\nTraining set error measures:\n                    ME    RMSE     MAE        MPE     MAPE      MASE\nTraining set -75.41839 803.969 635.127 -0.2331352 1.729529 0.4578664\n                    ACF1\nTraining set -0.01404832\n\n\nEl bajo valor de \\(\\beta\\) y \\(\\gamma\\) indican que ambas, la pendiente y la estacionalidad, varían muy lentamente en el tiempo (véase la Figura 10).\n\nautoplot(nacimientosEts,\n         xlab = \"Periodo\",\n         main = \"\")\n\n\n\n\nFigura 10: Componentes del modelo óptimo para Nacimientos\n\n\n\n\nRespecto de la calidad del modelo, el MAPE de 1.7% indica que estamos ante un modelo que se ajusta muy bien a los datos; y el valor de MASE igual a 0.46 indica que este modelo reduce en un 54% el error del método ingenuo con estacionalidad, el más sencillo posible.\nPodemos ver los últimos valores estimados del nivel, la pendiente y la estacionalidad para interpretarlos.\n\nTT <- nrow(nacimientosEts$states)\nnacimientosEts$states[TT,]\n\n           l            b           s1           s2           s3           s4 \n29320.777216   -97.750950   182.341671   -22.911478  1980.732161  1425.627100 \n          s5           s6           s7           s8           s9          s10 \n  900.711756  1155.516255 -1072.778571   328.464694 -1319.322763  -230.718073 \n         s11          s12 \n-3327.380739    -2.092064 \n\n\nFebrero es el mes con menor número de nacimientos: nacen 3327 bebés menos, respecto de la media anual. En octubre es cuando más bebés nacen: 1980 más que la media anual.\nPodemos usar estos valores para predecir un año,\n\nnacimientosEts$states[TT, 1] + (1:12) * nacimientosEts$states[TT, 2] + \n  nacimientosEts$states[TT, 14:3]\n\n     s12      s11      s10       s9       s8       s7       s6       s5 \n29220.93 25797.89 28796.81 27610.45 29160.49 27661.49 29792.04 29439.48 \n      s4       s3       s2       s1 \n29866.65 30324.00 28222.61 28330.11 \n\n\nNuestra predicción para enero de 2020 es de 29221 bebés y para diciembre de 2020 de 28330 bebés.\n\n\nPredicción\nSi pedimos los valores de predicción tenemos (sólo se muestran los primeros meses):\n\nnacimientosEtsPre <- forecast(nacimientosEts, \n                              h = 24, \n                              level = 95)\nnacimientosEtsPre\n\n\n\n         Point Forecast    Lo 95    Hi 95\nJan 2020       29220.93 27947.01 30494.86\nFeb 2020       25797.89 24529.51 27066.28\nMar 2020       28796.81 27311.06 30282.55\nApr 2020       27610.45 26045.43 29175.48\nMay 2020       29160.49 27434.66 30886.32\n\n\nLa Figura 11 muestra la serie Nacimientos, su predicción a dos años vista y el intervalo de confianza.\n\nautoplot(nacimientosEtsPre,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\")\n\n\n\n\nFigura 11: Nacimientos y predicción\n\n\n\n\n\n\nAnálisis del error\nSe identifica un valor claramente atípico –supera las 4 desviaciones típicas– que corresponde a enero de 2011. Abril de 2008 y Junio de 2016 son otros candidatos a intervención por superar las 2.5 desviaciones típicas.2\n\nerror <- residuals(nacimientosEts)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"Periodo\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(2000, 2019, 2)) \n\n\n\n\nFigura 12: Error + Intervención\n\n\n\n\n\n\nValidación: error extramuestral según horizonte temporal\nEn este ejemplo calcularemos el error extramuestral según el horizonte temporal de previsión, una metodología que ya hemos visto anteriormente.\n\nk <- 120                 \nh <- 12                  \nTT <- length(nacimientosb)\ns <- TT - k - h \n\nmapeAlisado <- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set <- subset(nacimientosb, start = i + 1, end = i + k)\n  test.set <-  subset(nacimientosb, start = i + k + 1, end = i + k + h)\n  \n  fit <- ets(train.set, model = \"MAA\", damped = FALSE)\n  fcast<-forecast(fit, h = h)\n  mapeAlisado[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n}\n\nerrorAlisado <- colMeans(mapeAlisado)\nerrorAlisado\n\n [1] 1.895690 2.132387 2.242887 2.585539 2.558460 2.566431 2.776798 2.744558\n [9] 2.832179 2.999329 3.016310 3.250591\n\n\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorAlisado)) +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"MAPE\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\nFigura 13: Error de predicción según horizonte temporal\n\n\n\n\nLa Figura 13 muestra el error de previsión extramuestral según el horizonte temporal. El error extramuestral a un periodo vista es comparable al error intramuestral (1.9% frente a 1.7%). Aunque el error de previsión aumenta conforme lo hace el horizonte temporal, siempre se mantiene muy bajo. Por ejemplo, en las previsiones a 12 meses vista el error es del 3.3%."
  },
  {
    "objectID": "03-05-Tema5.html#demanda-eléctrica",
    "href": "03-05-Tema5.html#demanda-eléctrica",
    "title": "Técnicas de Alisado Exponencial",
    "section": "11.3 Demanda eléctrica",
    "text": "11.3 Demanda eléctrica\nConsideraremos la serie de Demanda eléctrica desde el 1 de febrero hasta el 30 de mayo, 17 semanas completas de 2021.\n\nIdentificación y estimación del mejor modelo\nEl modelo óptimo (ANA) no presenta tendencia y tiene error y estacionalidad aditiva, es decir, \\(y_{t+1} = l_t + s_{t+1-m} + \\varepsilon_{t+1}\\).\n\nelectricidadEts <- ets(electricidad)\nsummary(electricidadEts) \n\nETS(A,N,A) \n\nCall:\n ets(y = electricidad) \n\n  Smoothing parameters:\n    alpha = 0.8153 \n    gamma = 0.0001 \n\n  Initial states:\n    l = 722.0988 \n    s = -88.4954 -48.1336 18.9787 30.7625 35.2935 36.6034\n           14.991\n\n  sigma:  17.1482\n\n     AIC     AICc      BIC \n1255.728 1257.765 1283.519 \n\nTraining set error measures:\n                     ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.5481487 16.48698 10.17435 -0.1139339 1.549188 0.5110171\n                   ACF1\nTraining set 0.04689494\n\n\nEl valor \\(\\gamma = 0\\) indica que las estacionalidad se mantiene contante en el tiempo, mientras que el elevado valor de \\(\\alpha\\) indica que el nivel de la serie cambia de forma constante.\nRespecto de la calidad del modelo, el MAPE de 1.5% indica que estamos ante un modelo que se ajusta muy bien a los datos; no hay sesgo (ME es casi cero); y el valor de ACF1 casi nulo indica que la fórmula usada para el cálculo del intervalo de confianza de las predicciones es válida.\nPodemos ver los últimos valores estimados del nivel y la estacionalidad para interpretarlos.\n\nTT <- nrow(electricidadEts$states)\nelectricidadEts$states[TT,]\n\n        l        s1        s2        s3        s4        s5        s6        s7 \n668.91416 -88.49612 -48.13342  18.97883  30.76151  35.29273  36.60043  14.98952 \n\n\nEl domingo la demanda eléctrica cae 88 GWh respecto de la media semanal. Por el contrario, el martes y el miércoles son los días de mayor demanda respecto de la media semanal: 36 GWh y 35 GWh respectivamente.\nTambién podemos usarlos para predecir una semana,\n\nelectricidadEts$states[TT, 1] + electricidadEts$states[TT, 8:2]\n\n      s7       s6       s5       s4       s3       s2       s1 \n683.9037 705.5146 704.2069 699.6757 687.8930 620.7807 580.4180 \n\n\n\n\nPredicción\nSi pedimos los valores de predicción para las cuatro semanas siguientes, tenemos (sólo se muestran las dos primeras):\n\nelectricidadEtsPre <- forecast(electricidadEts, \n                               h = 28, \n                               level = 95)\nelectricidadEtsPre\n\n\n\n         Point Forecast    Lo 95    Hi 95\n23.00000       683.9037 650.2939 717.5135\n23.14286       705.5146 662.1490 748.8802\n23.28571       704.2069 652.9085 755.5053\n23.42857       699.6757 641.5165 757.8348\n23.57143       687.8930 623.6012 752.1848\n23.71429       620.7807 550.8923 690.6692\n23.85714       580.4180 505.3479 655.4882\n24.00000       683.9037 603.9882 763.8192\n24.14286       705.5146 621.0312 789.9980\n24.28571       704.2069 615.3903 793.0235\n24.42857       699.6757 606.7276 792.6238\n24.57143       687.8930 590.9894 784.7966\n24.71429       620.7807 520.0769 721.4846\n24.85714       580.4180 476.0514 684.7847\n\n\nLa Figura 14 muestra la serie Demanda eléctrica, su predicción a cuatro semanas vista y el intervalo de confianza.\n\nautoplot(electricidadEtsPre,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\nFigura 14: Demanda eléctrica y predicción\n\n\n\n\n\n\nAnálisis del error\n\nerror <- residuals(electricidadEts)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"Semana\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(6, 26, 2)) \n\n\n\n\nFigura 15: Error + Intervención\n\n\n\n\nEl la Figura 15 se identifican en la semana 14 varios días atípicos –supera las 4 desviaciones típicas– que corresponden a la Semana Santa, que en 2021 fue del 28 de marzo al 4 de abril. Los dos valores mas extremos corresponden al Jueves Santo, que por ser vacacional tuvo un consumo de electricidad inferior al esperado; y el sábado siguiente donde el consumo fue superior al esperado."
  },
  {
    "objectID": "03-11-Ejemplo5.html#ajuste-por-alisado-exponencial",
    "href": "03-11-Ejemplo5.html#ajuste-por-alisado-exponencial",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.1 Ajuste por alisado exponencial",
    "text": "3.1 Ajuste por alisado exponencial\nSi se estima el modelo sin imponer ninguna restricción, ets identifica como modelo óptimo ETS(M,N,M).\n\nPernoctacionesEts <- ets(Pernoctaciones)\nsummary(PernoctacionesEts) \n\nETS(M,N,M) \n\nCall:\n ets(y = Pernoctaciones) \n\n  Smoothing parameters:\n    alpha = 0.5303 \n    gamma = 0.2397 \n\n  Initial states:\n    l = 19423762.6337 \n    s = 0.5419 0.6044 1.0578 1.3327 1.7077 1.6555\n           1.2735 1.0803 0.8096 0.7428 0.615 0.5788\n\n  sigma:  0.034\n\n     AIC     AICc      BIC \n7740.748 7742.890 7792.957 \n\nTraining set error measures:\n                   ME     RMSE    MAE       MPE     MAPE      MASE      ACF1\nTraining set 50058.34 727644.8 539050 0.1356775 2.606948 0.5925429 0.1478231\n\n\nEl modelo estimado no tiene pendiente, y tiene estacionalidad y residuo multiplicativos: \\[y_{t+1} = l_t  \\cdot s_{t+1-m} \\cdot (1 + \\varepsilon_{t+1}).\\]\nEl valor de \\(\\alpha\\) indica que el nivel de la serie ha ido variando lentamente en el tiempo. El valor de \\(\\gamma\\) relativamente bajo indica que la componente estacional ha evolucionado muy poco con el paso de los años. (Véase Figura 3.)\n\nautoplot(PernoctacionesEts,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 3: Descomposición para Pernoctaciones\n\n\n\n\nLa calidad del ajuste es bastante buena, con un MAPE de 2.6% y un RMSE de 728 mil pernoctaciones (o 539 si usamos el MAE). Además, según el MASE, el modelo de alisado exponencial supone una mejora del 41% respecto del método ingenuo con estacionalidad, que ya usamos para Pernoctaciones y tenía un MAPE del 4.6%. Parece que para la serie mensual el método de Alisado si supone una mejora notable en la calidad del ajuste respecto del método más sencillo.\nLos últimos valores estimados del nivel y la estacionalidad, que corresponden a diciembre de 2019, nos permiten mostrar gráficamente la componente estacional más reciente (Figura 4).\n\nTT <- nrow(PernoctacionesEts$states)\nPernoctacionesEts$states[TT,]\n\n\n\n          l          s1          s2          s3          s4          s5 \n25512327.37        0.55        0.61        1.12        1.40        1.70 \n         s6          s7          s8          s9         s10         s11 \n       1.60        1.30        1.13        0.86        0.73        0.59 \n        s12 \n       0.57 \n\n\n\ncomponenteEstacional <- PernoctacionesEts$states[TT, 13:2]\n\nggplot() +\n  geom_line(aes(x = 1:12, y = componenteEstacional)) + \n  geom_hline(yintercept = 1, colour = \"blue\", lty = 2) +\n  ggtitle(\"\") +\n  xlab(\"\") +\n  ylab(\"Efecto estacional\") +\n  scale_x_continuous(breaks= 1:12, \n                     labels = c(\"Ene\", \"Feb\", \"Mar\", \"Abr\", \"May\", \"Jun\", \n                                \"Jul\", \"Ago\", \"Sep\", \"Oct\", \"Nov\", \"Dic\")) \n\n\n\n\nFigura 4: Componente estacional\n\n\n\n\nEl nivel de las pernoctaciones en diciembre de 2019 (última observación) es de 25.5 millones de noches. El mayor número de pernoctaciones dentro del año tiene lugar en verano, en los meses de julio y agosto. En concreto, destaca el mes agosto con un incremento del 70% (s5) en las pernoctaciones respecto de la media anual. Las pernoctaciones en invierno bajan drásticamente respecto de la media anual, observándose en diciembre un 45% menos de pernoctaciones (s1). El efecto estacional estimado por el método de alisado es muy similar al estimado durante la descriptiva de la serie."
  },
  {
    "objectID": "03-11-Ejemplo5.html#predicción",
    "href": "03-11-Ejemplo5.html#predicción",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.2 Predicción",
    "text": "3.2 Predicción\nSi pedimos los valores de predicción y su intervalo de confianza al 95% para los próximos tres años, tenemos (numéricamente sólo se muestra el primer año):\n\nPernoctacionesEtsPre <- forecast(PernoctacionesEts, \n                                 h = 36, \n                                 level = 95)\n\nPernoctacionesEtsPre\n\n\n\n         Point Forecast    Lo 95    Hi 95\nJan 2020       14603125 13630775 15575475\nFeb 2020       15109537 13970636 16248438\nMar 2020       18575400 17029060 20121741\nApr 2020       21893628 19913635 23873620\nMay 2020       28781920 25987245 31576594\nJun 2020       33207095 29775705 36638484\nJul 2020       40832285 36372817 45291753\nAug 2020       43442634 38455577 48429690\nSep 2020       35699807 31411510 39988105\nOct 2020       28511154 24940956 32081352\nNov 2020       15479002 13464828 17493176\nDec 2020       13920455 12043305 15797604\n\n\n\nautoplot(PernoctacionesEtsPre,\n         xlab = \"\",\n         ylab = \"Casos\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\nFigura 5: Pernoctaciones (2000-2019) y predicción (2020-2022)\n\n\n\n\nLas predicciones no muestran tendencia. (Véase Figura 5.)"
  },
  {
    "objectID": "03-11-Ejemplo5.html#análisis-del-error",
    "href": "03-11-Ejemplo5.html#análisis-del-error",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.3 Análisis del error",
    "text": "3.3 Análisis del error\nLa Figura 6 muestra el residuo del modelo.\n\nerror <- residuals(PernoctacionesEts)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"black\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(2000, 2020, 2)) \n\n\n\n\nFigura 6: Error + Intervención\n\n\n\n\nSe observan cuatro meses en los que el residuo supera las tres desviaciones típicas: abril de 2005, 2011 y 2017, y mayo de 2013. Además, en abril de 2002 el residuo está cercano a las tres desviaciones típicas. La causa de los errores negativos elevados en abril de 2002 y 2005 es que la Semana Santa cayó en marzo, de forma que el turismo de Semana Santa se trasladó ese mes y en abril hubo menos pernoctaciones de las esperadas."
  },
  {
    "objectID": "03-11-Ejemplo5.html#validación",
    "href": "03-11-Ejemplo5.html#validación",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.4 Validación",
    "text": "3.4 Validación\nYa hemos visto que el modelo comete un error próximo al 2.6%. Este valor es la estimación del error en la previsión intramuestral y a un periodo vista. A fin de poder estimar mejor la capacidad predictiva del modelo vamos a estimar el error de previsión extramuestral según el horizonte temporal.\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k=120\\), y que el horizonte temporal es un año, \\(h = 12\\) meses.\n\nk <- 120                 \nh <- 12                  \nTT <- length(Pernoctaciones)\ns <- TT - k - h          \n\nmapeAlisado <- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set <- subset(Pernoctaciones, start = i + 1, end = i + k)\n  test.set <-  subset(Pernoctaciones, start = i + k + 1, end = i + k + h)\n  \n  fit <- ets(train.set, model = \"MNM\")\n  fcast<-forecast(fit, h = h)\n  mapeAlisado[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n}\n\nerrorAlisado <- colMeans(mapeAlisado)\nerrorAlisado\n\n [1] 3.013355 3.768883 4.163453 4.246792 4.372550 4.506715 4.654520 5.179270\n [9] 5.440347 5.329751 5.059357 4.895968\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorAlisado)) +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"MAPE\") +\n  ylim(0, 6) + \n  scale_x_continuous(breaks= 1:12)\n\n\n\n\nFigura 7: Error de predicción según horizonte temporal\n\n\n\n\nLa Figura 7 muestra el error de previsión extramuestral según el horizonte de previsión. Se observa como para horizontes de predicción de uno a nueve meses el error de predicción aumenta según aumenta el horizonte de predicción, pasando del 3% para predicciones a un mes vista hasta el 5.4% para predicciones a nueve meses vista.\nSin embargo, para previsiones a más largo plazo el error de predicción decrece, hasta situarse en el 4.9% en las previsiones a un año vista."
  },
  {
    "objectID": "03-03-Tema3.html#notación-y-definiciones",
    "href": "03-03-Tema3.html#notación-y-definiciones",
    "title": "Métodos sencillos de predicción. Evaluación de predicciones",
    "section": "2.1 Notación y definiciones",
    "text": "2.1 Notación y definiciones\nDada una serie temporal \\(\\{y_t\\}_{t=1}^T\\), se define:\n\nPrevisión \\(h\\) periodos adelante, como la previsión de la serie para el periodo \\(t+h\\) disponiendo de información hasta el periodo \\(t\\), y se denota por \\(\\hat{y}_{t+h|t}\\). Por simplicidad lo escribiremos también como \\(\\hat{y}_{t+h}\\).\n\n\nAsí, \\(\\hat{y}_{t+1|t}\\) es la previsión un periodo adelante o a un periodo vista. Es decir, la previsión de la serie en el periodo \\(t+1\\) desde el periodo \\(t\\).\n\nPor simplicidad denotaremos a \\(\\hat{y}_{t+1|t}\\) como \\(\\hat{y}_{t+1}\\); y como \\(\\hat{y}_{t}\\) a la previsión en \\(t\\), con datos hasta el periodo \\(t-1\\) (\\(\\hat{y}_{t} = \\hat{y}_{t|t-1}\\)).\nSe define como error de previsión a un periodo vista a \\[\\hat{e}_t=y_t-\\hat{y}_t,\\] de forma que la serie \\(\\{\\hat{e}_t\\}_{t=1}^T\\) nos permitirá definir varios criterios de calidad de ajuste."
  },
  {
    "objectID": "03-03-Tema3.html#medidas-de-precisión-de-la-predicción",
    "href": "03-03-Tema3.html#medidas-de-precisión-de-la-predicción",
    "title": "Métodos sencillos de predicción. Evaluación de predicciones",
    "section": "2.2 Medidas de precisión de la predicción",
    "text": "2.2 Medidas de precisión de la predicción\nDada una serie \\(\\{y_t\\}_{t=1}^T\\), un método de predicción y su vector de errores asociado \\(\\{\\hat{e}_t\\}_{t=1}^T\\), podemos definir múltiples medidas de calidad del método de predicción que hacen referencia a la presencia de sesgo en las predicciones, la magnitud del error cometido y la calidad del intervalo de confianza de las predicciones. Las más habituales son (siglas en inglés):\n\nError medio (ME): \\(\\frac{1}{T}\\sum_{t=1}^T \\hat{e}_t\\)\n\n\nRaíz del error cuadrático medio (RMSE): \\(\\sqrt{\\frac{1}{T}\\sum_{t=1}^T \\hat{e}^2_t}\\)\n\n\nError absoluto medio (MAE): \\(\\frac{1}{T}\\sum_{t=1}^T |\\hat{e}_t|\\)\n\n\nError porcentual medio (MPE): \\(\\frac{100}{T}\\sum_{t=1}^T \\frac{\\hat{e}_t}{y_t}\\)\n\n\nError porcentual absoluto medio (MAPE): \\(\\frac{100}{T}\\sum_{t=1}^T \\big|\\frac{\\hat{e}_t}{y_t}\\big|\\)\n\n\nError porcentual absoluto medio simétrico (sMAPE): \\(\\frac{200}{T}\\sum_{t=1}^T \\Big|\\frac{\\hat{e}_t}{y_t + \\hat{y}_t}\\Big|\\)\n\n\nError escalado absoluto medio (MASE): \\(\\big(\\frac{1}{T}\\sum_{t=1}^T |\\hat{e}_t|\\big)/q\\), donde \\(q\\) es el error absoluto medio para un método ingenuo de predicción: el método ingenuo I para series sin estacionalidad y el método ingenuo con estacionalidad para series con estacionalidad.\n\n\nCorrelación entre \\(\\hat{e}_t\\) y \\(\\hat{e}_{t-1}\\) (ACF1).\n\n\n\nME y MPE permiten valorar el sesgo de las predicciones (que estas estén sistemáticamente por encima o por debajo de los valores reales).\n\nLo esperado es un valor cercano a cero (con relación al valor medio de la serie). Valores muy alejados de cero son indicadores de sesgo de predicción.\n\nRMSE y MAE indican el error medio cometido, medido en las mismas unidades que la serie temporal.\n\nEstán acotadas inferiormente por el valor óptimo de 0, pero no hay cota superior.\n\nMAPE y sMAPE indican el error porcentual medio cometido.\n\nEstán acotadas inferiormente por el valor óptimo de 0%, y la cota superior natural es 100%, aunque podría sobrepasarse.\nSi \\(y_t\\) puede valer 0, entonces MAPE no se puede calcular. Además, MAPE penaliza más los errores negativos frente a los errores positivos. La medida de precisión sMAPE se define a fin de corregir estos problemas.\n\nMASE es la ratio entre el error del método usado y el error de un método ingenuo de predicción. Permite saber cuánto ganamos en capacidad predictiva al pasar de un método ingenuo a otro más complicado.\n\nUn valor cercano a 1 indica que el método usado no es mejor que el método ingenuo\nCuanto más cercano a 0, mejor es el método usado respecto del método ingenuo\nSu complementario a 1 se puede interpretar como la tasa de mejora\n\nACF1 permite saber si la fórmula usada para estimar el intervalo de confianza de las predicciones es válida:\n\nUn valor muy cercano a 0 (menor que 0.1) indica que la fórmula es válida.\nValores mínimamente alejados de 0 indican que la fórmula no es válida.\n\n\n\n\n\n\n\nCálculo del intervalo de confianza de las predicciones\n\n\n\nVe a la Píldora Bootstrapping para intervalos de predicción para saber más sobre las fórmulas usadas para estimar el intervalo de confianza de las predicciones y alternativas de cálculo cuando estas fórmulas no son válidas.\n\n\nLas medias se pueden sustituir por medianas. Esto es especialmente útil cuando para algunas observaciones hay errores atípicamente altos.\n\n\nSi para realizar la predicción del periodo \\(t\\) se usa una metodología que utiliza datos incluido dicho periodo, se hablará de predicción intramuestral y error intramuestral (o de ajuste). En caso contrario, si la predicción del periodo \\(t\\) usa una metodología que solo necesita de datos hasta el periodo \\(t-1\\), se hablará de predicción y error extramuestral.\nLos indicadores de calidad que se basan en predicciones intramuestrales a un periodo vista presentan dos problemas. Primero, evalúan el error de predicción a un periodo vista, cuando en muchas situaciones reales las predicciones se realizan sobre un horizonte temporal más amplio. Segundo, son errores intramuestrales, resultantes de predecir los mismos datos que ha usado el método para calcular la predicción y, por tanto, sobrestiman la capacidad predictiva del modelo.\nVeremos en el epígrafe 4 de este tema métodos de evaluación de la calidad de las predicciones que superan estas limitaciones."
  },
  {
    "objectID": "03-03-Tema3.html#métodos-sencillos-de-predicción-1",
    "href": "03-03-Tema3.html#métodos-sencillos-de-predicción-1",
    "title": "Métodos sencillos de predicción. Evaluación de predicciones",
    "section": "3.1 Métodos sencillos de predicción",
    "text": "3.1 Métodos sencillos de predicción\n\nSeries sin tendencia y sin estacionalidad\nMétodo de la Media: \\(\\hat{y}_{T+h}=(y_1+\\ldots,y_T)/T\\).\n\nLa predicción para cualquier periodo futuro es la media de las observaciones disponibles previas.\nFunción de R: meanf(y, h)\n\nMétodo ingenuo I: \\(\\hat{y}_{T+h}=y_T\\).\n\nLa predicción para cualquier periodo futuro es la última observación disponible.\nFunción de R: naive(y, h) o rwf(y, h) (rw de random walk)\nPara series sin estacionalidad este es el método ingenuo de comparación del MASE.\n\n\n\nSeries con tendencia y sin estacionalidad\nMétodo ingenuo II: \\(\\hat{y}_{T+h}=y_T + h(y_T-y_{T-1})\\).\n\nLa predicción \\(h\\) periodos adelante es la última observación disponible más \\(h\\) veces el último incremento observado.\nNo tiene función en R, pero se podría emular mediante la función ets (véase epígrafe de 5.5 del tema 5, Alisado exponencial de Holt).\n\nMétodo de la deriva: \\(\\hat{y}_{T+h}=y_T+h\\frac{y_T - y_1}{T-1}\\).\n\nLa predicción \\(h\\) periodos adelante es la última observación disponible más \\(h\\) veces el incremento medio observado.\nFunción de R: rwf(y, h, drift = TRUE)\n\n\n\nSeries sin tendencia y con estacionalidad\nMétodo ingenuo con estacionalidad: \\(\\hat{y}_{T+h}=y_{T-m(k+1)}\\).\n\n\\(k\\) es la parte entera de \\((h-1)/m\\), es decir, el número de estaciones completas en el periodo de predicción previo al periodo \\(T+h\\).\nLa predicción para un periodo es la última observación disponible de la misma estación que la fecha que se desea predecir.\nFunción de R: snaive(y, h)\nPara series con estacionalidad este es el método ingenuo de comparación del MASE\n\nNo hay métodos sencillos cuando la serie tiene tendencia y estacionalidad, así que se suele usar el método ingenuo con estacionalidad."
  },
  {
    "objectID": "03-03-Tema3.html#ejemplo-de-aplicación",
    "href": "03-03-Tema3.html#ejemplo-de-aplicación",
    "title": "Métodos sencillos de predicción. Evaluación de predicciones",
    "section": "3.2 Ejemplo de aplicación",
    "text": "3.2 Ejemplo de aplicación\n\nSerie Libros\n\nlibros <- read.csv2(\"./series/libros.csv\", header = TRUE)\nlibros <- ts(libros[ ,2], start = 1993, frequency  = 1)\n\nLas siguientes salidas muestra el resultado de la aplicación de algunos de estos métodos sencillos a la serie Libros (número de títulos publicados anualmente en España desde 1993 hasta 2019), con independencia de su adecuación dadas las componentes de esta serie.\nLos métodos de la Media e Ingenuo I realizan una predicción constante, el primero la media de títulos en el periodo de análisis (61873) y el segundo el último dato observado (64154). El método de deriva ofrece una predicción creciente porque la serie Libros tiene una pendiente media positiva en el periodo de análisis.\n\n(mediaLibros <- meanf(libros, h = 5))\n\n     Point Forecast    Lo 80    Hi 80    Lo 95   Hi 95\n2020       61873.93 47672.32 76075.53 39674.35 84073.5\n2021       61873.93 47672.32 76075.53 39674.35 84073.5\n2022       61873.93 47672.32 76075.53 39674.35 84073.5\n2023       61873.93 47672.32 76075.53 39674.35 84073.5\n2024       61873.93 47672.32 76075.53 39674.35 84073.5\n\n(naiveLibros <- naive(libros, h = 5))\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2020          64154 56581.87 71726.13 52573.42 75734.58\n2021          64154 53445.39 74862.61 47776.59 80531.41\n2022          64154 51038.68 77269.32 44095.85 84212.15\n2023          64154 49009.73 79298.27 40992.84 87315.16\n2024          64154 47222.19 81085.81 38259.04 90048.96\n\n(derivaLibros <- rwf(libros,  h = 5, drift = TRUE))\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2020       65053.85 57421.83 72685.86 53381.69 76726.00\n2021       65953.69 54954.79 76952.60 49132.32 82775.06\n2022       66853.54 53135.50 80571.58 45873.60 87833.47\n2023       67753.38 51632.77 83874.00 43099.04 92407.73\n2024       68653.23 50321.73 86984.74 40617.61 96688.85\n\n\nEn la Figura 1 muestra el resultado gráfico de la aplicación de estos métodos. Se ha fijado un horizonte de previsión de cinco años (h = 5). El argumento PI = FALSE hace que no se impriman los intervalos de confianza de las predicciones.\n\nautoplot(libros, \n         series = \"Libros\",\n         xlab = \"\",\n         ylab = \"Títulos\",\n         main = \"\") +\n  autolayer(mediaLibros, series=\"Media\", PI = FALSE) +\n  autolayer(naiveLibros, series=\"Ingenuo\", PI = FALSE) +\n  autolayer(derivaLibros, series=\"Deriva\", PI = FALSE) +\n  scale_colour_discrete(limits=c(\"Libros\", \"Media\", \"Ingenuo\", \"Deriva\")) +\n  labs(colour=\"Métodos\") + \n  theme(legend.position=c(0.1,0.8))\n\n\n\n\nFigura 1: Libros y predicción por métodos sencillos\n\n\n\n\nCon la función accuracy se puede obtener el error de predicción intramuestral a un periodo vista de cada método:\n\naccuracy(mediaLibros)\naccuracy(naiveLibros)\naccuracy(derivaLibros)\n\n\n\n              ME     RMSE     MAE   MPE  MAPE MASE  ACF1\nMedia       0.00 10407.07 7938.30 -3.06 13.66 1.75  0.77\nIngenuo I 899.85  5908.57 4524.38  1.34  7.14 1.00 -0.04\nDeriva      0.00  5839.64 4229.65 -0.13  6.66 0.93 -0.04\n\n\nPodemos destacar que:\n\nEl método de Media presenta una baja calidad de ajuste debido a que la serie Libros tiene tendencia (MAPE = 14%). Además, el intervalo de confianza de las predicciones no es fiable (ACF1 = 0.77).\nEl método de Deriva tiene la mejor calidad de ajuste, con un error porcentual del 6.7% (MAPE), y un error medio aproximado de 6000 títulos (RMSE). No presenta sesgo (ME = 0) y el intervalo de confianza de las predicciones es fiable (ACF1 = -0.04).\nEl método Ingenuo I tiene buena calidad de ajuste, pero las previsiones están ligeramente sesgadas (ME = 900).\nPara series sin estacionalidad el método sencillo de comparación usado en el cálculo del MASE es el Ingenuo I. Es por ello que este indicador vale 1 para este método.\nEl error medio (ME) siempre será nulo para el método de la Media y de la Deriva, lo que indica que nos equivocamos tanto por exceso como por defecto. Esta es una buena propiedad, que el método Ingenuo I no verifica.\n\n\n\nSerie Nacimientos\nPodemos usar el método ingenuo con estacionalidad con la serie Nacimientos para obtener una previsión a dos años vista. El error absoluto porcentual medio es del 3.6%. Es decir, aplicando algo tan simple como predecir el número de nacimientos para un mes como los nacimientos del mismo mes del año previo, tenemos ya un error de predicción muy bajo. La Figura 2 muestra la serie y la predicción que, debido al método usado, no incorpora la tendencia decreciente de los últimos años.\n\nnacimientos <- read.csv2(\"./series/nacimientos.csv\", header = TRUE)\nnacimientos <- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  frequency = 12)\n\n(snaive.nacimientos <- snaive(nacimientos, h = 24, level = 95))\n\n         Point Forecast    Lo 95    Hi 95\nJan 2020          30858 27484.08 34231.92\nFeb 2020          27324 23950.08 30697.92\nMar 2020          29333 25959.08 32706.92\nApr 2020          28780 25406.08 32153.92\nMay 2020          29732 26358.08 33105.92\nJun 2020          28475 25101.08 31848.92\nJul 2020          31329 27955.08 34702.92\nAug 2020          31134 27760.08 34507.92\nSep 2020          31083 27709.08 34456.92\nOct 2020          31896 28522.08 35269.92\nNov 2020          29610 26236.08 32983.92\nDec 2020          29193 25819.08 32566.92\nJan 2021          30858 26086.55 35629.45\nFeb 2021          27324 22552.55 32095.45\nMar 2021          29333 24561.55 34104.45\nApr 2021          28780 24008.55 33551.45\nMay 2021          29732 24960.55 34503.45\nJun 2021          28475 23703.55 33246.45\nJul 2021          31329 26557.55 36100.45\nAug 2021          31134 26362.55 35905.45\nSep 2021          31083 26311.55 35854.45\nOct 2021          31896 27124.55 36667.45\nNov 2021          29610 24838.55 34381.45\nDec 2021          29193 24421.55 33964.45\n\naccuracy(snaive.nacimientos)\n\n                    ME    RMSE      MAE       MPE     MAPE MASE      ACF1\nTraining set -588.3163 1721.42 1389.544 -1.512298 3.647559    1 0.7170624\n\n\n\n\n\n\n\n\nInterpretación de ACF1\n\n\n\n¿Es fiable el intervalo de confianza calculado para las predicciones?\n\n\n\nautoplot(snaive.nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\nFigura 2: Nacimientos y predicción por el método Ingenuo con estacionalidad\n\n\n\n\n\n\nSerie Demanda eléctrica\nPodemos usar el método ingenuo con estacionalidad con la serie Demanda eléctrica, que tiene una estacionalidad de orden 7, pero no parece presentar tendencia. El error absoluto porcentual medio es del 4% o 43 GWh (RMSE), un error razonablemente reducido. Sin embargo, como ya pasaba con nacimientos, el ACF1 indica que la fórmula usada para el cálculo del intervalo de confianza de las predicciones no es válida.\n\nelectricidad <- read.csv2(\"./series/Consumo electrico.csv\", header = TRUE)\nelectricidad <- ts(electricidad[, 2],\n                   start = c(1, 5),\n                   frequency = 7)\n\nsnaive.electricidad <- snaive(electricidad, h = 28, level = 95)\naccuracy(snaive.electricidad)\n\n                    ME     RMSE      MAE        MPE     MAPE MASE      ACF1\nTraining set -2.117489 42.99794 27.93245 -0.4964111 4.000218    1 0.7096965\n\n\nLa Figura 3 muestra la serie y la predicción a cuatro semanas vista. Debido a que la semana de referencia para predecir es la semana de Navidad, donde el consumo eléctrico es inferior al usual, las predicciones resultan ser claramente incorrectas. Este es un buen ejemplo de la diferencia entre calidad de ajuste y calidad de las predicciones.\n\nautoplot(snaive.electricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\nFigura 3: Demanda eléctrica y predicción por el método Ingenuo con estacionalidad"
  },
  {
    "objectID": "03-03-Tema3.html#validación-por-la-metodología-de-training-settest-set-para-series-temporales",
    "href": "03-03-Tema3.html#validación-por-la-metodología-de-training-settest-set-para-series-temporales",
    "title": "Métodos sencillos de predicción. Evaluación de predicciones",
    "section": "4.1 Validación por la metodología de Training set/Test set para Series Temporales",
    "text": "4.1 Validación por la metodología de Training set/Test set para Series Temporales\nVamos a estimar la calidad de las predicciones obteniendo medidas de error para previsiones extramuestrales a varios periodos vista usando la filosofía del método training set/test set. Dividimos la serie temporal \\(\\{y_t\\}_{t=1}^T\\) en dos subseries. Los primeros datos \\(\\{y_t\\}_{t=1}^{T_0}\\), \\(T_0 < T\\), se usarán para estimar el modelo; y los últimos datos \\(\\{y_t\\}_{t={T_0+1}}^{T}\\) para validar el modelo.\nEsta metodología, muy efectiva para datos de corte transversal, genera dos problemas cuando se aplica a series temporales: i) el error obtenido es una mezcla de errores de predicción a corto, medio y largo plazo; ii) los resultados dependen tremendamente del punto de corte temporal seleccionado.\n\nSerie Libros\nVamos a reservar, por ejemplo, las últimas 7 observaciones de la serie Libros (años 2013 a 2019) y ajustar el modelo con las restantes. Después usaremos este modelo para calcular las predicciones a 7 periodos vista y compararlas con los valores reales de la serie.\n\n# Definimos las observaciones intra- y extramuestrales\nlibrosIntra <- subset(libros, end = length(libros) - 7)\nlibrosExtra <- subset(libros, start = length(libros) - 6)\n\n# Estimamos el modelo con todos los datos menos los 7 ultimos y\n# predecimos los 7 años que hemos quitado de la serie \nlibrosExtraPre <- rwf(librosIntra,  h = 7, drift = TRUE)\n\n# Vemos la calidad del ajuste. Primero la predicción y luego los datos reales\naccuracy(librosExtraPre, librosExtra)\n\n\n\n                    ME     RMSE      MAE    MPE  MAPE MASE  ACF1 Theil's U\nTraining set      0.00  5863.85  4279.52  -0.05  6.53 0.87 -0.19        NA\nTest set     -15817.32 15867.44 15817.32 -26.44 26.44 3.23 -0.21       6.4\n\n\nAtendiendo al MAPE se tiene que el error de previsión a un periodo vista en el periodo intramuestral de 1993 a 2012 es del 6.5%; mientras que el error de previsión a largo plazo en el periodo extramuestral de 2014 a 2019 es del 26.4%. Ademas, para el periodo extramuestral el error medio (ME) es negativo y muy elevado, un indicativo de que las previsiones están segadas (sobrestiman la realidad). En resumen, la calidad del modelo se deteriora muy rápidamente en cuanto nos salimos de las condiciones óptimas.\nUn gráfico puede ayudar a entender este proceso de validación. En la Figura 4:\n\nLa línea de puntos vertical separa el periodo muestral (1993-2012) usado para estimar el modelo, del periodo extramuestral (2013-2019) usado sólo para hacer las previsiones.\nLa serie Libros aparece como una línea sólida en negro, desde 1993 hasta 2019.\nLa previsión intramuestral (a un periodo vista) de la serie Libros aparece como una línea azul.\nLa línea en rojo es la previsión extramuestral a largo plazo. Observa que todas las previsiones están por encima del valor real de la serie.\nAl lado de cada previsión (intra y extramuestral) se ha indicado el error estimado (MAPE).\n\nClaramente estos resultados dependen del punto de corte seleccionado.\n\n\n\n\n\nFigura 4: Libros, predicción intra- y extramuestral\n\n\n\n\n\n\n\n\n\n\nImportamcia del punto de corte\n\n\n\nPrueba a reservar las últimas 6 observaciones de la serie Libros y repite el análisis.\n\n\n\n\nSerie Nacimientos\nCalculamos de nuevo los diferentes criterios de bondad de ajuste para valorar la calidad de las previsiones extramuestrales a largo plazo. En este caso vamos a reservar los últimos 36 meses como periodo extramuestral.\n\nnacimientosIntra <- subset(nacimientos, end = length(nacimientos) - 36)\nnacimientosExtra <- subset(nacimientos, start = length(nacimientos) - 35)\n\nnacimientosExtraPre <- snaive(nacimientosIntra, h = 36)\n\naccuracy(nacimientosExtraPre, nacimientosExtra)\n\n\n\n                   ME    RMSE     MAE   MPE MAPE MASE ACF1 Theil's U\nTraining set  -529.76 1734.21 1389.62 -1.30 3.59 1.00 0.72        NA\nTest set     -2926.75 3221.74 2926.75 -9.57 9.57 2.11 0.69      1.89\n\n\n\n\n\n\n\nFigura 5: Nacimientos, predicción intra- y extramuestral\n\n\n\n\nLas previsiones extramuestrales muestran una menor pendiente que los casos reales de nacimientos. Así, conforme se avanza en el horizonte temporal las previsiones se van alejando de la realidad y el error extramuestral es del 9.6%, muy alejado del error de estimación intramuestral de 3.6%.\n\n\nSerie Demanda eléctrica\nPara la serie de consumo eléctrico vamos a reservar las 8 últimas semanas (56 días) como periodo extramuestral.\n\nelectricidadIntra <- subset(electricidad, end = length(electricidad) - 56)\nelectricidadExtra <- subset(electricidad, start = length(electricidad) - 55)\n\nelectricidadExtraPre <- snaive(electricidadIntra, h = 56)\n\naccuracy(electricidadExtraPre, electricidadExtra)\n\n\n\n                ME  RMSE   MAE   MPE  MAPE MASE ACF1 Theil's U\nTraining set -1.78 39.17 25.09 -0.41  3.57 1.00 0.68        NA\nTest set     59.30 86.23 73.54  7.82 10.14 2.93 0.48      1.55\n\n\nEl error intramuestral obtenido es del 3.6%, que prácticamente se triplica al obtener el error de previsión extramuestral (10.1%). El elevado valor positivo del error medio indica que las previsiones extramuestrales subestiman el consumo real de electricidad."
  },
  {
    "objectID": "03-03-Tema3.html#validación-cruzada-para-series-temporales",
    "href": "03-03-Tema3.html#validación-cruzada-para-series-temporales",
    "title": "Métodos sencillos de predicción. Evaluación de predicciones",
    "section": "4.2 Validación cruzada para Series Temporales",
    "text": "4.2 Validación cruzada para Series Temporales\nVeamos ahora una técnica, basada en el concepto de validación cruzada (cross validation) que permite obtener de forma individualizada los errores de previsión extramuestral a un periodo vista, a dos periodos vista, etc.\nSupongamos que para estimar el modelo se necesita un mínimo de \\(k\\) observaciones y que se desea predecir hasta un horizonte temporal \\(h\\).\n\nSeleccionamos las observaciones \\(1,2,...,k\\) para estimar el modelo y predecimos las observaciones desde \\(k+1\\) hasta \\(k+h\\). Tenemos, por tanto, \\(h\\) predicciones.\nCalculamos el error de predicción para las predicciones desde \\(k+1\\) hasta \\(k+h\\).\nRepetimos este proceso desplazando el número de observaciones seleccionadas para la estimación un periodo adelante. Es decir, ahora usamos las observaciones \\(2,3,...,k+1\\) para estimar el modelo, predecimos las observaciones desde \\(k+2\\) hasta \\(k+1+h\\) y calculamos el error de predicción.\nIteramos el proceso, desplazando cada vez las observaciones de la estimación un periodo adelante.\nEn general para \\(i=0,1,...,T-k-h\\), donde \\(T\\) es el número total de observaciones:\n\nSeleccionamos las observaciones \\(i+1,i+2,...,i+k\\) para estimar el modelo.\nPredecimos las observaciones desde \\(i+k+1\\) hasta \\(i+k+h\\).\nCalculamos el error de predicción para las observaciones desde \\(i+k+1\\) hasta \\(i+k+h\\).\nPara cada horizonte temporal de predicción se calcula la medida de error deseada.\n\n\n\n\n\n\n\n\nFigura 6: Descripción del proceso de origen de predicción móvil\n\n\nEste procedimiento se denomina origen de predicción móvil (rolling forecast origin) o rolling windows.\nCuando se aplica esta metodología hay que tener en cuenta que los resultados pueden depender del número \\(k\\) de datos usados para la estimación del modelo.\n\nEjemplo de aplicación con Nacimientos\nVamos a aplicar la metodología previa a la serie anual de Nacimientos. Asumimos que se precisan veinte años para hacer una buena estimación, \\(k=20\\), y que el horizonte temporal es de cinco años, \\(h = 5\\). Como la serie es anual, usaremos el método de la deriva para predecir. La siguiente rutina permite obtener el MAPE para previsiones con un horizonte temporal desde uno a cinco años.\n\nnacAnual <- aggregate(nacimientos, FUN = sum)\nk <- 20                   #Minimo numero de datos para estimar\nh <- 5                    #Horizonte de las prediciciones\nTT <- length(nacAnual)    #Longitud serie\ns <- TT - k - h           #Total de estimaciones\n\nmapeRwf <- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set <- subset(nacAnual, start = i + 1, end = i + k)\n  test.set <-  subset(nacAnual, start = i + k + 1, end = i + k + h)\n  \n  fcast <- rwf(train.set, h = h, drift = TRUE)\n  mapeRwf[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n}\n\nmapeRwf <- colMeans(mapeRwf)\nround(mapeRwf, 2)\n\n[1]  4.13  8.07 12.16 16.38 20.67\n\n\nEl error de previsión extramuestral crece linealmente con el horizonte de previsión. Para el primer año el error de predicción se mantiene en un moderado 4.1%. Sin embargo, para el segundo año de predicción el MAPE salta al 8.1% y para los restantes años sigue creciendo rápidamente. Predecir usando la tendencia media solo es un buen método para predecir a un año vista.\n\n\nEjemplo de aplicación con Demanda eléctrica\nAhora aplicaremos la metodología origen de predicción móvil la serie de Demanda eléctrica. En este caso se asumirá que se precisan veinte semanas para hacer una buena estimación, \\(k = 140\\), y que el horizonte temporal es de 4 semanas, \\(h = 28\\). Como la serie tiene estacionalidad, usaremos el método ingenuo con estacionalidad para predecir. La siguiente rutina permite obtener el RMSE para previsiones con un horizonte temporal desde uno a 28 días.\n\nk <- 140                  #Minimo numero de datos para estimar\nh <- 28                   #Horizonte de las prediciciones\nTT <- length(electricidad)#Longitud serie\ns <- TT - k - h           #Total de estimaciones\n\nrmseRwf <- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set <- subset(electricidad, start = i + 1, end = i + k)\n  test.set <-  subset(electricidad, start = i + k + 1, end = i + k + h)\n  \n  fcast <- snaive(train.set, h = h)\n  rmseRwf[i + 1,] <- (test.set - fcast$mean)^2\n}\n\nrmseRwf <- sqrt(colMeans(rmseRwf))\nround(rmseRwf, 2)\n\n [1] 31.77 31.82 32.86 33.32 34.28 34.29 34.30 40.98 41.08 41.12 41.12 41.11\n[13] 41.12 41.12 43.74 43.78 43.78 43.79 43.86 44.12 45.38 55.30 55.68 56.31\n[25] 56.94 57.59 58.34 59.57\n\n\nEl error de previsión extramuestral crece lentamente con el horizonte de previsión. Para predicciones a un día vista el error de predicción es de 32 GWh, para 14 días vista sube hasta 41 GWh y a 28 días vista alcanza los 60 GWh"
  },
  {
    "objectID": "03-10-Ejemplo4.html",
    "href": "03-10-Ejemplo4.html",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "",
    "text": "1 Introducción\nConsideremos de nuevo la serie temporal correspondiente al número de pernoctaciones que los turistas extranjeros realizan en España en alojamientos turísticos autorizados (que llamaremos Pernoctaciones en adelante). Esta serie está disponible en Eurostat desde enero de 2000 hasta diciembre de 2019, un total de 20 años y 240 observaciones.\nLa serie presenta tendencia decreciente hasta finales de la primera década del presente siglo y luego creciente hasta los dos últimos años. La estacionalidad de orden 12 esta determinada por la temperatura. El esquema es multiplicativo.\n\nPernoctaciones <- read.csv2(\"./series/Pernoctaciones.csv\", \n                            header = TRUE)\nPernoctaciones <- ts(Pernoctaciones[, 2], \n                     start = 2000, \n                     frequency = 12)\n\n\nautoplot(Pernoctaciones/1000000,\n         xlab = \"\",\n         ylab = \"Noches (millones)\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(2000, 2020, 2)) \n\n\n\n\nFigura 1: Pernoctaciones\n\n\n\n\n\n\n\n\n\n2 Predicción por el método de las medias móviles\nVamos ajustar y predecir la serie de pernoctaciones por el método de las medias móviles. Como este método asume que la serie no tiene estacionalidad, vamos a trabajar con la serie anualizada de pernoctaciones, donde cada dato será el número de pernoctaciones anuales.\n\nPernoctaciones <- aggregate(Pernoctaciones/1000000, FUN = sum)\n\nautoplot(Pernoctaciones,\n         xlab = \"\",\n         ylab = \"Noches (millones)\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(2000, 2020, 2))  \n\n\n\n\nFigura 2: Pernoctaciones anuales\n\n\n\n\nDado que la serie tiene tendencia, no esperemos grandes resultados con el método de las medias móviles, ni en ajuste ni en calidad de las predicciones. Así usaremos nuestra función mmf.\n\nmmf <- function(x, r = 3, h = 5) {\n  z <- NULL\n  z$x <- x\n  z$orden = r\n  \n  TT <- length(x)\n  inicio <- start(x)\n  frecuencia <-frequency(x)\n  \n  z$mm <- stats::filter(x, rep(1/r, r), side = 1)\n  \n  z$fitted <- ts(c(NA, z$mm[-TT]), \n                 start = inicio, \n                 freq = frecuencia)\n  \n  z$mean <- ts(rep(z$mm[TT], h), \n               start = time(x)[TT] + 1/frecuencia, \n               freq = frecuencia)\n  \n  z$residuals <- x - z$fitted\n  \n  class(z) <- \"forecast\"\n  z\n}\n\nVeamos primero los resultados asumiendo un orden de la media móvil de 4 años (\\(r = 4\\)) y realizando una predicción para los próximos 5 años (h = 5). La Figura 3 muestra la serie Pernoctaciones, la serie ajustada y la predicción.\n\nmmPernoctaciones <- mmf(Pernoctaciones, \n                        r = 4, \n                        h = 5)\n\nautoplot(mmPernoctaciones,\n         xlab = \"\",\n         ylab = \"Noches\",\n         main = \"\") +\n  autolayer(mmPernoctaciones$fitted) +\n  theme(legend.position=\"none\")\n\n\n\n\nFigura 3: Pernoctaciones y predicción (doble media móvil de orden 4)\n\n\n\n\nUn desastre. El orden es muy elevado así que perdemos 4 observaciones al inicio, le cuesta capturar el primer cambio de tendencia –en la serie original tiene lugar en 2010 y en la serie estimada en 2013– y las predicciones no tienen en cuenta el descenso en las pernoctaciones ocurrido en los dos últimos años.\n\naccuracy(mmPernoctaciones)\n\n                   ME     RMSE      MAE      MPE     MAPE     MASE      ACF1\nTraining set 12.00632 21.00165 18.40444 4.162454 7.260456 1.966314 0.5940658\n\n\nAsí, no sorprenden los valores de los criterios de bondad de ajuste: un error porcentual de 7.3%, algo elevado y un error medio (RMSE) de 21 millones de pernoctaciones. Estos errores son superiores a los obtenidos con el método Ingenuo I y de la Deriva (RMSE = 12 millones de pernoctaciones; MAPE = 3.9%).\n\n\n\n\n\n3 Selección del orden y predicción usando origen de predicción móvil\nVamos a mejorar nuestra primera aproximación usando el método del origen de predicción móvil para identificar el orden óptimo de la doble media móvil según que la predicción sea de 1 a 5 años. Asumiremos que el mínimo número de años para hacer las estimaciones es de 10\n\nk <- 10                   \nh <- 5                    \nTT <- length(Pernoctaciones)\ns <- TT - k - h           \n\n\nfor (r in 1:5) {\n\n  tmpMape <- matrix(NA, s + 1, h)\n  \n  for (i in 0:s) {\n  \n    train.set <- subset(Pernoctaciones, start = i + 1, end = i + k)\n    test.set <-  subset(Pernoctaciones, start = i + k + 1, end = i + k + h)\n    \n    fit <- mmf(train.set, r = r, h = 5)\n    tmpMape[i + 1, ] <- 100*abs(test.set - fit$mean)/test.set\n  }\n  tmpMape <- colMeans(tmpMape)\n \n  cat(\"\\nPara un orden de\", \n      r, \n      \"los errores son\", \n      formatC(tmpMape, format = \"f\", digits = 2))  \n}\n\n\nPara un orden de 1 los errores son 4.74 9.59 13.23 16.25 18.58\nPara un orden de 2 los errores son 5.83 10.69 14.27 17.27 19.54\nPara un orden de 3 los errores son 7.06 11.57 15.10 18.05 20.28\nPara un orden de 4 los errores son 8.09 12.28 15.76 18.67 20.91\nPara un orden de 5 los errores son 8.67 13.05 16.48 19.38 21.63\n\n\nCon independencia del horizonte temporal de predicción considerado, el error se hace mínimo para un orden de las medias móviles de 1, ¡el método Ingenuo I!. Además, el error para las predicciones extramuestrales es solo aceptable a un año vista."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicción con Datos Temporales (36519). Grado en Inteligencia y Analítica de Negocios (GBIA)",
    "section": "",
    "text": "Hola a tod@s y bienvenid@s a la página web del curso.\nEsta es la página web de la edición 2022-23 del curso de Predicción con Datos Temporales (GBIA) de la Universitat de València. El repositorio para crear esta web lo tenéis aquí en Github.\nResumen del curso: la idea principal es aprender a manejarse con datos con estructura temporal, en contraposición a los datos de corte transversal (que has visto en el primer semestre con Paco Goerlich) o espacial. Veremos como describir una serie temporal, como ajustarla a un modelo, como hacer predicciones y, lo que es más importante, a valorar la calidad de las predicciones desde un enfoque de Machine Learning.\nPara más información de este módulo, por favor visita la Guía del curso en esta misma página web o descárgate la Guía Docente desde la web de la Universitat de València.\n– Iván Arribas"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Predicción con Datos Temporales (36519). Grado en Inteligencia y Analítica de Negocios (GBIA)",
    "section": "License",
    "text": "License\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license."
  },
  {
    "objectID": "04-06-Covid_Nacimientos.html",
    "href": "04-06-Covid_Nacimientos.html",
    "title": "Efecto de la Covid-19 sobre la serie Nacimientos",
    "section": "",
    "text": "Introducción\nExiste un amplia y creciente literatura sobre el efecto de la Covid-19 en España en todos los ámbitos posibles: demográfico, laboral, económico, educación, salud… Dentro de la dimensión demográfica, vamos a centrarnos en el impacto de la Covid en la natalidad, concretamente, en el numero de nacimientos mensuales.\nDurante 2020 en España se tomaron diferentes medidas de confinamiento que tuvieron un impacto directo sobre las familias, afectaron la tasa de fecundidad y, por tanto, el número de nacimientos. Repasemos brevemente estas medidas:\n\nEn 14 de marzo de 2020 se decreta el estado de alarma y todas las personas deben permanecer confinadas en sus hogares.\nEl 2 de mayo empieza el proceso de desconfinamiento, permitiéndose a la gente salir a pasear cerca del domicilio y hacer deporte. Este proceso se inició en momentos diferentes en cada comunidad, dependiendo de una serie de indicadores de gravedad.\nEn las semanas sucesivas cada territorio fue relajando las medidas de movilidad entre provincias y de aforo en los locales públicos.\nEl 21 de junio finaliza el estado de alarma y se acaban todas las restricciones de movilidad.\n\nResumiendo, podemos considerar que hubo un periodo duro de confinamiento en marzo y abril, seguido de otro periodo de confinamiento más leve de mayo a junio. Como a efectos del número de nacimientos es necesario esperar nueve meses desde el inicio del confinamiento para ver su impacto, su efecto se empezaría a notar a partir de noviembre de 2020 hasta febrero de 2021.\nPor supuesto, hay mucha literatura mucha literatura que analiza el efecto de la Covid sobre la natalidad. Por ejemplo González (2021) estima que en diciembre de 2020 y enero de 2021 hubo un 21% menos de nacimientos de lo esperado; que entre noviembre de 2020 y febrero de 2021 se redujo el número de nacimientos en 13000 sobre lo esperado; y que en marzo de 2021 se volvió a los niveles prepandemia. Por otro lado, Blanes, Domingo, and Esteve (2021) estiman que la caída en diciembre de 2020 y enero de 2021 fue del 20%; que entre noviembre de 2020 y febrero de 2021 la reducción en el número de nacimientos fue de 8000 a 10000 bebés; y que en marzo y abril de 2021 hubo un ligero incremento en el número de nacimientos respecto del año previo. Las discrepancias entre los trabajos pueden deberse a la información de que disponían en el momento de su elaboración, los supuestos asumidos, la metodología empleada, etc.\nPero ¿qué podemos decir nosotros? Con los conocimientos que disponemos de análisis y predicción de series temporales, ¿en cuánto cuantificamos la caída en los nacimientos a finales del año 2020 y principios del 2021? ¿Ha habido un efecto rebote, con un mayor número de concepciones tras el confinamiento? ¿Cuándo volvió el número de nacimientos a los niveles prepandemia?\n\n\nQue dicen los datos\nComo primera aproximación, vamos a ver el número de nacimientos en los años prepandemia, durante el año 2020 y en los años postpandemia. El INE nos da información consolidada sobre el número de nacimientos hasta diciembre de 2020. Para el año 2021 los datos aun son provisionales, y hasta agosto de 2022 muestra estimaciones realizadas con su propia metodología.1\nLa Figura 1 muestra el número mensual de nacimientos desde 2018 hasta agosto de 2022. Lo primero que se observa es la tendencia decreciente en el número de nacimientos, que viene observándose desde la Gran Recesión. Con independencia del mes, los nacimientos en 2020 son inferiores a los de 2019, y estos inferiores a los de 2018.\nEn segundo lugar, claramente desde noviembre de 2020 hasta febrero de 2021, el número de nacimientos es muy inferior al esperado. En tercer lugar, desde marzo 2021 el número de nacimientos alcanza una cifra similar a la observada en el mismo mes de 2020. Es decir, si que puede existir un efecto rebote.\n\nnacimientos <- read.csv2(\"./series/nacimientos extendida.csv\",\n                         header = TRUE)\n\nnacimientos <- ts(nacimientos[, 2],\n                  start = 1975,\n                  frequency = 12)\n\nnacimientos <- window(nacimientos, \n                      start = 2000)\n\n\nggseasonplot(window(nacimientos, start = 2018)) +\n  ylab(\"Nacimientos\") +\n  xlab(\"\") +\n  ggtitle(\"\") + \n  geom_point()\n\n\n\n\nFigura 1: Nacimientos mensuales (enero 2018 - agosto 2022)\n\n\n\n\nEn cuarto y último lugar, las estimaciones del INE para 2022 parecen apuntar a que el número de nacimientos ya se ha normalizado y está en línea con lo que cabría esperar si no hubiera habido Covid-19.\nPero todas estas afirmaciones son meramente descriptivas y es necesario un análisis más riguroso para poder confirmarlas o descartarlas.\n\n\n¿Qué hubiera pasado si…?\nEn los dos epígrafes previos hemos usado constantemente la coletilla respecto de lo esperado: los nacimientos en diciembre de 2020 son inferiores a lo esperado y en marzo de 2021 superiores a lo esperado. Pero ¿qué era lo esperado?\nLa forma óptima de ver el efecto del confinamiento sobre los nacimientos sería comparar lo que realmente ha ocurrido con lo que hubiera pasado en un hipotético mundo paralelo sin Covid. Pero claro, esto es imposible, solo podemos observar una realidad y en ella la Covid ha ocurrido.\nUn forma de construir el mundo hipotético sin Covid es considerar la serie de nacimientos hasta mediados de 2020, ajustarla a un modelo y predecir los siguientes meses (desde finales de 2020 hasta julio de 2022). Podríamos interpretar estas predicciones como los nacimientos que hubieran tenido lugar en un mundo sin Covid, donde todo hubiera discurrido según la inercia de los años previos. Después, podemos comparar los nacimientos reales con las predicciones y responder a nuestra principal pregunta: ¿cómo ha afectado el confinamiento el número de nacidos?\n\nAjuste a un modelo y predicción\nVamos a considerar la serie de nacimientos desde enero de 2000 hasta octubre de 2020, ajustarla a un modelo ARIMA y predecir hasta agosto de 2022.\nEl siguiente código estima el mismo modelo visto en el tema de ARIMA con estacionalidad para la serie nacimientos. La única diferencia es que en esta ocasión la serie alcanza hasta septiembre de 2020. El ajuste, con un error porcentual del 1.5% es muy bueno.\n\nnacimientos2 <- window(nacimientos, \n                       end = c(2020, 10))\n\nDiasMes <- monthdays(nacimientos2)\nSemanaSanta <- easter(nacimientos2)\n\nd1206 <- 1*(cycle(nacimientos2) == 12 & trunc(time(nacimientos2)) == 2006)\nd1210 <- 1*(cycle(nacimientos2) == 12 & trunc(time(nacimientos2)) == 2010)\nd0111 <- 1*(cycle(nacimientos2) == 1  & trunc(time(nacimientos2)) == 2011)\nd0416 <- 1*(cycle(nacimientos2) == 4  & trunc(time(nacimientos2)) == 2016)\nd0616 <- 1*(cycle(nacimientos2) == 6  & trunc(time(nacimientos2)) == 2016)\n\nmodelo <- Arima(nacimientos2, \n                order = c(0, 1, 1),\n                seasonal =  c(0, 1, 1),\n                lambda = 0,\n                xreg = cbind(DiasMes, SemanaSanta, \n                             d1206, d1210, d0111, d0416, d0616))\nsummary(modelo)\n\nSeries: nacimientos2 \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasMes  SemanaSanta    d1206   d1210    d0111    d0416\n      -0.4915  -0.7604   0.0293      -0.0216  -0.0463  0.0616  -0.0559  -0.0511\ns.e.   0.0601   0.0451   0.0072       0.0049   0.0157  0.0163   0.0162   0.0166\n       d0616\n      0.0281\ns.e.  0.0158\n\nsigma^2 = 0.0003878:  log likelihood = 595.36\nAIC=-1170.73   AICc=-1169.75   BIC=-1136.05\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE   MAPE      MASE       ACF1\nTraining set -75.88295 699.8042 565.3364 -0.2245479 1.5423 0.4137952 0.02443195\n\n\nAhora vamos a predecir la serie desde noviembre de 2020 hasta agosto de 2022 (22 meses).\n\ntmp <- ts(rep(0, 22), start = c(2020, 11), freq = 12)\npdm <- monthdays(tmp)\npss <- easter(tmp)\nprediccion <- forecast(modelo, \n                       h = 21,\n                       xreg = cbind(pdm, pss, rep(0,22), rep(0,22), \n                                    rep(0,22), rep(0,22), rep(0,22)))\n\nLa Figura 2 muestra la serie original de nacimientos desde 2018 y la predicción. Por la metodología seguida, desde noviembre de 2020 disponemos para cada mes de dos datos: los nacimientos en el mundo real con Covid (observaciones) y los nacimientos en un mundo sin Covid (previsiones).\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Bebés\",\n         main = \"\", \n         series = \"Con Covid\") + \n  xlim(2018, 2023) +\n  ylim(20000, 35000) + \n  autolayer(prediccion, series = \"Sin Covid\", PI = FALSE) + \n  labs(colour = \"Nacimientos\") + \n  theme(legend.position=c(0.9,0.85)) \n\n\n\n\nFigura 2: Nacimientos mensuales (enero 2018 - agosto 2022) y previsiones (noviembre 2020 - agosto 2022)\n\n\n\n\nUna lectura rápida de la Figura 2 muestra que efectivamente, el número observado de nacimientos entre noviembre de 2020 y febrero de 2021 (en rojo) fue muy inferior a los valores esperados (en azul). Sin embargo, el resto del año 2021 el número de nacimientos fue superior al esperado, apuntando a un ligero efecto rebote. En 2022 el efecto de la pandemia ha desaparecido totalmente. Veamos estas observaciones en detalle.\n\n\nEfecto del confinamiento sobre los nacimientos\nEn primer lugar veamos cuál ha sido la caída en el número de nacimientos entre noviembre de 2020 y febrero de 2021.\n\nCon_covid <- as.numeric(window(nacimientos, \n                               start = c(2020, 11), \n                               end = c(2021, 2)))\nSin_covid <- as.numeric(window(prediccion$mean, \n                               start = c(2020, 11), \n                               end = c(2021, 2)))\n# Caida porcentual\n(Con_covid - Sin_covid)/Con_covid\n\n[1] -0.06893514 -0.20764127 -0.19638402 -0.03668175\n\n#Caida en el total de nacidos\nsum(Con_covid - Sin_covid)\n\n[1] -12311.26\n\n\nLa mayor caída porcentual en el número de nacimientos tuvo lugar en los meses de diciembre de 2020 y enero de 2021 (20.7% y 19.6%, respectivamente) y nuestras estimaciones coinciden con las aportadas en González (2021) y Blanes, Domingo, and Esteve (2021). Respecto del número de nacimientos, nosotros estimamos una reducción de 12300 nacidos, un valor algo inferior a la estimación en González (2021) y muy superior a la estimación en Blanes, Domingo, and Esteve (2021).\n\n\nEfecto rebote\nUna posibilidad es que el confinamiento no hizo que las parejas decidieran no tener hijos de forma permanente, sino que simplemente retrasó la decisión de tenerlos. Si esto es así, cabria esperar a mediados o finales de 2021 un número de nacimientos superior al esperado: por una lado tendríamos los nacimientos de las parejas que tenían pensado tener hijos en ese momento y por otro los de las parejas que habían retrasado el momento de la maternidad. Si es así, la Covid no habría reducido de forma permanente el número de nacimientos y el acumulado en el medio/largo plazo seria el mismo que si no hubiera habido Covid.\nPara poder responder mejor a esta pregunta, vamos a calcular la diferencia acumulada entre el número de nacimientos esperado y el real desde noviembre de 2020 hasta agosto de 2022. La diferencia acumulada crece hasta los 12300 bebés en febrero de 2021. Esta diferencia máxima se va reduciendo lentamente hasta los 4500 bebés en diciembre de 2021. Es decir, efectivamente parece que hay un efecto rebote que ha compensado a lo largo del año 2021 en un total de 7800 bebés la caída hasta febrero de ese año.\nDurante el año 2022 la diferencia acumulada ha seguido reduciéndose hasta desaparecer en agosto de 2022. Sin embargo, hay que tener en cuenta que en 2022 estamos comparando las previsiones de nuestro modelo con las estimaciones del INE, así que las conclusiones que obtengamos son muy poco fiables.\n\nCon_covid <- window(nacimientos, start = c(2020, 11))\nSin_covid <- window(prediccion$mean, start = c(2020, 11))\n\nDiferencia <- cumsum(Sin_covid - Con_covid)\nDiferencia <- ts(Diferencia, \n                 start = c(2020, 11),\n                 frequency = 12)\n\n\nautoplot(Diferencia,\n         xlab = \"\",\n         ylab = \"Bebés\",\n         main = \"\") + \n  xlim(2020.7, 2023) +\n   scale_x_continuous(breaks= seq(2020 + 11/12, 2022 + 7/12, 2/12),\n                     label = c(\"Dic-20\", \"Feb-21\", \"Abr-21\", \"Jun-21\",\n                               \"Ago-21\", \"Oct-21\", \"Dic-21\", \"Feb-22\",\n                               \"Abr-22\", \"Jun-22\", \"Ago-22\"))\n\n\n\n\nFigura 3: Déficit acumulado de nacimientos desde noviembre de 2020\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBlanes, Amand, Andreu Domingo, and Albert Esteve. 2021. “Consecuencias Demográficas de La COVID-19 En España: Entre La Novedad Excepcional y La Reincidencia Estructural.” Panorama Social 33. https://www.funcas.es/wp-content/uploads/2021/07/Esteve.pdf.\n\n\nGonzález, Libertad. 2021. “La Natalidad En España Durante La Pandemia: Actualización.” Nada Es Gratis. https://nadaesgratis.es/libertad-gonzalez/la-natalidad-en-espana-durante-la-pandemia-actualizacion.\n\nFootnotes\n\n\nLos datos para los años 2021 y 2022 fueron descargados en noviembre de 2022.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Iván Arribas",
    "section": "",
    "text": "Iván Arribas es licenciado en Ciencias Matemáticas con la especialidad de Estadística e Investigación Operativa, y doctor en Ciencias Económicas. Actualmente es Profesor Titular en el Departamento de Análisis Económico de la Universitat de València, donde lleva más de 25 años como docente e investigador."
  },
  {
    "objectID": "03-12-Ejemplo6.html#la-fig-pernoctacionesfac-muestra-una-serie-con-tendencia-y-la-función-ndiffs-que-realiza-un-contraste-formal-de-estacionariedad-también-indica-que-la-serie-no-es-estacionaria.-sin-embargo-vimos-que-el-modelo-de-alisado-que-mejor-ajustaba-a-los-datos-era-un-modelo-sin-tendencia.",
    "href": "03-12-Ejemplo6.html#la-fig-pernoctacionesfac-muestra-una-serie-con-tendencia-y-la-función-ndiffs-que-realiza-un-contraste-formal-de-estacionariedad-también-indica-que-la-serie-no-es-estacionaria.-sin-embargo-vimos-que-el-modelo-de-alisado-que-mejor-ajustaba-a-los-datos-era-un-modelo-sin-tendencia.",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "2.1 La Figura 1 muestra una serie con tendencia y la función ndiffs (que realiza un contraste formal de estacionariedad) también indica que la serie no es estacionaria. Sin embargo, vimos que el modelo de alisado que mejor ajustaba a los datos era un modelo sin tendencia.",
    "text": "2.1 La Figura 1 muestra una serie con tendencia y la función ndiffs (que realiza un contraste formal de estacionariedad) también indica que la serie no es estacionaria. Sin embargo, vimos que el modelo de alisado que mejor ajustaba a los datos era un modelo sin tendencia.\nAnte esta situación se opta por explorar ambas opciones. Es decir, analizaremos la serie \\(Pernoctaciones \\sim I(0)\\) así como la serie \\(Pernoctaciones \\sim I(1)\\) y veremos cual de las dos nos ofrece mejores predicciones.\n\nautoplot(Pernoctaciones, xlab = \"\", ylab = \"\", main = \"\")\nautoplot(diff(Pernoctaciones), xlab = \"\", ylab = \"\", main = \"\")\nggAcf(Pernoctaciones, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(Pernoctaciones), xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n(b) Dif. de la serie\n\n\n\n\n\n\n\n\n\n(c) FAC serie original\n\n\n\n\n\n\n\n(d) FAC dif. de la serie\n\n\n\n\nFigura 1: Gráfica y FAC para Pernoctaciones\n\n\n\n\nndiffs(Pernoctaciones)\n\n[1] 1"
  },
  {
    "objectID": "03-12-Ejemplo6.html#identificación",
    "href": "03-12-Ejemplo6.html#identificación",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.1 Identificación",
    "text": "3.1 Identificación\nVamos a identificar los valores de \\(p\\) y \\(q\\) a partir de auto.arima.\n\nauto.arima(Pernoctaciones, \n           d = 0)\n\nSeries: Pernoctaciones \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n         ar1     ma1      mean\n      0.8827  0.5341  253.6161\ns.e.  0.1027  0.2803   24.1170\n\nsigma^2 = 136.3:  log likelihood = -77.21\nAIC=162.43   AICc=165.09   BIC=166.41\n\n\nSe identifica un proceso ARIMA(1, 0, 1) con constante. La estimación de este modelo muestra que el coeficiente asociado al proceso autorregresivo es significativo, pero no está claro que lo sea el coeficiente de la media móvil. El análisis del error no revela ninguno que supere las 2.5 desviaciones típicas (Figura 2).\n\narima101 <- Arima(Pernoctaciones, \n                  order = c(1, 0, 1),\n                  include.constant = TRUE)\n\nerror <- residuals(arima101)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(2000, 2020, 2)) \n\n\n\n\nFigura 2: Error + Intervención"
  },
  {
    "objectID": "03-12-Ejemplo6.html#validación",
    "href": "03-12-Ejemplo6.html#validación",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "3.2 Validación",
    "text": "3.2 Validación\n\nVariables son significativas\nLos coeficientes asociados al proceso autorregresivo y la media (\\(\\phi_1\\) y \\(\\mu\\)) son significativos. El coeficiente de la media móvil (\\(\\theta_1\\)) no es significativo al 5% aunque si lo sería al 10%.\n\n# H0: phi1 = 0\nwald.test(b = coef(arima101), Sigma = vcov(arima101), Terms = 1)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 73.9, df = 1, P(> X2) = 0.0\n\n# H0: theta1 = 0\nwald.test(b = coef(arima101), Sigma = vcov(arima101), Terms = 2)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 3.6, df = 1, P(> X2) = 0.057\n\n# H0: constante = 0\nwald.test(b = coef(arima101), Sigma = vcov(arima101), Terms = 3)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 110.6, df = 1, P(> X2) = 0.0\n\n\nSi eliminamos el coeficiente de la media móvil y estimamos un modelo ARIMA(1, 0, 0) con constante, se puede comprobar que el nuevo modelo pierde ligeramente en calidad y presenta un ACF1 muy elevado, indicativo de que las fórmulas usadas para el cálculo del intervalo de confianza de las previsiones es incorrecto. Así, se opta por aceptar como válido el modelo ARIMA(1, 0 , 1) con constante.\n\n\n\n\nMedidas de error\nEl error medio es 10.8 millones de pernoctaciones (RMSE) y el error porcentual medio es 3.4% (MAPE).\n\naccuracy(arima101)\n\n\n\n               ME  RMSE  MAE  MPE MAPE MASE ACF1\nTraining set 1.03 10.76 8.06 0.17 3.45 0.86    0\n\n\n\n\nError de previsión extramuestral según horizonte temporal\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k=10\\), y que el horizonte temporal es tres años vista, \\(h = 3\\).\n\nk <- 10                  \nh <- 3                    \nT <- length(Pernoctaciones)     \ns <- T - k - h    \n\nmapeArima101 <- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set <- subset(Pernoctaciones, start = i + 1, end = i + k)\n  test.set <-  subset(Pernoctaciones, start = i + k + 1, end = i + k + h) \n  \n  fit <- Arima(train.set, \n               include.constant = TRUE,\n               order = c(1, 0, 1))\n  \n  fcast <- forecast(fit, h = h)\n  \n  mapeArima101[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n}\n\nmapeArima101 <- colMeans(mapeArima101)\nmapeArima101\n\n[1]  6.537726 12.420501 15.659016\n\n\nEl error de previsión extramuestral un periodo vista (6.5%) es notoriamente mayor que el error de estimación (3.9%). El error a dos y tres periodos vista supera el 10%."
  },
  {
    "objectID": "03-12-Ejemplo6.html#identificación-1",
    "href": "03-12-Ejemplo6.html#identificación-1",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "4.1 Identificación",
    "text": "4.1 Identificación\nIdentificaremos los valores de \\(p\\) y \\(q\\) a partir de auto.arima.\n\nauto.arima(Pernoctaciones, \n           d = 1)\n\nSeries: Pernoctaciones \nARIMA(0,1,0) \n\nsigma^2 = 146.4:  log likelihood = -74.33\nAIC=150.66   AICc=150.9   BIC=151.61\n\n\nSe identifica un proceso ARIMA(0, 1, 0) sin deriva, es decir un paseo aleatorio o método Ingenuo I. Tras estimar el modelo, el análisis del error revela de nuevo que no es necesaria la intervención (Figura 3).\n\narima010 <- Arima(Pernoctaciones, \n                  order = c(0, 1, 0),\n                  include.constant = FALSE)\n\narima010\n\nSeries: Pernoctaciones \nARIMA(0,1,0) \n\nsigma^2 = 146.4:  log likelihood = -74.33\nAIC=150.66   AICc=150.9   BIC=151.61\n\nerror <- residuals(arima010)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(2000, 2020, 2))\n\n\n\n\nFigura 3: Error + Intervención"
  },
  {
    "objectID": "03-12-Ejemplo6.html#validación-1",
    "href": "03-12-Ejemplo6.html#validación-1",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "4.2 Validación",
    "text": "4.2 Validación\n\nMedidas de error\nEl error medio es 11.8 pernoctaciones (RMSE) y el error porcentual medio es 3.7% (MAPE). Los intervalos de confianza de las predicciones no son válidos.\n\naccuracy(arima010)\n\n\n\n               ME  RMSE MAE  MPE MAPE MASE ACF1\nTraining set 3.27 11.79 8.9 1.11  3.7 0.95 0.27\n\n\n\n\nError de previsión extramuestral según horizonte temporal\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k=10\\), y que el horizonte temporal es tres años vista, \\(h = 3\\).\n\nk <- 10                  \nh <- 3                    \nT <- length(Pernoctaciones)     \ns <- T - k - h    \n\nmapeArima010 <- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set <- subset(Pernoctaciones, start = i + 1, end = i + k)\n  test.set <-  subset(Pernoctaciones, start = i + k + 1, end = i + k + h) \n  \n  fit <- Arima(train.set, \n               include.constant = FALSE,\n               order = c(0, 1, 0))\n  \n  fcast <- forecast(fit, h = h)\n  \n  mapeArima010[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n}\n\nmapeArima010 <- colMeans(mapeArima010)\nmapeArima010\n\n[1]  5.095108  8.957097 11.429987\n\n\nEl error de previsión varia entre el 5.1% a un periodo vista y el 11.4% a tres periodos vista."
  },
  {
    "objectID": "03-06-Tema6.html#definición-e-hipótesis-sobre-el-proceso",
    "href": "03-06-Tema6.html#definición-e-hipótesis-sobre-el-proceso",
    "title": "Procesos ARIMA",
    "section": "2.1 Definición e hipótesis sobre el proceso",
    "text": "2.1 Definición e hipótesis sobre el proceso\nUn proceso estocástico \\(Y_t\\) es (sin excesiva precisión) una variable aleatoria que corresponde a momentos sucesivos del tiempo. A diferencia de los temas previos, en este vamos a estimar modelos sobre procesos estocásticos. Sería el equivalente para series temporales al modelo de regresión lineal que viste en el primer semestre para datos transversales.\nAl igual que en Predicción con datos transversales, la aplicación de estos modelos requiere del cumplimiento de una serie de hipótesis. Para el caso de series temporales el proceso debe ser estacionario, ergódico y normal. Veamos estas hipótesis con algo más de detalle."
  },
  {
    "objectID": "03-06-Tema6.html#proceso-estacionario",
    "href": "03-06-Tema6.html#proceso-estacionario",
    "title": "Procesos ARIMA",
    "section": "2.2 Proceso estacionario",
    "text": "2.2 Proceso estacionario\nUn proceso es estacionario en sentido estricto cuando la distribución conjunta no varía al realizar un desplazamiento en el tiempo de todas las variables.\n\nSi \\(F(Y_{t_1},..., Y_{t_k})\\) es la función de distribución conjunta y \\(h>0\\), entonces el proceso es estacionario en sentido estricto si \\[F(Y_{t_1},..., Y_{t_k}) = F(Y_{t_1+h},..., Y_{t_k+h})\\]\n\nIntuitivamente, la distribución de un proceso estocástico es independiente del momento del tiempo.\nComprobar si un proceso es estacionario en sentido estricto es muy difícil, así que vamos a encontrar condiciones suficientes: estacionariedad en media y en sentido amplio (covarianza). Bajo normalidad un proceso estacionario en sentido amplio también lo será en sentido estricto.\nProceso estacionario en media\nUn proceso es estacionario en media (o de primer orden) si su nivel se mantiene en el tiempo: \\[E[Y_t] = \\mu \\; \\; \\forall t\\]\nProceso estacionario en sentido amplio\nUn proceso (ya estacionario en media) es estacionario en sentido amplio, o de segundo orden, si sus momentos de orden dos no dependen del tiempo:\n\nLa (auto)covarianza entre dos periodos de tiempo es finita y sólo depende del intervalo de tiempo transcurrido entre estos dos periodos: \\[Cov[Y_t, Y_{t+k}] = E[(Y_t - \\mu)(Y_{t+k} - \\mu)] = \\gamma_k,\\,\\,\\,\\forall t\\]\n\nObserva que la varianza será entonces \\(Var[Y_t] = E[(Y_t - \\mu)^2] = \\gamma_0\\).\nSi en el contexto de series temporales la autocovarianza la interpretamos como la información de la serie que se transmite entre dos periodos de tiempo \\(t\\) y \\(t'\\), el supuesto de estacionariedad en sentido amplio nos dice que la información transmitida solo depende de la distancia temporal entre los dos periodos \\(t - t'\\) y no los periodos en si mismos. Por ejemplo, los nacimientos en enero de un año me dan información sobre los nacimientos en enero del año siguiente, y esta información es la misma e independiente del año en consideración. Además, la información transmitida entre dos eneros consecutivos es la misma que entre dos febreros o dos marzos consecutivos porque la distancia es la misma, 12 meses.\n\n\nLa Figura 1 muestra la serie Nacimientos que no es estacionaria ni en media, ni en varianza. No lo es en media por que presenta tendencia y, por tanto, el valor medio de la serie cambia en el tiempo; y no lo es estacionaria en varianza por que al inicio de la serie los datos presenta más variabilidad que a finales del siglo pasado.\n\n\n\n\n\nFigura 1: Nacimientos mensuales"
  },
  {
    "objectID": "03-06-Tema6.html#proceso-ergódico",
    "href": "03-06-Tema6.html#proceso-ergódico",
    "title": "Procesos ARIMA",
    "section": "2.3 Proceso ergódico",
    "text": "2.3 Proceso ergódico\nPara que un proceso sea ergódico las observaciones nuevas tienen que aportar suficiente información para que la varianza del valor medio converja a 0.\nUna condición necesaria, pero no suficiente, para que un proceso estacionario sea ergódico es:\n\\[\\lim_{k\\rightarrow \\infty} \\gamma_k = 0.\\]\nEs decir, que cuanto más distancia hay entre dos periodos, menos información se transmite. Alternativamente, que el pasado cada vez ayuda menos a entender el presente."
  },
  {
    "objectID": "03-06-Tema6.html#normalidad",
    "href": "03-06-Tema6.html#normalidad",
    "title": "Procesos ARIMA",
    "section": "2.4 Normalidad",
    "text": "2.4 Normalidad\nAsumiremos que el error del modelo se distribuye como una variable aleatoria normal. Esta hipótesis se puede relajar si la serie tiene suficientes datos."
  },
  {
    "objectID": "03-06-Tema6.html#ideas-generales",
    "href": "03-06-Tema6.html#ideas-generales",
    "title": "Procesos ARIMA",
    "section": "3.1 Ideas generales",
    "text": "3.1 Ideas generales\nUna serie temporal \\(\\{y_t\\}_{t=1}^T\\) no tiene porque verificar las condiciones de estacionariedad y ergodicidad. A continuación veremos una serie de transformaciones que convierten una serie no estacionaria en estacionaria; y no ergódica en ergódica.\nEn el panel superior de la Figura 2 vuelves a tener la serie de nacimientos, que denominaremos \\(y_t\\); en el segundo panel la diferencia de la serie, \\(y_t - y_{t-1}\\); y en el panel inferior tienes la diferencia de la transformación logarítmica de la serie, \\(log(y_t) - log(y_{t-1})\\). La serie nacimientos no es estacionaria en media ni en varianza, su diferencia es estacionaria en media, pero no en varianza. Sin embargo, la transformación logarítmica y la diferencia han logrado que sea estacionaria en ambos sentidos.\n\n\n\n\n\nFigura 2: Serie Nacimientos, su diferencia y la diferencia del logaritmo\n\n\n\n\nEn este tema veremos con detalle estas dos transformaciones –diferenciación y logaritmo– para series sin estacionalidad y dejaremos para el tema siguiente el caso de las series con estacionalidad."
  },
  {
    "objectID": "03-06-Tema6.html#diferenciación",
    "href": "03-06-Tema6.html#diferenciación",
    "title": "Procesos ARIMA",
    "section": "3.2 Diferenciación",
    "text": "3.2 Diferenciación\nLa diferenciación permite transformar una serie no estacionaria en media en estacionaria en media.\nDiferenciar de orden \\(k\\) consiste en restar a la observación de un periodo la de \\(k\\) periodos antes: \\[\\nabla_k y_t = y_t - y_{t-k}.\\]\n\nDiferenciación regular (\\(k=1\\))\nUn caso concreto es la diferenciación regular o diferenciación de orden uno, que consiste en restar a la observación de un periodo la del periodo precedente: \\[\\nabla y_t = y_t - y_{t-1}.\\]\nSi \\(\\nabla y_t\\) no fuera estacionaria, se diferenciaría una segunda vez para obtener una doble diferenciación de primer orden: \\[\\nabla^{2} y_t = \\nabla(\\nabla y_t) = \\nabla y_t - \\nabla y_{t-1} = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2}\\]\nEn la práctica una sola diferenciación suele ser suficiente para obtener la estacionariedad en media; diferenciar dos veces es excepcional; y diferenciar tres o más veces no se da.\nExiste la diferencia estacional, que consiste en restar a la observación de un periodo la observación precedente de la misma estación y que veremos en detalle en el tema 7.\nAdemás, en series sin estacionalidad la diferenciación también permite alcanzar la ergodicidad.\nLa Figura 3 muestra un ejemplo de diferenciación regular. En el panel superior aparece la serie anual de Nacimientos \\(y_t\\); en el segundo panel se muestra la serie diferenciada regularmente una vez \\(\\nabla y_t\\); y en el panel inferior la serie doblemente diferenciada \\(\\nabla^2 y_t\\).\n\n\n\n\n\nFigura 3: Nacimientos (anual), su primera y su segunda diferencia regular\n\n\n\n\n¿Qué transformación para nacimientos consideras que genera una serie estacionaria? Siempre hay un cierto grado de subjetividad en la elección de las diferencias que hay que aplicar a una serie. En la Figura 3 podemos considerar que la diferenciación regular (panel medio) es suficiente para lograr la estacionariedad (en media y en varianza) y terminar el proceso de diferenciación. Pero también podemos considerar que la serie es no del todo estacionaria en media, y optar por la doble diferenciación regular (panel inferior).\n\n\nDiferenciación con R\nR dispone de la función diff para diferenciar una serie:\n\ndiff(x, lag = k) calcula la diferencia de orden \\(k\\), \\(\\nabla_k y_t\\)\ndiff(x) calcula la diferencia regular o de orden \\(1\\), \\(\\nabla y_t\\) (el valor por defecto de lag es 1)\ndiff(x, difference = d) calcula \\(d\\) diferencias regulares, \\(\\nabla^d y_t\\)\n\nAdemás, en forecast está disponible la función ndiffs que estima el número de diferencias regulares necesarias para que una serie sea estacionaria. Para ello usa un contraste de raíces unitarias (que no veremos en este curso).\n\n\nOperador Retardo\nDefinimos el operador retardo \\(L\\) como \\(Ly_t = y_{t-1}\\), es decir, retrasa un periodo la serie. En inglés se denomina lag operator (L) o backward shift (B)\nAsí, se tiene que \\[L^k y_t = y_{t-k}\\] y por tanto que\n\\[\n\\begin{aligned}\n  \\nabla y_t & =  y_t - y_{t-1} = y_t - Ly_t = (1-L)y_t \\\\\n  \\nabla^d y_t & =  (1-L)^d y_t\n\\end{aligned}\n\\]\nLa Tabla 1 muestra un sencillo ejemplo del efecto del operador retardo sobre la serie \\(y_t\\)\n\n\n\n\nTabla 1: Ejemplo de aplicación del operador retardo\n\n\ny\nlag1_y\nlag2_y\nlag3_y\n\n\n\n\n1\nNA\nNA\nNA\n\n\n2\n1\nNA\nNA\n\n\n3\n2\n1\nNA\n\n\n4\n3\n2\n1\n\n\n5\n4\n3\n2\n\n\n6\n5\n4\n3\n\n\n7\n6\n5\n4"
  },
  {
    "objectID": "03-06-Tema6.html#transformación-logarítmica",
    "href": "03-06-Tema6.html#transformación-logarítmica",
    "title": "Procesos ARIMA",
    "section": "3.3 Transformación logarítmica",
    "text": "3.3 Transformación logarítmica\nSi la serie original no es estacionaria en sentido amplio –por ejemplo porque la amplitud estacional crece con el nivel de la serie–, es posible obtener la estacionariedad por medio de transformaciones simples. La transformación logarítmica de una serie es una alternativa1.\nLa Figura 4 muestra la serie Nacimientos y su logaritmo. La variabilidad estacional con la transformación logarítmica (panel inferior) es más constante que en la serie original (panel superior).\n\n\n\n\n\nFigura 4: Serie Nacimientos y su transformaciones logarítmica\n\n\n\n\nSin embargo, para series sin estacionalidad, el uso de la transformación logarítmica para alcanzar la estacionariedad en varianza es muy excepcional."
  },
  {
    "objectID": "03-06-Tema6.html#diferencia-logaritmo-y-tasa-de-variación",
    "href": "03-06-Tema6.html#diferencia-logaritmo-y-tasa-de-variación",
    "title": "Procesos ARIMA",
    "section": "3.4 Diferencia, Logaritmo y Tasa de variación",
    "text": "3.4 Diferencia, Logaritmo y Tasa de variación\nLa transformación \\(\\nabla y_t\\) se puede interpretar como variaciones en nivel. Sin embargo, cuando una serie tiene que ser diferenciada para conseguir su estacionariedad, vale la pena probar una transformación alternativa que también es interpretable: \\(\\nabla \\log(y_t)\\).\nDado que, \\[\\nabla \\log(y_t) = \\log(y_t) - \\log(y_{t-1}) = \\log\\big(\\frac{y_t}{y_{t-1}} \\big)\n  \\approx \\frac{y_t}{y_{t-1}} - 1 = \\frac{y_t - y_{t-1}}{y_{t-1}} =TV y_t.\\]\nPara una serie la diferencia regular del logaritmo (natural) es la Tasa de Variación de la serie, que tiene una clara interpretación como variación porcentual. Si la serie es anual, tendremos la tasa de variación anual; si es semanal, la tasa de variación semanal.\n\n\n\n\n\n\n\n\nLa diferenciacion y el logaritmo en series sin estacionalidad\n\n\n\n\nSi se tiene una serie sin estacionalidad y no estacionaria, bastará diferenciarla una o, a lo sumo, dos veces para que sea estacionaria en media, en sentido amplio y ergódica.\nLa transformación logarítmica se puede usar si se desea ganar en interpretabilidad o para intentar mejorar las predicciones.\nLa transformación logarítmica no es necesaria para alcanzar la estacionariedad de la serie."
  },
  {
    "objectID": "03-06-Tema6.html#procesos-autorregresivos-arp",
    "href": "03-06-Tema6.html#procesos-autorregresivos-arp",
    "title": "Procesos ARIMA",
    "section": "5.1 Procesos autorregresivos AR(p)",
    "text": "5.1 Procesos autorregresivos AR(p)\n\nDefinición\nEl modelo general autorregresivo de orden p, \\(y_t \\sim AR(p)\\) viene definido por \\[y_t=c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\varepsilon_t,\\] que usando el operador retardo queda \\[(1 - \\phi_1 L - \\phi_2 L^2 - \\ldots - \\phi_p L^p)y_t = c + \\varepsilon_t\\]\nEn este y en cualquier proceso ARIMA, al polinomio en \\(L\\) que acompaña a \\(y_t\\) se le denomina polinomio autorregresivo.\nSe suele asumir que el error del modelo \\(\\varepsilon_t\\) verifica las hipótesis estándar de media cero, incorrelación, homocedasticidad e idéntica distribución: \\(\\varepsilon_t \\sim iid(0, \\sigma^2)\\). En este curso no vamos a prestar atención a este conjunto de hipótesis porque no jugarán ningún papel en la elección del modelo óptimo –aquel con mejores predicciones.\n\n\nEjemplos\n\n\\(y_t \\sim AR(1): \\;\\;y_t = c + \\phi_1 y_{t-1} + \\varepsilon_t\\) o \\((1 - \\phi_1 L)y_t = c + \\varepsilon_t\\)\n\\(y_t \\sim AR(2): \\;\\;y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\varepsilon_t\\) o \\((1 - \\phi_1 L - \\phi_2 L^2)y_t = c + \\varepsilon_t\\)"
  },
  {
    "objectID": "03-06-Tema6.html#procesos-en-medias-móviles-maq",
    "href": "03-06-Tema6.html#procesos-en-medias-móviles-maq",
    "title": "Procesos ARIMA",
    "section": "5.2 Procesos en medias móviles MA(q)",
    "text": "5.2 Procesos en medias móviles MA(q)\n\nDefinición\nEl modelo general en medias móviles de orden q, \\(y_t \\sim MA(q)\\) viene definido por \\[y_t=c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\ldots + \\theta_q \\varepsilon_{t-q},\\] que usando el operador retardo queda \\[y_t = c + (1 + \\theta_1 L + \\theta_2 L^2 + \\ldots + \\theta_q L^q) \\varepsilon_t\\]\nEn este y en cualquier proceso ARIMA, al polinomio en \\(L\\) que acompaña a \\(\\varepsilon_t\\) se le denomina polinomio en medias móviles.\n\n\nEjemplos\n\n\\(y_t \\sim MA(1): \\;\\;y_t = c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}\\) o \\(y_t = c + (1 + \\theta_1 L)\\varepsilon_t\\)\n\\(y_t \\sim MA(2): \\;\\;y_t=c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2}\\) o \\(y_t = c + (1 + \\theta_1 L + \\theta_2 L^2)\\varepsilon_t\\)"
  },
  {
    "objectID": "03-06-Tema6.html#procesos-armapq",
    "href": "03-06-Tema6.html#procesos-armapq",
    "title": "Procesos ARIMA",
    "section": "5.3 Procesos ARMA(p,q)",
    "text": "5.3 Procesos ARMA(p,q)\n\nDefinición\nEl modelo general \\(y_t \\sim ARMA(p,q)\\) viene dado por \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p}  +\n        \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\ldots +\n        \\theta_q \\varepsilon_{t-q}+ \\varepsilon_t,\\] que usando el operador retardo queda \\[(1 - \\phi_1 L - \\ldots - \\phi_p L^p)y_t = c + (1 + \\theta_1 L + \\ldots + \\theta_q L^q) \\varepsilon_t.\\]\n\n\nEjemplos\n\n\\(y_t \\sim ARMA(1, 1): \\;\\;y_t = c + \\phi_1 y_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\varepsilon_{t}\\) o \\((1 - \\phi_1 L)y_t = c + (1 + \\theta_1 L)\\varepsilon_t\\)\n\\(y_t \\sim ARMA(0, 0): \\;\\;y_t = c + \\varepsilon_{t}\\). Si \\(c = 0\\), a este proceso se le denomina ruido blanco."
  },
  {
    "objectID": "03-06-Tema6.html#proceso-arimapdq",
    "href": "03-06-Tema6.html#proceso-arimapdq",
    "title": "Procesos ARIMA",
    "section": "5.4 Proceso ARIMA(p,d,q)",
    "text": "5.4 Proceso ARIMA(p,d,q)\nSi la serie \\(y_t\\) no es estacionaria, pero tras diferenciarla \\(d\\) veces se hace estacionaria, diremos que la serie es integrada de orden \\(d\\): \\(y_t \\sim I(d)\\). Por tanto,\n\nuna serie estacionaria se indicará como \\(y_t \\sim I(0)\\)\n\\(y_t \\sim I(d)\\) es equivalente a \\(\\nabla^d y_t = (1 - L)^d y_t \\sim I(0)\\)\n\nUna serie \\(y_t\\) sigue un proceso \\(ARIMA(p,d,q)\\) si:\n\n\\(y_t \\sim I(d)\\), hay que diferenciarla \\(d\\) veces para hacerla estacionaria, y\n\\(\\nabla^d y_t \\sim ARMA(p,q)\\).\n\nEntonces, podemos escribir \\(y_t \\sim ARIMA(p,d,q)\\):\n\\[\\begin{equation*}\n  \\begin{array}{c@{\\qquad}c@{\\quad}ccc}\n   (1 - \\phi_1 L - \\ldots - \\phi_p L^p) & (1- L)^d y_t & = & c + (1 + \\theta_1 L + ... + \\theta_q L^q) \\varepsilon_t \\\\\n                           \\uparrow                            & \\uparrow      &   & \\uparrow \\\\\n                           AR(p)                               & I(d)          &   & MA(q)\n  \\end{array}\n\\end{equation*}\\]\n\nEjemplos\n\n\\(y_t \\sim ARIMA(1, 1, 1): \\;\\;(1 - \\phi_1 L)(1- L) y_t = c + (1 + \\theta_1 L) \\varepsilon_t\\) o \\(y_t = c + y_{t-1} + \\phi_1(y_{t-1} - y_{t-2}) + \\theta_1 \\varepsilon_{t-1} + \\varepsilon_t\\).\n\\(\\log(y_t) \\sim ARIMA(1, 1, 1): \\;\\;(1 - \\phi_1 L)(1- L) \\log(y_t) = (1 - \\phi_1 L)TVy_t = c + (1 + \\theta_1 L) \\varepsilon_t\\) o \\(TVy_t = c + \\phi_1 TVy_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\varepsilon_t\\).\n\\(y_t \\sim ARIMA(0, 1, 0): \\;\\;(1- L) y_t = c + \\varepsilon_t\\) o \\(y_t = c + y_{t-1} + \\varepsilon_t\\). Si \\(c=0\\), tenemos un paseo aleatorio; si \\(c \\neq 0\\), tenemos un paseo aleatorio con deriva."
  },
  {
    "objectID": "03-06-Tema6.html#títulos-de-libros-y-panfletos",
    "href": "03-06-Tema6.html#títulos-de-libros-y-panfletos",
    "title": "Procesos ARIMA",
    "section": "7.1 Títulos de libros y panfletos",
    "text": "7.1 Títulos de libros y panfletos\nVamos a aplicar la metodología de Box-Jenkins a la serie Libros (número de títulos publicados anualmente en España desde 1993 hasta 2019).\n\nlibros <- read.csv2(\"./series/libros.csv\", header = TRUE)\nlibros <- ts(libros[, 2], start = 1993, frequency = 1)\n\nautoplot(libros,\n         xlab = \"\", \n         ylab = \"Titulos\", \n         main = \"\")\n\n\n\n\nFigura 7: Títulos publicados\n\n\n\n\n\nTransformación de la serie\nEl primer paso es transformar la serie original para que sea estacionaria. La Figura 8 muestra la gráfica temporal y la FAC para la serie original y su primera diferencia.\n\nautoplot(libros, xlab = \"\", ylab = \"\", main = \"\")\nautoplot(diff(libros), xlab = \"\", ylab = \"\", main = \"\")\nggAcf(libros, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(libros), xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n(b) Dif. de la serie\n\n\n\n\n\n\n\n\n\n(c) FAC serie original\n\n\n\n\n\n\n\n(d) FAC dif. de la serie\n\n\n\n\nFigura 8: Gráfica y FAC para Libros\n\n\nAdemás,\n\nndiffs(libros)\n\n[1] 1\n\n\nPodemos concluir que la primera diferencia de la serie Libros es estacionaria y ergódica. Es decir, \\(d=1\\) o \\(libros_t \\sim I(1)\\).\n\n\nIdentificación\nTras diferenciar la serie, vamos a identificar los valores de \\(p\\) y \\(q\\). Este es el proceso más difícil y para simplificar las cosas vamos a ayudarnos de la función auto.arima. Como ya hemos decidido el número de diferenciaciones, fijamos este parámetro con d = 1.\n\nauto.arima(libros, \n           d = 1, \n           trace = TRUE)\n\n\n ARIMA(2,1,2) with drift         : Inf\n ARIMA(0,1,0) with drift         : 529.2727\n ARIMA(1,1,0) with drift         : 531.8087\n ARIMA(0,1,1) with drift         : 531.8033\n ARIMA(0,1,0)                    : 527.5277\n ARIMA(1,1,1) with drift         : Inf\n\n Best model: ARIMA(0,1,0)                    \n\n\nSeries: libros \nARIMA(0,1,0) \n\nsigma^2 = 34911235:  log likelihood = -262.68\nAIC=527.36   AICc=527.53   BIC=528.62\n\n\nObserva como la identificación automática da como mejor modelo \\(p=q=0\\). Es decir \\(libros_t \\sim ARIMA(0,1,0)\\) sin deriva (sin constante), por tanto, sigue un paseo aleatorio: \\[libros_t = libros_{t-1} + \\varepsilon_t\\]\n\n\nEstimación\nAunque existe la función arima de stats, vamos a usar la función Arima de la librería forecast para estimar el modelo identificado por ser más versátil. El argumento order indica los valores de (p, d , q) como un vector y el argumento lógico include.constant indica si se desea incluir la constante \\(c\\) en el modelo.2.\n\narima010 <- Arima(libros, \n                 order=c(0, 1, 0), \n                 include.constant = FALSE)\narima010\n\nSeries: libros \nARIMA(0,1,0) \n\nsigma^2 = 34911235:  log likelihood = -262.68\nAIC=527.36   AICc=527.53   BIC=528.62\n\n\nNuestro modelo estimado es: \\(\\widehat{libros}_t = libros_{t-1}\\). La mejor predicción para un año, es la observación del año anterior, ¡el método ingenuo I!\n\n\nIntervención\nSe analiza si para algún año se observa un error atípico (por ejemplo 3 veces superior al error estándar). La Figura 9 muestra que en este caso en varios periodos, años 2008, 2009 y 2013, el residuo sobrepasa los dos errores estándar pero queda lejos de los tres errores estándar así que asumiremos que no hay valores atípicos.\n\nerror <- residuals(arima010)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1993, 2019, 2)) \n\n\n\n\nFigura 9: Error + Intervención\n\n\n\n\n\n\nMedidas de error\nEl error medio es 5798 títulos (RMSE) y el error porcentual medio (MAPE) es 6.88%.\n\naccuracy(arima010)\n\n\n\n                 ME    RMSE     MAE  MPE MAPE MASE  ACF1\nTraining set 868.03 5798.12 4358.32 1.29 6.88 0.96 -0.04\n\n\nAdemás, hay cierto sesgo (las previsiones intramuestrales en media subestiman los datos reales) y la fórmula usada para la previsión por intervalo es correcta.\n\n\nPredicción\nUna vez validado el modelo podemos pasar a realizar predicciones, en este caso a 5 años vista.\n\nparima010 <- forecast(arima010, \n                      h = 5, \n                      level = 95)\nparima010\n\n     Point Forecast    Lo 95    Hi 95\n2020          64154 52573.41 75734.59\n2021          64154 47776.57 80531.43\n2022          64154 44095.83 84212.17\n2023          64154 40992.82 87315.18\n2024          64154 38259.01 90048.99\n\n\n\nautoplot(parima010, \n         xlab = \"\", \n         ylab = \"Títulos\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1993, 2024, 2)) \n\n\n\n\nFigura 10: Libros (1993-2019) y predicción (2020-2024)\n\n\n\n\nLa Figura 10 muestra la serie, la previsión y el intervalo de confianza al 95%. La predicción es constante e igual al último dato. En las series diferenciadas el intervalo de confianza de las predicciones crece muy rápidamente porque los errores se van acumulando sin ningún tipo de amortiguamiento."
  },
  {
    "objectID": "03-06-Tema6.html#aforo-de-vehículos",
    "href": "03-06-Tema6.html#aforo-de-vehículos",
    "title": "Procesos ARIMA",
    "section": "7.2 Aforo de vehículos",
    "text": "7.2 Aforo de vehículos\nVamos a aplicar de nuevo la metodología de Box-Jenkins a la serie aforo de vehículos por Oropesa, carretera N-340, km. 996,48 (fuente Ministerio de Fomento). La serie es anual de 1960 a 2019 (60 datos).\n\naforo <- read.csv2(\"./series/aforo_oropesa.csv\", header = TRUE)\naforo <- ts(aforo, start = 1960, freq = 1)\n\nautoplot(aforo, \n         xlab = \"\", \n         ylab = \"Vehículos (000)\",\n         main = \"\")\n\n\n\n\nFigura 11: Aforo de vehículos en N-340, Oropesa\n\n\n\n\nEn este ejemplo incluiremos, por primera vez, intervención y veremos como la presencia de valores atípicos puede distorsionar el proceso de identificación automática. Por ello, es conveniente realizar en paralelo ambas actividades, identificar el proceso y detectar valores atípicos.\n\n\n\nTransformación de la serie\nLa Figura 12 muestra que la serie Aforo no es estacionaria. Así, el primer paso es transformar la serie original para que lo sea. La serie no es estacionaria, pero sí lo es su primera diferencia. Ten siempre presente que diferenciar más veces de las necesarias puede dificultar la identificación y la interpretación. Por otro lado, la función ndiffs aconseja una diferenciación. Así, optamos por fijar \\(d = 1\\).\n\nautoplot(aforo, xlab = \"\", ylab = \"\", main = \"\")\nautoplot(diff(aforo), xlab = \"\", ylab = \"\", main = \"\")\nggAcf(aforo, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(aforo), xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n(b) Dif. de la serie\n\n\n\n\n\n\n\n\n\n(c) FAC serie original\n\n\n\n\n\n\n\n(d) FAC dif. de la serie\n\n\n\n\nFigura 12: Gráfica y FAC para Aforo\n\n\n\nndiffs(aforo)\n\n[1] 1\n\n\n\n\n\n\nIdentificación y Estimación\nVeamos a identificar los valores de \\(p\\) y \\(q\\) a partir de auto.arima, indicándole que d = 1. La función sugiere un proceso ARIMA(2,1,2).\n\nauto.arima(aforo, \n           d = 1)\n\nSeries: aforo \nARIMA(2,1,2) \n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      1.3202  -0.6730  -1.2817  0.9363\ns.e.  0.1311   0.1194   0.0979  0.1955\n\nsigma^2 = 642488:  log likelihood = -477.45\nAIC=964.91   AICc=966.04   BIC=975.3\n\n\nUna forma rápida, aunque imprecisa, de determinar si un coeficiente es relevante (significativo) es compararlo con su error estándar (standard error, s.e). Si el coeficiente es mayor que dos veces su error estándar, hay evidencia de que es significativo. En la salida de R, en la tabla Coefficients tienes en la primera fila el nombre de los coeficientes (ar en lugar de \\(\\phi\\) y ma en lugar de \\(\\theta\\)); su valor estimado aparece en la segunda fila de la tabla; y los errores estándar en la tercera fila (encabezada por s.e.). Todos los coeficientes estimados superan las dos desviaciones estándar y parece que son significativos.\nVamos a ver la gráfica de los residuos del modelo ARIMA(2,1,2) para identificar los valores extremos (intervención).\n\narima212 <- Arima(aforo, \n                  order = c(2, 1, 2),\n                  include.constant = FALSE)\n\nerror <- residuals(arima212)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1960, 2020, 4)) \n\n#time(error)[abs(error) > 2.5*sderror]\n\n\n\n\nFigura 13: Error + Intervención\n\n\n\n\nSe identifican dos posibles valores extremos, dos intervenciones, en los años 1979 y 2011 (el error supera las 2.5 desviaciones típicas). Cada una de las intervenciones es del tipo pulso porque solo afecta un periodo de la serie.\nAhora, creamos una variable ficticia asociada a cada intervención, que denominaremos d1979 y d2011. La forma de definir la variable ficticia asociada a un pulso consiste en crear una variable de ceros, excepto para el periodo atípico en que la variable valdrá 1.\n\nd1979 <- 1*(time(error) == 1979)\nd2011 <- 1*(time(error) == 2011)\n\nPor último, incluimos las dos variables ficticias en la autoidentificación.\n\nauto.arima(aforo,\n           d = 1,\n           xreg = cbind(d1979,  d2011))\n\nSeries: aforo \nRegression with ARIMA(2,1,0) errors \n\nCoefficients:\n         ar1     ar2       d1979       d2011\n      0.1852  0.4420  -1716.4503  -1194.6499\ns.e.  0.1181  0.1206    489.4467    455.9998\n\nsigma^2 = 593139:  log likelihood = -474.07\nAIC=958.14   AICc=959.27   BIC=968.53\n\n\nObserva como la inclusión de intervención modifica la autoidentificación, que ahora es un proceso ARIMA(2,1,0).\n\narima210 <- Arima(aforo, \n                  order = c(2, 1, 0),\n                  include.constant = FALSE,\n                  xreg = cbind(d1979, d2011))\narima210\n\nSeries: aforo \nRegression with ARIMA(2,1,0) errors \n\nCoefficients:\n         ar1     ar2       d1979       d2011\n      0.1852  0.4420  -1716.4503  -1194.6499\ns.e.  0.1181  0.1206    489.4467    455.9998\n\nsigma^2 = 593139:  log likelihood = -474.07\nAIC=958.14   AICc=959.27   BIC=968.53\n\n\nLa Figura 14 muestra que para ningún año se observa un error atípico. Es decir, no es necesaria más intervención.\n\nerror <- residuals(arima210)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1960, 2020, 4)) \n\n\n\n\nFigura 14: Error + Intervención\n\n\n\n\n\n\n\n\nValidación\nLa identificación de errores atípicos –para la posterior inclusión de sus variables de intervención asociadas– ha sido un tanto arbitraria: ¿es atípico el error que supera las 2 desviaciones típicas, las dos y media, las tres desviaciones típicas?\nA fin de poner un poco de objetividad en la decisión, podemos ver si sus coeficientes son significativos (distintos de cero) y dejar solo aquellas variables de intervención cuyo coeficientes lo sean. Aunque si la serie es suficientemente larga, también podríamos saltarnos este paso y dejar las variables de intervención que mejoren las predicciones extramuestrales del modelo o las que recojan efectos conocidos.\nLa prueba de Wald permite contrastar si un subconjunto de coeficientes es significativo (se precisa la librería aod). Esta función requiere de tres argumentos: el vector de coeficientes (b), su matriz de covarianzas (Sigma) y la posición del coeficiente cuya significatividad deseamos contrastar (Terms). Los dos primeros argumentos los podemos obtener del objeto arima210 con las funciones coef y vcov.\nVeamos qué coeficientes estimados son significativos.\n\n# ar1 = 0\nwald.test(b = coef(arima210), Sigma = vcov(arima210), Terms = 1)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 2.5, df = 1, P(> X2) = 0.12\n\n# ar2 = 0\nwald.test(b = coef(arima210), Sigma = vcov(arima210), Terms = 2)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 13.4, df = 1, P(> X2) = 0.00025\n\n# d1979 = 0\nwald.test(b = coef(arima210), Sigma = vcov(arima210), Terms = 3)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 12.3, df = 1, P(> X2) = 0.00045\n\n# d2011 = 0\nwald.test(b = coef(arima210), Sigma = vcov(arima210), Terms = 4)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 6.9, df = 1, P(> X2) = 0.0088\n\n\nLas dos variables de intervención son significativas y el coeficiente \\(\\phi_2\\) (ar2) también. No es significativo el coeficiente \\(\\phi_1\\) (ar1), pero no lo podemos eliminar. Los modelos Arima son modelos jerárquicos donde la presencia de un coeficiente significativo de cierto orden exige que los coeficientes de orden inferior estén presentes, sean o no significativos. En nuestro caso, como el coeficiente \\(\\phi_2\\) es significativo, se debe dejar en el modelo el coeficiente \\(\\phi_1\\).\nConfirmamos que \\(aforo_t \\sim ARIMA(2,1,0)\\) con intervención.\n\n\n\n\nMedidas de error\nEl error medio es 737 miles de vehículos (RMSE) y el error porcentual medio (MAPE) es 5.67%. No hay sesgo de predicción y las fórmulas empleadas para el cálculo de los intervalos de confianza de las predicciones son válidas.\n\naccuracy(arima210)\n\n\n\n               ME   RMSE    MAE  MPE MAPE MASE ACF1\nTraining set 47.2 737.37 535.16 1.32 5.67 0.79 0.04\n\n\n\n\n\n\nInterpretación del modelo\nEl modelo teórico es \\(aforo_t \\sim ARIMA(2,1,0) + d1979 + d2011\\): \\[(1 - \\phi_1 L - \\phi_2 L^2)(1 - L)aforo_t =  \\varepsilon_t + \\gamma_1 \\cdot d1979 + \\gamma_2 \\cdot d2011.\\]\nSi desarrollamos, queda: \\[aforo_t = aforo_{t-1} + \\phi_1(aforo_{t-1}-aforo_{t-2}) + \\phi_2(aforo_{t-2}-aforo_{t-3}) +\\] \\[\\gamma_1 \\cdot d1979 +  \\gamma_2 \\cdot d2011 + \\varepsilon_t.\\]\nFinalmente, el modelo estimado es: \\[\\widehat{aforo}_t = aforo_{t-1} + 0.19(aforo_{t-1}-aforo_{t-2}) + 0.44(aforo_{t-2}-aforo_{t-3})\\] \\[-1716 \\cdot d1979 -1195 \\cdot d2011\\] Cada año el aforo es el mismo que el aforo del año pasado más un 19% del último incremento observado y un 44% del incremento anterior.\nRespecto de la intervención, en 1979 hubo un 1.7 millones menos de vehículos de lo esperado, debido a la apertura de la autopista AP-7 y en 2011 hubo 1.2 millones menos de vehículos, posiblemente debido a la Gran Recesión.\n\n\nPredicción\nComo hemos incluido dos variables ficticias en el ajuste, de cara a predecir el aforo hemos de indicar cuales serán los valores futuros para estas variables. En este caso serán cero puesto que son intervenciones que no responden a un efecto calendario. Las causas detrás de estas dos intervenciones no se espera que se repitan en el futuro.\nEn R esto se hace incluyendo en el comando forecast el argumento xreg = cbind(rep(0, 5), rep(0, 5)) que añade cinco ceros por cada variable de intervención porque la predicción va a ser a cinco años vista.\n\nparima210 <- forecast(arima210, \n                      h = 5, \n                      level = 95,\n                      xreg = cbind(d1979=rep(0, 5), d2011=rep(0, 5)))\nparima210\n\n     Point Forecast    Lo 95    Hi 95\n2020       9700.908 8191.432 11210.38\n2021       9647.604 7306.822 11988.39\n2022       9643.437 6212.738 13074.14\n2023       9619.103 5212.971 14025.24\n2024       9612.754 4207.919 15017.59\n\n\n\nautoplot(parima210, \n         xlab = \"\",\n         ylab = \"Vehículos (000)\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1960, 2024, 4)) \n\n\n\n\nFigura 15: Aforo (1960-2019) y predicción (2020-2024)\n\n\n\n\n\n\n\n\nValidación con origen de predicción móvil\nVamos a calcular el error extramuestral según el horizonte temporal de previsión. En este caso, la presencia de variables de intervención hace el código algo más complejo.\nAsumiremos que se precisan 30 años para estimar el modelo, fijaremos el horizonte temporal en 5 años y calcularemos el error MAPE, que para la predicción intramuestral era de 5.7%.\n\nk <- 30                  \nh <- 5                    \nT <- length(aforo)     \ns <- T - k - h    \n\nmapeArima <- matrix(NA, s + 1, h)\n\nX <- data.frame(cbind(d1979, d2011))\n\nfor (i in 0:s) {\n  train.set <- subset(aforo, start = i + 1, end = i + k)\n  test.set <-  subset(aforo, start = i + k + 1, end = i + k + h) \n  \n  X.train <- data.frame(X[(i + 1):(i + k),])\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- data.frame(X[(i + k + 1):(i + k + h),])\n  X.test <- X.test[, hay>0]\n  \n  if (length(X.train) > 0) {\n    fit <- Arima(train.set, \n                 include.constant = FALSE,\n                 order = c(2, 1, 0),\n                 xreg=as.matrix(X.train))\n    \n    fcast <- forecast(fit, h = h, xreg = as.matrix(X.test))\n  } else {\n      fit <- Arima(train.set, \n                 include.constant = FALSE,\n                 order = c(2, 1, 0))\n    \n      fcast <- forecast(fit, h = h)\n  }\n  \n  mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n}\n\nmapeArima <- colMeans(mapeArima)\nmapeArima\n\n[1]  5.651824  9.691844 14.902944 19.538856 23.247141\n\n\nEl error de previsión extramuestral crece notablemente con el horizonte temporal. El error de las previsiones a un año vista es del 5.7%, prácticamente igual al error intramuestral, pero para dos años vista casi alcanza el 10% y a cinco años vista supera el 20%."
  },
  {
    "objectID": "03-06-Tema6.html#consumo-de-alimentos-en-el-hogar-per-cápita",
    "href": "03-06-Tema6.html#consumo-de-alimentos-en-el-hogar-per-cápita",
    "title": "Procesos ARIMA",
    "section": "7.3 Consumo de alimentos en el hogar per cápita",
    "text": "7.3 Consumo de alimentos en el hogar per cápita\nAnalizaremos el consumo alimentario en hogar per cápita en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (disponible en el Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (disponible en el Instituto Nacional de Estadística). Es una serie anual de 1987 a 2021 (35 datos) y la unidad es el Kg per cápita. La Figura 16 muestra que es una serie estacionaria.\n\nalimentospc <- read.csv2(\"./series/alimentacionpc.csv\", header = TRUE)\nalimentospc <- ts(alimentospc, start = 1987, freq = 1)\n    \nautoplot(alimentospc, \n         xlab = \"\", \n         ylab = \"Kg per cápita\",\n         main = \"\",\n         ylim = c(0, 700))\n\n\n\n\nFigura 16: Consumo alimentario en hogar\n\n\n\n\nEl pico en el año 2020 se debe al aumento del consumo de alimentos en el hogar causado por el periodo de confinamiento por la Covid-19 (marzo a junio) y el aumento del trabajo desde casa.\n\nTransformación de la serie\nLa Figura 17 indica que la serie original ya es estacionaria y la función ndiffs lo corrobora. Por tanto asumimos que \\(d=0\\) o \\(alimentospc_t \\sim I(0)\\).\n\nautoplot(alimentospc, xlab = \"\", ylab = \"\", main = \"\")\nautoplot(diff(alimentospc), xlab = \"\", ylab = \"\", main = \"\")\nggAcf(alimentospc, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(alimentospc), xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n(b) Dif. de la serie\n\n\n\n\n\n\n\n\n\n(c) FAC serie original\n\n\n\n\n\n\n\n(d) FAC dif. de la serie\n\n\n\n\nFigura 17: Gráfica y FAC para Alimentos\n\n\n\nndiffs(alimentospc)\n\n[1] 0\n\n\n\n\nIdentificación y Estimación\nPara identificar los valores de \\(p\\) y \\(q\\) veremos que nos sugiere auto.arima :\n\nauto.arima(alimentospc,\n            d = 0)\n\nSeries: alimentospc \nARIMA(1,0,0) with non-zero mean \n\nCoefficients:\n         ar1      mean\n      0.3359  638.4829\ns.e.  0.1711    3.8911\n\nsigma^2 = 254.2:  log likelihood = -145.61\nAIC=297.22   AICc=297.99   BIC=301.88\n\n\nLa identificación automática sugiere un proceso AR(1) con constante. El coeficiente de la constante (“mean” en la salida) es significativo –su valor supera dos veces su error estándar–, pero el del proceso autorregresivo no está tan claro.\nVamos a ver la gráfica de los residuos de este proceso para identificar rápidamente si hay valores extremos (Figura 18).\n\narima100 <- Arima(alimentospc, \n                  order = c(1, 0, 0),\n                  include.constant = TRUE)\n\nerror <- residuals(arima100)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1987, 2021, 3)) \n\n#time(alimentospc)[abs(error) > 2 * sderror]\n\n\n\n\nFigura 18: Error + Intervención\n\n\n\n\nHay un claro valor extremos en 2020. Tras incluir la variable ficticia asociada al año 2020 y estimar el nuevo modelo aparecen otros valores extremos en los años 1993, 1995 y 2009. Aunque ninguno supera las 2.5 desviaciones típicas, su efecto es significativo.\n\nd1993 <- 1* (time(alimentospc) == 1993)\nd1995 <- 1* (time(alimentospc) == 1995)\nd2009 <- 1* (time(alimentospc) == 2009)\nd2020 <- 1* (time(alimentospc) == 2020)\n\narima100 <- Arima(alimentospc, \n                  order = c(1, 0, 0),\n                  include.constant = TRUE,\n                  xreg = cbind(d1993, d1995, d2009, d2020))\narima100\n\nSeries: alimentospc \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    d1993     d1995    d2009    d2020\n      0.8164   638.5062  21.9317  -15.8202  21.3736  55.8455\ns.e.  0.1153     7.7231   7.1062    7.1049   7.1063   7.1295\n\nsigma^2 = 101.5:  log likelihood = -127.76\nAIC=269.53   AICc=273.67   BIC=280.41\n\n\n\n\nValidación\nTanto \\(\\phi_1\\) como el intercepto \\(\\mu\\) y las variables de intervención son significativas.\n\nancho <- max(nchar(names(coef(arima100)))) + 2\nfor(i in 1:length(coef(arima100))) {\n  wt <- wald.test(b = coef(arima100), \n                  Sigma = vcov(arima100), \n                  Terms = i)\n  cat(\"\\nCoeficiente: \", \n      format(names(coef(arima100))[i], width = ancho), \n      \"valor de p: \", \n      formatC(wt$result$chi2[3], digits = 4, format = \"f\"))\n}\n\n\nCoeficiente:  ar1         valor de p:  0.0000\nCoeficiente:  intercept   valor de p:  0.0000\nCoeficiente:  d1993       valor de p:  0.0020\nCoeficiente:  d1995       valor de p:  0.0260\nCoeficiente:  d2009       valor de p:  0.0026\nCoeficiente:  d2020       valor de p:  0.0000\n\n\nConsideraremos que \\(alimentospc_t \\sim ARIMA(1,0,0)\\) con constante e intervención.\n\n\nMedidas de error\nEl error medio es 9.2 Kg per cápita (RMSE) y el error porcentual medio (MAPE) es 1.2%. No hay sesgo y los intervalos de confianza de las predicciones son correctos.\n\naccuracy(arima100)\n\n\n\n                ME RMSE  MAE   MPE MAPE MASE ACF1\nTraining set -1.26 9.17 7.57 -0.22 1.19 0.62 0.05\n\n\n\n\nInterpretación del modelo\nEl modelo teórico identificado es \\[(1 - \\phi_1 L) alimentospc_t = c + \\gamma_1 d1993 + \\gamma_2 d1995 + \\gamma_3 d2009 + \\gamma_4 d2020 + \\varepsilon_t,\\] que desarrollando queda \\[alimentospc_t = c + \\phi_1 alimentospc_{t-1} + \\gamma_1 d1993 + \\gamma_2 d1995 + \\gamma_3 d2009 + \\gamma_4 d2020 + \\varepsilon_t.\\]\nFinalmente, el modelo estimado es \\[\\widehat{alimentospc}_t = 117.2 + 0.82 \\cdot alimentospc_{t-1} +\\] \\[21.93\\cdot d1993 - 15.82\\cdot d1995 + 21.37\\cdot d2009 + 55.85\\cdot d2020\\]\n\n\n\n\n\n\nLa contante del modelo teórico y la media del modelo estimado\n\n\n\nEl término contante \\(\\mu\\) que estima R no es el valor “c” que hemos visto en la teoría. Para convertir la contante estimada por R en “c” hemos de multiplicarla por el polinomio autorregresivo. En este caso, \\[c = \\mu \\cdot (1 - \\phi_1) = 638.5062\\cdot(1 - 0.8164) = 117.23\\]\n\n\nCada año el consumo de alimentos per cápita en el hogar es 117.2 kilos más un 82% del consumo del año pasado.\nEn 2020, debido al efecto combinado del periodo de confinamiento entre marzo y junio y el incremento del trabajo en casa, se produjo un fuerte aumento del consumo de alimentos en el hogar, estimado en 56 Kg per cápita. En 1993 y 2009 el consumo medio de alimentos per cápita fue entre 21 y 22 Kg superior a lo esperado. Posiblemente, este incremento se deba al aumento del paro y la caída de los ingresos debida a la crisis de 1993 y la Gran Recesión (2008-2014), respectivamente. Finalmente, en 1995 se observa un consumo 16 Kg menor de lo esperado.\n\n\nPredicciones de la serie\n\nparima100 <- forecast(arima100, \n                      h = 5, \n                      level = 95,\n                      xreg = cbind(rep(0, 5), rep(0, 5), rep(0, 5), rep(0, 5)))\nparima100\n\n     Point Forecast    Lo 95    Hi 95\n2022       626.9791 607.2378 646.7203\n2023       629.0957 603.6114 654.5800\n2024       630.8237 602.1434 659.5040\n2025       632.2344 601.6088 662.8600\n2026       633.3860 601.5298 665.2422\n\n\nPuedes comprobar que cada valor de la predicción se ha obtenido a partir del modelo estimado, donde \\(alimentospc_{t-1}\\) se sustituye por la predicción del año precedente.\n\nautoplot(parima100, \n         xlab = \"\",\n         ylab = \"Kilos per cápita\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1986, 2026, 4)) \n\n\n\n\nFigura 19: Consumo de alimentos y predicción"
  },
  {
    "objectID": "03-06-Tema6.html#comparación-con-alisado-exponencial",
    "href": "03-06-Tema6.html#comparación-con-alisado-exponencial",
    "title": "Procesos ARIMA",
    "section": "7.4 Comparación con alisado exponencial",
    "text": "7.4 Comparación con alisado exponencial\nVeamos una comparativa, para los tres ejemplos vistos, entre los resultados obtenidos con ARIMA y con Alisado exponencial.\n\nLibros:\n\nMAPE ARIMA: \\(6.88\\%\\) - ARIMA(0,1,0) sin deriva\nMAPE ETS: \\(6.90\\%\\) - ETS(M,N,N), \\(\\alpha=1\\)\nAmbos métodos han estimado el mismo modelo.\n\n\n\nAforo:\n\nMAPE ARIMA: \\(5.67\\%\\) - ARIMA(2,1,0) sin deriva, con intervención\nMAPE ETS: \\(6.09\\%\\) - ETS(M,A,N), \\(\\alpha=1\\), \\(\\beta=0.1\\)\nCada método estima un modelo diferente\nARIMA tiene menor error a costa de incluir dos variables de intervención\n\n\n\nAlimentos per cápita:\n\nMAPE ARIMA: \\(1.19\\%\\) - ARIMA(1,0,0) con constante e intervención\nMAPE ETS: \\(1.93\\%\\) - ETS(A,N,N), \\(\\alpha = 0.37\\)\nCada método ha estimado un modelo diferente\nARIMA tiene menor error a costa de incluir cuatro variables de intervención"
  },
  {
    "objectID": "04-02-Multiples_CS.html#descomposición",
    "href": "04-02-Multiples_CS.html#descomposición",
    "title": "Múltiples componentes estacionales",
    "section": "2.1 Descomposición",
    "text": "2.1 Descomposición\nPodemos descomponer la serie de forma análoga a como se hacia para series con una componente estacional usando la función mstl.\n\ndescomposicion <- mstl(electricidad)\nautoplot(descomposicion) \n\n\n\n\nFigura 2: Descomposición de Consumo eléctrico por hora\n\n\n\n\nEn la Figura 2 aparecen los mismos paneles que has visto en el tema 2 –datos originales, tendencia y residuo–, más los dos paneles correspondientes a las dos componentes estacionales, de orden 24 y 168.\nPara poder interpretar adecuadamente cada serie hay que fijarse en la escala de los ejes verticales. La tendencia apenas cambia en el periodo de análisis. Las dos componentes estacionales oscilan sobre un rango de valores mayor. Dentro de cada día el consumo de electricidad oscila aproximadamente 20 GW entre las horas pico y las valle: en los picos se consumen unos 7.5 GW más que la media diaria, y en los valles unos 12.5 GW menos que la media diaria. Por otro lado, el rango de variación semanal en el consumo también es de aproximadamente 20 GWh: de lunes a viernes se consumen como máximo unos 5 GW más que la media semanal y los domingo unos 15 GWh menos.\nAunque por defecto mstl permite que la componente estacional varíe en el tiempo, vamos a mostrar un detalle de la componente estacional diaria, semana y su composición para la primera semana de la serie (véase Figura 3).\n\n\n\n\n\n\n\n(a) Componente estacional diaria\n\n\n\n\n\n\n\n\n\n(b) Componente estacional semanal\n\n\n\n\n\n\n\n\n\n(c) Componente estacional diaria + semanal\n\n\n\n\nFigura 3: Componentes estacionales para Consumo eléctrico"
  },
  {
    "objectID": "04-02-Multiples_CS.html#predicción-a-partir-de-la-descomposición-y-alisado-exponencial",
    "href": "04-02-Multiples_CS.html#predicción-a-partir-de-la-descomposición-y-alisado-exponencial",
    "title": "Múltiples componentes estacionales",
    "section": "2.2 Predicción a partir de la descomposición y alisado exponencial",
    "text": "2.2 Predicción a partir de la descomposición y alisado exponencial\nExisten varios métodos para poder estimar series con estacionalidad múltiple. Uno de los más sencillos consiste en descomponer de la serie. Después, predecir las componentes estacionales por simple repetición y predecir la componente de la tendencia usando Alisado Exponencial. En último lugar, se combinan la predicción de la tendencia con las predicciones de las estacionalidades para obtener una predicción de la serie.\nLa función stlf hace todas estas operaciones de forma automática. Por defecto la tendencia se predice usando Alisado Exponencial (“ets”), pero con el argumento method se pueden especificar otros modelos alternativos, “arima”, “naive” o “rwdrift”.\nEn la figura se muestra el resultado de aplicar stlf a la serie de Demanda eléctrica. La línea negra representa la serie de Demanda y la línea azul la predicción para las dos semanas siguientes (dos primeras semanas de marzo de 2021). El título por defecto de la figura indica que la tendencia de la serie se ha ajustado usando la función ets y presenta una pendiente aditiva amortiguada y error multiplicativo.\n\npdatos_stfl <- stlf(electricidad)\n\nautoplot(pdatos_stfl, PI = FALSE) + \n  ylab(\"GW\") + \n  xlab(\"Semanas\") + \n  scale_x_continuous(breaks = seq(1,10, by = 1))\n\n\n\n\nFigura 4: Consumo eléctrico y predicción. Descomposición + Alisado"
  },
  {
    "objectID": "04-02-Multiples_CS.html#predicción-a-partir-de-asilado-exponencial-y-series-de-furier",
    "href": "04-02-Multiples_CS.html#predicción-a-partir-de-asilado-exponencial-y-series-de-furier",
    "title": "Múltiples componentes estacionales",
    "section": "2.3 Predicción a partir de Asilado Exponencial y series de Furier",
    "text": "2.3 Predicción a partir de Asilado Exponencial y series de Furier\nUno de los inconvenientes del método visto es que estima todos los elementos de cada componente estacional (\\(24 + 168\\) elementos en nuestro ejemplo) como si fueran independientes, sin tener en cuenta que, por lo general, evolucionan siguiendo una suave curva. Véase en el ejemplo de Demanda eléctrica la curva que sigue la estacionalidad diaria, donde la componente de una hora determinada está muy relacionada con la componente de la hora precedente y posterior.\nAlgunos métodos alternativos de predicción usan la dependencia observada entre los elementos de una componente estacional para ajustarlos a una curva paramétrica, por ejemplo funciones trigonométricas o series de Fourier.\nEntre los que usan funciones trigonométricas está el implementado en Livera, Hyndman, and Snyder (2011). El método de estimación que emplean los autores es complejo, requiere tiempo de computación y no siempre el ajuste obtenido es el más adecuado, así que los resultados pueden ser en ocasiones malos.\nVeamos un ejemplo de la implementación del método de Livera, Hyndman, and Snyder (2011) con la función tbats. La Figura 5 muestra la predicción para dos semanas.\n\ntmp <- Sys.time()\n\ndatos_tbats <- tbats(electricidad)\n\npdatos_tbats <- forecast(datos_tbats, \n                         h = 14 * 24,\n                         level = 95)\n\nautoplot(pdatos_tbats, PI = FALSE) + \n  ylab(\"GW\") + \n  xlab(\"Semanas\") + \n  scale_x_continuous(breaks = seq(1,6, by = 1))\n\nSys.time() - tmp\n\nTime difference of 35.70118 secs\n\n\n\n\n\nFigura 5: Consumo eléctrico y predicción. Alisado y ajuste componentes estacionales por funciones trigonométricas\n\n\n\n\n\n\n\nEntre los métodos que involucran series de Fourier una propuesta reciente es el modelo Prophet, disponible a través del paquete fable.prophet. Este modelo fue introducido por Facebook (Taylor and Letham (2018)), originalmente para pronosticar datos diarios con estacionalidad semanal y anual, además de efectos calendario. Posteriormente se amplió para cubrir más tipos de datos estacionales."
  },
  {
    "objectID": "04-04-Series_acotadas.html#predicciones-positivas",
    "href": "04-04-Series_acotadas.html#predicciones-positivas",
    "title": "Series acotadas",
    "section": "Predicciones positivas",
    "text": "Predicciones positivas\nPara imponer que las predicciones sean positivas basta trabajar con la transformación logarítmica. Por ejemplo, consideremos la serie anual de nacimientos. Vamos ha realizar predicciones a muy largo plazo (30 años) usando Alisado de Holt con y sin transformación logarítmica.\n\n\n\nEn el panel superior de la Figura 1, donde se ha usado la transformación logarítmica, no solo la predicción, sino también el intervalo es siempre positivo. Por el contrario, en el panel inferior de la Figura 1, donde no se ha usado la transformación logarítmica, las predicciones a partir de 2042 ya son negativas y el extremo inferior del intervalo de confianza es negativo desde el año 2028.\n\nConLog <- forecast(ets(nacimientos, model = \"AAN\", damped = FALSE, lambda = 0),\n                   h = 30,\n                   level = 95)\n\nSinLog <- forecast(ets(nacimientos, model = \"AAN\", damped = FALSE),\n                   h = 30,\n                   level = 95)\n\nautoplot(ConLog, main = \"\", xlab = \"\", ylab = \"Bebés (miles)\")\n\nautoplot(SinLog, main = \"\", xlab = \"\", ylab = \"Bebés (miles)\") + \n  geom_hline(yintercept=0, size = .3, linetype = 2)\n\n\n\n\n\n\n\n(a) Con transformación logarítmica\n\n\n\n\n\n\n\n\n\n(b) Sin transformación logarítmica\n\n\n\n\nFigura 1: Ajuste y predicción de Nacimientos con Holt"
  },
  {
    "objectID": "04-04-Series_acotadas.html#predicciones-dentro-de-un-intervalo",
    "href": "04-04-Series_acotadas.html#predicciones-dentro-de-un-intervalo",
    "title": "Series acotadas",
    "section": "Predicciones dentro de un intervalo",
    "text": "Predicciones dentro de un intervalo\nSupongamos que el valor de la serie es un porcentaje y que debe estar comprendido entre \\(a = 0\\) y \\(b = 100\\), como por ejemplo la serie anual consistente en el porcentaje de bebés nacidos de mujeres con nacionalidad española. La transformación que garantiza que las predicciones se mantendrán dentro del intervalo \\([a,\\;b]\\) es\n\\[z_t = \\log\\Big(\\frac{y_t - a}{b - y_t}\\Big),\\] donde \\(y_t\\) es la serie original y \\(z_t\\) la serie transformada. Una vez tenemos las predicciones de la serie \\(z_t\\), tenemos que deshacer la transformación con\n\\[y_t = \\frac{a +b\\, e^{z_t}}{1 + e^{z_t}}.\\]\nEn este caso no hay un argumento lambda que nos facilite el trabajo y hay que escribir más código.\n\n\n\n\na <- 0\nb <- 100\n\nz <- log((serie - a) / (b - serie))\n\nmodelo <- ets(z, \n              model = \"AAN\", \n              damped = FALSE)\n\npz <- forecast(modelo, \n               h = 30,\n               level = 95)\n\npz[[\"mean\"]] <-  (a + b * exp(pz[[\"mean\"]]) ) / (1 + exp(pz[[\"mean\"]]))\npz[[\"lower\"]] <- (a + b * exp(pz[[\"lower\"]])) / (1 + exp(pz[[\"lower\"]]))\npz[[\"upper\"]] <- (a + b * exp(pz[[\"upper\"]])) / (1 + exp(pz[[\"upper\"]]))\npz[[\"x\"]] <- serie\n\nautoplot(pz, \n         main = \"\",\n         xlab = \"\")\n\n\n\n\nFigura 2: Predicción con Holt. Valores acotados entre 0% y 100%\n\n\n\n\nHemos solicitado una previsión a 30 años vista para poder ver mejor el efecto de acotar la serie. En la Figura 2 se observa que en nuestro ejemplo no solo la predicción, sino también el intervalo está siempre entre 0% y 100%."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#análisis-de-la-tendencia",
    "href": "03-14-Ejemplo-Pasajeros.html#análisis-de-la-tendencia",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.1 Análisis de la tendencia",
    "text": "2.1 Análisis de la tendencia\nSi anualizamos la serie podemos, por un lado, identificar mejor en que años se producen los cambios en la tendencia y, por otro lado, poner cifras al volumen de pasajeros en transporte urbano.\n\nPasajerosAnual <- aggregate(Pasajeros, FUN = sum)\n\nautoplot(PasajerosAnual, colour = \"darkblue\",\n         xlab = \"\",\n         ylab = \"Millones de pasajeros\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 2: Pasajeros en transporte urbano (datos anuales)\n\n\n\n\nLa Figura 2 muestra el volumen anual de pasajeros en transporte urbano. El crecimiento continuado, posiblemente iniciado antes de 1996 y que permitió superar los 3000 millones de pasajeros en 2007, se ve interrumpido con el inicio de la pasada crisis económica. La caída en el número de pasajeros se interrumpe en 2014, año que marca la salida de la Gran recesión y el inicio de la recuperación en el serie. En 2019 se superaron los 3100 millones de pasajeros.\nEl incremento en el uso del transporte urbano observado antes y después de la crisis puede tener distintas causas: un uso más intensivo del transporte urbano en detrimento de otros medios de transporte, una reorganización de los servicios de transporte urbano que haya mejorado la conectividad dentro de los municipios, o un aumento en el número de líneas de autobús, tranvía o metro en determinadas ciudades.\nLa causa del repunte aislado observado el año 2011, en plena crisis, fue una ligera recuperación de la economía que tuvo lugar a finales de 2010 y principios de 2011."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#análisis-de-la-estacionalidad",
    "href": "03-14-Ejemplo-Pasajeros.html#análisis-de-la-estacionalidad",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.2 Análisis de la estacionalidad",
    "text": "2.2 Análisis de la estacionalidad\nLa principal causa de la estacionalidad observada en la serie es la estructura vacacional de la sociedad, especialmente caracterizada por las vacaciones de verano (julio a septiembre) y las vacaciones de Semana Santa (en marzo y/o abril, según el año). Además, debido a que el transporte urbano se usa principalmente para ir a trabajar, también influye el número de días laborables del mes. Por ejemplo, en 2017 el mes de junio tuvo 22 días laborables, mientras que en 2019 tuvo 20 días laborables. Esta diferencia de dos días tendrá un efecto sobre el volumen de pasajeros.\nEl número de días laborables de un mes viene marcado por los fines de semana del mes y por las festividades nacionales. Es cierto que el sábado se trabaja en diversos sectores (comercio, ocio, distribución) pero la caída en el número de trabajadores respecto de los días entre semana (lunes a viernes) es muy notable. También es cierto que, además de las festividades nacionales, hay muchas festividades autonómicas o municipales que podrían afectar al volumen de pasajeros en transporte urbano. Por ejemplo, las festividades regionales en comunidades como Madrid o Cataluña pueden tener un efecto significativo sobre la serie Pasajeros. Sin embargo, las festividades no nacionales no se van a tener en cuenta.\nPor tanto, para realizar un análisis detallado de la estacionalidad, es necesario crear una serie con el número de días laborables de cada mes. Además, esta serie se usará más adelante para modelizar y predecir la serie Pasajeros.\n\nDías laborables de cada mes\nLa librería timeData proporciona una serie de funciones que permiten definir un calendario de festividades, identificar los fines de semana y, a partir de aquí, crear la serie de días laborables (véase código más abajo).\n\nCon timeCalendar se definen las festividades nacionales que vamos a considerar: Año nuevo (1 de enero), Reyes (6 de enero), Viernes Santo (fecha variable), Día del Trabajo (1 de mayo), Día de la Asunción (15 de agosto), Día de la Hispanidad (12 de octubre), Día de Todos los Santos (1 de noviembre), la Constitución (6 de diciembre) la Inmaculada Concepción (8 de diciembre) y Navidad (25 de diciembre).\nPor claridad, cada festivo se ha definido de forma independiente para después crear una variable con todas las festividades (FestivosNacionales).\nEl rango para todos los cálculos va desde 1996 hasta 2024, que incluye el rango de la serie Pasajeros más cinco años de predicción.\nLa función utilizada Easter de la librería timeDate difiere de la función easter de forecast.\nA continuación, con timeSequence se crea una serie diaria desde el 1 de enero de 1996 hasta el 31 de diciembre de 2024.\nLas dos siguientes líneas eliminan de la serie diaria los festivos y los fines de semana, (función isBizday), para después dar a esta nueva serie el formato año-mes eliminando el día. De esta forma, la serie de días laborales tendrá el mismo identificador para todos los días del mismo mes.\nDespués, se crea una tabla que, por la naturaleza de la serie de días laborales, tendrá para cada año-mes el numero de días laborables. Por último fechamos la tabla, que es nuestra serie de días laborables y mostramos algunos datos.\nLas dos últimas líneas de código dividen la serie en el periodo muestral y el de predicción.\n\n\n\n\nAnoNuevo <- timeCalendar(d = 1, m = 1, y = 1996:2024)\nReyes <- timeCalendar(d = 6, m = 1, y = 1996:2024)\nViernesSanto <- Easter(1996:2024, shift = -2)\nDiaTrabajo <- timeCalendar(d = 1, m = 5, y = 1996:2024)\nAsuncion <- timeCalendar(d = 15, m = 8, y = 1996:2024)\nHispanidad <- timeCalendar(d = 12, m = 10, y = 1996:2024)\nTodoSantos <- timeCalendar(d = 1, m = 11, y = 1996:2024)\nConstitucion <- timeCalendar(d = 6, m = 12, y = 1996:2024)\nInmaculada <- timeCalendar(d = 8, m = 12, y = 1996:2024)\nNavidad <- timeCalendar(d = 25, m = 12, y = 1996:2024)\n\nFestivosNacionales <- c(AnoNuevo, Reyes, ViernesSanto,\n                        DiaTrabajo, Asuncion,  Hispanidad, TodoSantos, \n                        Constitucion, Inmaculada, Navidad)\n\nfechaDiaria <- timeSequence(from = \"1996-01-01\", to = \"2024-12-31\")\nbiz <- fechaDiaria[isBizday(fechaDiaria, holidays = FestivosNacionales)]\nbizdays <- format(biz, format = \"%Y-%m\")\n\nDiasLaborables <- table(bizdays)\nDiasLaborables <- ts(DiasLaborables, start = 1996, frequency = 12)\n\nsubset(DiasLaborables, start = 289) #Mostramos solo los 5 últimos años\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2020  21  20  22  21  20  22  23  21  22  21  21  21\n2021  19  20  23  21  21  22  22  22  22  20  21  21\n2022  20  20  23  20  22  22  21  22  22  20  21  20\n2023  21  20  23  19  22  22  21  22  21  21  21  18\n2024  22  21  20  22  22  20  23  21  21  23  20  20\n\npDiasLaborables <- subset(DiasLaborables, start = length(DiasLaborables) - 59)\nDiasLaborables <- subset(DiasLaborables, end = length(DiasLaborables) - 60)\n\nEs conveniente indicar que la identificación de las festividades nacionales dista de ser perfecta por varios motivos:\n\nalgunos festivos nacionales si caen en domingo, se pasan a lunes (por ejemplo Reyes y la Inmaculada de 2019), aspecto que no se ha tenido en cuenta.\nalgunos festivos nacionales pueden ser sustituidos por otros días por las Comunidades Autónomas, por ejemplo Reyes o Jueves Santo.\n\n\n\n\n\nAnálisis gráfico de la estacionalidad\nVeamos ahora una descriptiva detallada de la estacionalidad de la serie Pasajeros, haciendo especial hincapié en el efecto de las vacaciones (verano y Semana Santa) y el número de días laborables. Para ello, mostraremos gráficamente las subseries definidas por el mes tanto para Pasajeros como para Pasajeros por día laborable, esta segunda resultado de dividir Pasajeros por DiasLaborables.\n\nPasajerosDL <- Pasajeros/DiasLaborables\n\nggsubseriesplot(Pasajeros) +\n  ylab(\"Millones de pasajeros\") +\n  xlab(\"\") +\n  ggtitle(\"\")\n\nggsubseriesplot(PasajerosDL) +\n  ylab(\"Millones de pasajeros\") +\n  xlab(\"\") +\n  ggtitle(\"\")\n\n\n\n\n\n\n\n(a) Pasajeros\n\n\n\n\n\n\n\n\n\n(b) Pasajeros por día laborable\n\n\n\n\nFigura 3: Estacionalidad de la serie Pasajeros\n\n\n\nLa Figura 3 muestra para cada mes la serie de pasajeros (total o por día laborable) y el valor medio (línea azul horizontal). En ambos paneles se identifica perfectamente el efecto de los periodos vacacionales sobre el transporte urbano de pasajeros. En las vacaciones de verano se observa una fuerte caída en el número de pasajeros, especialmente en agosto y, en menor medida, en julio y septiembre. Por otro lado, las subseries de marzo y abril muestran mucha más irregularidad que las de otros meses debido a que el volumen de pasajeros depende de cómo ha caído la Semana Santa. Si un año esta cae en marzo, ese mes presentará un volumen de pasajeros inferior al de los meses de marzo sin Semana Santa, mientras que en abril se dará el efecto contrario. Diciembre, para ser un mes de 31 días, presenta también un reducido número de pasajeros debido a las vacaciones navideñas.\nLa Figura 3 a) muestra el efecto estacional total debido al número de días del mes y de días laborales. Por ejemplo, en febrero, el mes con menos días y por tanto con menos días laborales, en media se transportan menos pasajeros, comparado con enero o marzo. Octubre, un mes con 31 días, muestra un volumen medio de pasajeros mayor que noviembre de 30 días.\nEn la Figura 3 b) se ha eliminado el efecto de los días laborables al trabajar con la serie de pasajeros transportados por día laborable. Si la comparamos con la Figura 3 a), destaca que las diferencias entre las medias (lineas azules) se han reducido: prácticamente no hay diferencias entre los meses de enero a junio, o entre los meses de octubre a diciembre.\nCabría pensar que al excluir de la serie de días laborables la Semana Santa, en la Figura 3 b) las subseries de marzo y abril deberían ser tan suaves como las observadas para otro meses, pero no es así. Claramente la simple exclusión de los festivos nacionales de Semana Santa no es suficiente para recoger bien su efecto sobre el transporte urbano. La razón hay que buscarla en las vacaciones escolares de este periodo, que en algunas comunidades autónomas tiene lugar durante la propia semana de Semana Santa, mientras que en otras comunidades tiene lugar en la semana posterior. De esta forma, el efecto sobre el transporte urbano de Semana Santa no es homogéneo en el territorio nacional y resulta difícil incluirlo en el análisis de la serie Pasajeros.\n\n\n\n\nAnálisis numérico de la estacionalidad\nPodemos obtener la componente estacional de forma sencilla para poder valorarla numéricamente y ver que efecto tiene el número de días laborables. Previamente, debemos determinar el esquema, aditivo o multiplicativo, de la serie.\nLa gráfica media-desviación típica (Figura 4) refuerza la impresión que se obtenía de la gráfica de la serie (Figura 1), que el esquema es aditivo.\n\nMediaAnual <- aggregate(Pasajeros, FUN = mean)\nDesviacionAnual <- aggregate(Pasajeros, FUN = sd)\n\nggplot() +\n  geom_point(aes(x = MediaAnual, y = DesviacionAnual), size = 2) +\n  xlab(\"Media de pasajeros por año\") + \n  ylab(\"Desviación típica de pasajeros por año\") + \n  ggtitle(\"\")\n\n\n\n\nFigura 4: Identificación del tipo de esquema\n\n\n\n\n\nPasajerosMedia <- tapply(Pasajeros - mean(Pasajeros), \n                         cycle(Pasajeros), \n                         mean)\n\nPasajerosDLMedia <- tapply((PasajerosDL - mean(PasajerosDL)), \n                           cycle(PasajerosDL), \n                           mean)\n\ndatos <- cbind(PasajerosMedia, PasajerosDLMedia)\ncolnames(datos) <- c(\"Pasajeros\", \"Pasajeros por día laborable\")\nrownames(datos) <- meses\n\nkable(datos, \n      digits = 2)\n\n\n\nTabla 1: Efecto estacional\n\n\n\nPasajeros\nPasajeros por día laborable\n\n\n\n\nEne\n3.19\n0.32\n\n\nFeb\n-0.36\n0.46\n\n\nMar\n17.71\n0.47\n\n\nAbr\n3.36\n0.35\n\n\nMay\n19.11\n0.70\n\n\nJun\n6.30\n0.17\n\n\nJul\n-15.09\n-1.22\n\n\nAgo\n-71.64\n-3.51\n\n\nSep\n-7.75\n-0.50\n\n\nOct\n27.60\n1.05\n\n\nNov\n15.36\n0.97\n\n\nDic\n2.20\n0.74\n\n\n\n\n\n\nLa Tabla 1 pone en cifras el efecto estacional sobre los Pasajeros (primera columna): en agosto la caída en el número de pasajeros, respecto de la media anual, se cifra en 72 millones de pasajeros. En julio, septiembre y en menor medida febrero también el uso del transporte urbano es inferior a la media anual, en el caso de los dos primeros meses por las vacaciones de verano y en febrero debido a ser el mes con menos días del año. Por otro lado, destaca el elevado número de pasajeros en los meses de marzo, mayo y octubre, por tener 31 días, y noviembre, por razones desconocidas.\nTras la corrección por el número de días laborales, el efecto estacional es más suave (véase la segunda columna en la Tabla 1). Ahora, los meses de febrero y marzo tienen un efecto similar, al igual que octubre y noviembre. También se observa que las diferencias entre marzo y abril se han reducido."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#descomposición-de-la-serie",
    "href": "03-14-Ejemplo-Pasajeros.html#descomposición-de-la-serie",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.3 Descomposición de la serie",
    "text": "2.3 Descomposición de la serie\nYa hemos realizado una descripción detallada de las principales componentes de la serie, tendencia y estacionalidad. Ahora vamos a proceder a descomponerla a fin de analizar, aunque sea de forma gráfica, el error y tener ya una primera impresión sobre la relevancia de la componente de intervención en Pasajeros.\nDado que la serie presenta un esquema aditivo, usaremos el método de descomposición por regresiones locales ponderadas, asumiendo una componente estacional constante y considerando la presencia de posibles valores extremos.\n\nPasajerosStl <- stl(Pasajeros[,1], \n                    s.window = \"periodic\", \n                    robust = TRUE)\n\nerror <- remainder(PasajerosStl)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(3, 2, -2, -3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"),\n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 5: Error + Intervención. Descomposición de Pasajeros\n\n\n\n\nLa Figura 5 muestra el error de la descomposición y los intervalos de confianza al 95% (líneas verdes) y el 99.7% (líneas rojas). Se aprecian claramente múltiples valores extremos (superan las tres desviaciones típicas) en forma de compensación (dos errores extremos consecutivos de signo opuesto) que corresponden a los meses de marzo y abril de 1997, 2002, 2008 y 2013, y otro valor extremo en abril de 2005. Además, en marzo y abril de 2016 hay dos valores atípicos. Nótese que todos los valores identificados corresponden a los meses de marzo y abril, y en todos los casos el error negativo tiene lugar en marzo y el positivo en abril. Si miramos un calendario veremos que tienen lugar en los años en que Semana Santa cayó en marzo.\nSi repetimos este análisis para la serie de Pasajeros por día laborable, los resultados son bien diferentes (véase Figura 6). Ahora solo se detectan dos valores extremos en diciembre de 2000 y 2006. También destaca el error de diciembre de 2017. Los errores en diciembre se dan cuando Navidad cae en lunes, de forma que la caída en el transporte urbano debida a la nochebuena coincide con la de cualquier domingo. Así, estos meses de diciembre presentan más transporte urbano que los meses de diciembre donde la nochebuena cae entre semana.\n\nPasajerosStl <- stl(PasajerosDL[,1], \n                    s.window = \"periodic\", \n                    robust = TRUE)\n\nerror <- remainder(PasajerosStl)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(3, 2, -2, -3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"),\n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 6: Error + Intervención. Descomposición de Pasajeros por día laborable\n\n\n\n\nPara la identificación de los valores extremos se ha hecho uso de las funciones easter del paquete forecast y dayOfWeek del paquete timeDate."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#conclusión",
    "href": "03-14-Ejemplo-Pasajeros.html#conclusión",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.4 Conclusión",
    "text": "2.4 Conclusión\nLa serie de pasajeros en transporte urbano muestra una tendencia creciente solo interrumpida entre 2008 y 2013 debido a la Gran Recesión.\nLos principales determinantes de la estacionalidad de la serie Pasajeros son los grandes periodos vacacionales en España (Semana Santa y verano) y el número de días laborales del mes. Así, en la serie Pasajeros corregida por días laborables la componente estacional se ha suavizado y prácticamente queda determinada por las vacaciones.\nLa intervención tiene lugar en los meses de marzo y abril debido al carácter móvil de la Semana Santa, y en diciembre cuando el día de Navidad cae en lunes de forma que la caída de pasajeros de nochebuena se solapa con la de cualquier domingo."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#análisis-de-la-serie-pasajeros",
    "href": "03-14-Ejemplo-Pasajeros.html#análisis-de-la-serie-pasajeros",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.1 Análisis de la serie Pasajeros",
    "text": "4.1 Análisis de la serie Pasajeros\n\n\n\nEstimación e interpretación\nEl modelo óptimo, estimado con la función ets sin imponer ninguna restricción, es ETS(M,Ad,A): pendiente aditiva con amortiguamiento, estacionalidad aditiva y residuo multiplicativo. \\[y_{t+1} = (l_t + \\phi b_t + s_{t+1-m}) \\cdot (1 + \\varepsilon_{t+1}).\\]\n\nPasajerosEts <- ets(Pasajeros)\nsummary(PasajerosEts) \n\nETS(M,Ad,A) \n\nCall:\n ets(y = Pasajeros) \n\n  Smoothing parameters:\n    alpha = 0.1374 \n    beta  = 0.017 \n    gamma = 1e-04 \n    phi   = 0.9438 \n\n  Initial states:\n    l = 203.1964 \n    b = -0.4508 \n    s = 2.0753 14.0423 27.0229 -8.7953 -72.369 -15.0625\n           6.6935 19.5129 4.2863 18.0168 0.373 4.2039\n\n  sigma:  0.0333\n\n     AIC     AICc      BIC \n2823.866 2826.408 2889.799 \n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.5929553 7.492818 5.654414 0.1765693 2.447442 0.6844865\n                   ACF1\nTraining set -0.1434247\n\n\nEl valor de \\(\\phi=\\) 0.94 indica que la inclusión de amortiguamiento en el modelo mejora sensiblemente su ajuste a los datos. Por otro lado, \\(\\gamma\\) es técnicamente cero, indicando que el efecto estacional se mantiene constante en el tiempo. Sin embargo, el valor de \\(\\beta\\), reducido pero no nulo, indica que la pendiente cambia en el tiempo de forma muy lenta.\nLa calidad del ajuste es bastante buena, con un error porcentual del 2.4% o un error de 7.5 millones de pasajeros (RMSE). La aplicación del método de alisado supone una reducción de un punto en el error porcentual respecto del método ingenuo, o una reducción de 3.5 millones de pasajeros. Es decir, el modelo de alisado exponencial supone una mejora en la calidad del ajuste del 32% respecto del método ingenuo con estacionalidad visto en el epígrafe previo (MASE). El indicador ACF1 revela que las fórmulas usadas para el intervalo de confianza no son válidas.\nEl efecto estacional, que recordemos se mantiene constante en el tiempo, es prácticamente idéntico al estimado en la descriptiva y viene determinado por los periodos vacacionales y el número de días del mes (y por consiguiente el número de días laborables). Véase la Figura 8.\nEn verano (julio a septiembre) el uso del transporte urbano es inferior a la media anual, destacando agosto con un descenso de 72 millones de pasajeros. Por el contrario, octubre destaca por ser el mes con mayor incremento en el volumen de pasajeros (27 millones) respecto de la media anual.\n\nPasajerosEtsEst <- PasajerosEts$states[nrow(PasajerosEts$states), 14:3]\nnames(PasajerosEtsEst) <- meses\n\nround(PasajerosEtsEst, 2)\n\n   Ene    Feb    Mar    Abr    May    Jun    Jul    Ago    Sep    Oct    Nov \n  4.21   0.37  18.02   4.29  19.51   6.69 -15.06 -72.37  -8.79  27.02  14.04 \n   Dic \n  2.07 \n\nggplot() +\n  geom_line(aes(x = 1:12, y = PasajerosEtsEst), colour = \"darkblue\") + \n  geom_hline(yintercept = 0, colour = \"black\", lty = 2) +\n  ggtitle(\"\") +\n  xlab(\"\") +\n  ylab(\"Efecto estacional\") +\n  scale_x_continuous(breaks= 1:12, \n                     labels = meses)\n\n\n\n\nFigura 8: Componente estacional estimada con Alisado exponencial\n\n\n\n\n\n\n\n\nPredicción\nPodemos ahora pedir los valores de predicción para los próximos cinco años. No mostramos los resultados numéricos, pero si gráficos (Figura 9). Las predicciones muestran una tendencia creciente amortiguada y, por tanto, no tan acusada como la observada en los años precedentes.\n\nPasajerosEtsPre <- forecast(PasajerosEts, \n                            h = 60)\n\nautoplot(PasajerosEtsPre,\n         xlab = \"\",\n         ylab = \"Millones de pasajeros\",\n         main = \"\",\n         PI = FALSE) \n\n\n\n\nFigura 9: Pasajeros (1996-2019) y predicción (2020-2024). Método de Alisado Exponencial\n\n\n\n\nEn el año 2020 se esperan 3153 millones de pasajeros, un 1.6% más que en 2019.2\n\n\n\n\nAnálisis del error\nEl residuo del modelo (Figura 10) muestra varios valores que pueden ser considerados como atípicos y que se dan siempre en los meses de marzo y abril para los años donde la Semana Santa recayó en marzo.\n\nerror <- residuals(PasajerosEts)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 10: Error + Intervención. Método de Alisado Exponencial"
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#otras-alternativas-de-análisis",
    "href": "03-14-Ejemplo-Pasajeros.html#otras-alternativas-de-análisis",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.2 Otras alternativas de análisis",
    "text": "4.2 Otras alternativas de análisis\nEn la descriptiva se ha visto que la serie de pasajeros por día laborable tiene un comportamiento más suave (la componente estacional era más plana) y presentaba un menor número de valores atípicos que la serie original Pasajeros. Cabe esperar, por tanto, que esta serie presente un mejor ajuste con los métodos de Alisado Exponencial y ofrezca mejores predicciones.\nPor otro lado, siempre vale la pena analizar la transformación logarítmica de la serie y ver si ofrece mejores resultados que la serie original. La transformación logarítmica es especialmente eficaz para series no lineales, así que para Pasajeros posiblemente no suponga ningún mejora.\nLas transformaciones indicadas en los dos párrafos precedentes son solo dos de las posibles. También se pude analizar la serie de pasajeros por día del mes o la transformación óptima de Box-Cox. La idea es no quedarse con lo inmediato –la serie tal cual nos la han ofrecido–, sino probar otras alternativas. Por ejemplo, la serie de Pasajeros es el agregado del número de pasajeros que viajan en transporte urbano según el tipo de transporte (autobús, metro, tranvía…). Se podría proceder a analizar cada serie por separado (pasajeros en autobús, pasajeros en metro, etc.), para luego agregar los resultados y ver si este enfoque da mejores resultados que el análisis directo de la serie agregada Pasajeros.\nEn este epígrafe se analizarán tres de las transformaciones indicadas: la transformación logarítmica, los pasajeros por día laborable y los pasajeros por día del mes. El objetivo es ver si es posible mejorar la calidad de las predicciones obtenidas para Pasajeros. Se usará como criterio de bondad el error de las predicciones extramuestrales según el horizonte temporal, obtenido con el procedimiento origen de predicción móvil. Asumiremos que son necesarios 12 años para obtener una buena estimación del modelo y el horizonte temporal se fijará en 12 meses (\\(k = 144, h = 12\\)). Previamente, hay que crear la serie Pasajeros por día del mes, e identificar el mejor modelo para las series transformadas.\n\nPasajerosDM <- Pasajeros/monthdays(Pasajeros)\nets(Pasajeros, lambda = 0)$method\n\n[1] \"ETS(A,Ad,A)\"\n\nets(PasajerosDL)$method\n\n[1] \"ETS(M,Ad,A)\"\n\nets(PasajerosDM)$method\n\n[1] \"ETS(A,Ad,A)\"\n\n\n\n\n\nk <- 144                 \nh <- 12                  \nTT <- length(Pasajeros)  \ns <- TT - k - h          \n\nmapeAlisadoPas <- matrix(NA, s + 1, h)\nmapeAlisadolPas <- matrix(NA, s + 1, h)\nmapeAlisadoPasDL <- matrix(NA, s + 1, h)\nmapeAlisadoPasDM <- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set <- subset(Pasajeros, start = i + 1, end = i + k)\n  test.set <-  subset(Pasajeros, start = i + k + 1, end = i + k + h)\n  \n  trainDL.set <- subset(PasajerosDL, start = i + 1, end = i + k)\n  testDL.set <-  subset(PasajerosDL, start = i + k + 1, end = i + k + h)\n  \n  trainDM.set <- subset(PasajerosDM, start = i + 1, end = i + k)\n  testDM.set <-  subset(PasajerosDM, start = i + k + 1, end = i + k + h)\n  \n  fit <- ets(train.set, model = \"MAA\", damped = TRUE)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadoPas[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n  fit <- ets(train.set, model = \"AAA\", damped = TRUE, lambda = 0)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadolPas[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n  fit <- ets(trainDL.set, model = \"MAA\", damped = TRUE)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadoPasDL[i + 1,] <- 100*abs(testDL.set - fcast$mean)/testDL.set\n  \n  fit <- ets(trainDM.set, model = \"AAA\", damped = TRUE)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadoPasDM[i + 1,] <- 100*abs(testDM.set - fcast$mean)/testDM.set\n}\n\nerrorAlisadoPas <- colMeans(mapeAlisadoPas)\nerrorAlisadolPas <- colMeans(mapeAlisadolPas)\nerrorAlisadoPasDL <- colMeans(mapeAlisadoPasDL)\nerrorAlisadoPasDM <- colMeans(mapeAlisadoPasDM)\n\ndatos <- data.frame(\n  factor = c(rep(\"Pasajeros\", 12), \n             rep(\"Pasajeros por día laborable\", 12), \n             rep(\"Pasajeros por día del mes\", 12), \n             rep(\"Pasajeros (log)\", 12)),\n  x = c(1:12, 1:12, 1:12, 1:12),\n  y = c(errorAlisadoPas, errorAlisadoPasDL, errorAlisadoPasDM, errorAlisadolPas)\n)\n\n\nggplot(datos, aes(x = x, y = y,  colour= factor)) + \n  geom_line() +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"%\") +\n  scale_x_continuous(breaks= 1:12) +\n  scale_y_continuous(breaks= seq(2.6, 4, .2)) +\n  labs(colour = \"Métodos\") + \n  theme(legend.position=c(0.15,0.7))\n\n\n\n\nFigura 11: Error de predicción (MAPE) según horizonte temporal y enfoque. Método de Alisado Exponencial\n\n\n\n\nAntes de pasar al análisis de los resultados, indicar que en las predicciones sobre el logaritmo no se ha pedido corrección por sesgo, y que al trabajar con errores porcentuales no es necesario pasar la predicción de pasajeros por día (laborable o del mes) a predicción de pasajeros.\nLa Figura 11 muestra los errores de predicción según el horizonte temporal para las tres aproximaciones. En todos los casos el error aumenta con el horizonte temporal de predicción, de forma que las predicciones a doce meses vista tienen un error un punto porcentual superior a las predicciones a un mes vista.\nPor otro lado, para horizontes temporales de hasta 8 meses vista las predicciones realizadas sobre el logaritmo de la serie original Pasajeros son las más ajustadas. Para predicciones de 9 a 12 meses, las mejores predicciones se obtienen con Pasajeros por día del mes, seguidas de las predicciones realizadas sobre la serie sin transformar. En contra de lo esperado, la predicción a partir de la serie de pasajeros por día laborable es la que mayor error porcentual presenta."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#conclusión-1",
    "href": "03-14-Ejemplo-Pasajeros.html#conclusión-1",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.3 Conclusión",
    "text": "4.3 Conclusión\nLos modelos de Alisado Exponencial resultan excelentes para predecir la serie Pasajeros. El error de ajuste, del 2.4%, es un punto inferior al error obtenido con el método ingenuo con estacionalidad. Además, en las predicciones extramuestrales a 12 meses vista el error porcentual sigue manteniéndose bajo, no superando el 4%.\nLa transformación logarítmica de la serie pasajeros y la serie Pasajeros por día del mes han dado mejores predicciones por Alisado Exponencial que el análisis directo de la serie."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#transformación-de-la-serie-pasajeros",
    "href": "03-14-Ejemplo-Pasajeros.html#transformación-de-la-serie-pasajeros",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.1 Transformación de la serie Pasajeros",
    "text": "5.1 Transformación de la serie Pasajeros\nPor un lado, el análisis por Alisado Exponencial ha puesto de relieve el carácter lineal de Pasajeros y la efectividad que podría tener usar la transformación logarítmica para mejorar la calidad de las predicciones.\nPor otro lado, tras un análisis preliminar por modelos ARIMA se puede ver que la transformación logarítmica mejora la identificación del modelo y facilitaba su interpretación.\nAsí, optamos por aplicar la transformación logarítmica a Pasajeros.\nLas FAC del logaritmo de la serie y algunas de sus diferenciaciones (Figura 12) indican que es necesaria la doble diferenciación regular y estacional para alcanzar las hipótesis de estacionariedad y ergodicidad: \\(\\log(Pasajeros) \\sim I(1)I_{12}(1)\\). Los resultados ofrecidos por las funciones ndiffs y nsdiffs apoyan esta conclusión.\n\nggAcf(log(Pasajeros), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pasajeros)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pasajeros), lag = 12), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(diff(log(Pasajeros), lag=12)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n(a) Log serie\n\n\n\n\n\n\n\n(b) Dif. regular log serie\n\n\n\n\n\n\n\n\n\n(c) Dif. estacional log serie\n\n\n\n\n\n\n\n(d) Dif. regular y estacional log serie\n\n\n\n\nFigura 12: FAC para Pasajeros\n\n\n\n\nndiffs(log(Pasajeros))\n\n[1] 1\n\nnsdiffs(log(Pasajeros))\n\n[1] 1\n\n\nLa Figura 13 muestra la serie original \\(y_t\\) y la serie transformada \\(\\nabla \\nabla_{12} \\log(y_t)\\). En la serie transformada destacan las compensaciones asociadas a la intervención de Semana Santa.\n\nseries <- cbind(\"Original\" = Pasajeros,\n                \"Dif reg. y est. de log\" = diff(diff(log(Pasajeros), lag = 12)))\n\nautoplot(series, facets = TRUE,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\")\n\n\n\n\nFigura 13: Serie original de Pasajeros y su transformación"
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#identificación-de-la-serie-pasajeros",
    "href": "03-14-Ejemplo-Pasajeros.html#identificación-de-la-serie-pasajeros",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.2 Identificación de la serie Pasajeros",
    "text": "5.2 Identificación de la serie Pasajeros\nVamos a identificar los valores de \\(p\\), \\(q\\), \\(P\\) y \\(Q\\) del proceso ARIMA. Para ello, solicitaremos con auto.arima y seas una identificación automática, en el primer caso incluyendo todos los efectos calendario ya identificados.\n\n\n\nIdentificación automática con auto.arima\nPara ayudar a la función auto.arima en el proceso de identificación vamos a definir previamente todas las variables de intervención que en el desarrollo del análisis de la serie hemos ido identificando: días del mes, días laborables del mes, meses de diciembre con el día de Navidad en lunes y Semana Santa:\n\nLa variables días laborables del mes ya ha sido definida previamente como DiasLaborables.\nLa variable Días del mes se puede definir directamente con la función monthdays de la librería forecast. En lugar de días del mes, consideraremos la variable días no laborables del mes, resultante de restar a los días del mes los días laborables.\nLos meses de diciembre en que el día de Navidad cae en lunes requiere un poco más de trabajo. La idea general es generar un rango de fechas diarias que cubra todo el periodo de análisis (variable fechas), identificar los lunes de Navidad (variable dicotómica lunesNavidad), eliminar el identificador del día del rango de fechas con la función format y, por último, con tapply sumar para cada mes-año los lunes de Navidad, que lógicamente solo tendrán lugar algunos meses de diciembre y una sola vez. En los objetos definidos con la función as.POSIXlt los meses van de 0 a 11 (enero a diciembre) y los días de la semana de 0 a 6 (domingo a sábado).\nLa creación de las variables de intervención que estiman el efecto de la Semana Santa es aún más complejo. El efecto del viernes de Semana Santa ya queda recogido en la variable DiasNoLaborables. Lo que vamos a hacer ahora es crear una variable que permita estimar el efecto de las vacaciones escolares (y de muchos padres y madres) de lunes a Jueves Santo en aquellas comunidades donde así es; y otra variable para estimar el efecto de las vacaciones escolares que tienen lugar la semana posterior al Domingo de Resurrección, de lunes a viernes tras Semana Santa. Estas nuevas variables (DiasPreSanta y DiasPascua) valdrán cero para los meses distintos de marzo y abril, pero para marzo y abril valdrán la proporción de días vacacionales que recaen en el correspondiente mes.\n\nDías no laborables\nGeneramos la variables para DiasNoLaborables y se muestra su valor para los últimos 5 años.\n\nDiasNoLaborables <- monthdays(DiasLaborables) - DiasLaborables\npDiasNoLaborables <- monthdays(pDiasLaborables) - pDiasLaborables\ntail(DiasNoLaborables, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015  11   8   9   9  11   8   8  10   8  10   9  10\n2016  12   8   9   9   9   8  10   9   8  11   9  11\n2017  10   8   8  11   9   8  10   9   9  10   9  13\n2018   9   8  10   9   9   9   9   9  10   9   9  12\n2019   9   8  10   9   9  10   8  10   9   8  10  11\n\n\nNavidad cae en lunes\nGeneramos la variable que identifica los mes de diciembre en los que la Navidad cayó en lunes.\n\nfechas <- as.POSIXlt(seq(from = as.Date(\"1996-1-1\"), \n                         to = as.Date(\"2024-12-31\"), \n                         by = 1))\nLunesNavidad <- 1*(fechas$wday == 1 & fechas$mon == 11 & fechas$mday == 25)\nfechas <- format(fechas, format = \"%Y-%m\")\nLunesNavidad <- tapply(LunesNavidad, fechas, sum)\nLunesNavidad <- ts(LunesNavidad, start = 1996, frequency = 12)\npLunesNavidad <- subset(LunesNavidad, start = length(LunesNavidad) - 59)\nLunesNavidad <- subset(LunesNavidad, end = length(LunesNavidad) - 60)\n\nLunesNavidad[LunesNavidad == 1]\n\n2000-12 2006-12 2017-12 \n      1       1       1 \n\n\nSemana Santa\nSe generan las variables DiasPreSanta y DiasPascua y se muestra su valor para los últimos 5 años.\n\nLunSanto <- Easter(1996:2024, shift = -6)\nMarSanto <- Easter(1996:2024, shift = -5)\nMieSanto <- Easter(1996:2024, shift = -4)\nJueSanto <- Easter(1996:2024, shift = -3)\n\nPreSanta <- c(LunSanto, MarSanto, MieSanto, JueSanto)\nbiz <- fechaDiaria[isBizday(fechaDiaria, holidays = PreSanta, wday = 0:6)]\nbizdays <- format(biz, format = \"%Y-%m\")\n\nDiasPreSanta <- table(bizdays)\nDiasPreSanta <- ts(DiasPreSanta, start = 1996, frequency = 12)\nDiasPreSanta <- (monthdays(DiasPreSanta) - DiasPreSanta)/4\n\npDiasPreSanta <- subset(DiasPreSanta, start = length(DiasPreSanta) - 59)\nDiasPreSanta <- subset(DiasPreSanta, end = length(DiasPreSanta) - 60)\n\nLunPascua <- Easter(1996:2024, shift = 1)\nMarPascua <- Easter(1996:2024, shift = 2)\nMiePascua <- Easter(1996:2024, shift = 3)\nJuePascua <- Easter(1996:2024, shift = 4)\nViePascua <- Easter(1996:2024, shift = 5)\n\nPascua <- c(LunPascua, MarPascua, MiePascua, JuePascua, ViePascua)\nbiz <- fechaDiaria[isBizday(fechaDiaria, holidays = Pascua, wday = 0:6)]\nbizdays <- format(biz, format = \"%Y-%m\")\n\nDiasPascua <- table(bizdays)\nDiasPascua <- ts(DiasPascua, start = 1996, frequency = 12)\nDiasPascua <- (monthdays(DiasPascua) - DiasPascua)/5\n\npDiasPascua <- subset(DiasPascua, start = length(DiasPascua) - 59)\nDiasPascua <- subset(DiasPascua, end = length(DiasPascua) - 60)\n\ntail(DiasPreSanta, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2016 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2017 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2018 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2019 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n\ntail(DiasPascua, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2016 0.0 0.0 0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2017 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2018 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2019 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n\n\nAhora tenemos todos los elementos para pedir la identificación automática con auto.arima.\n\nauto.arima(Pasajeros, \n           lambda = 0,\n           d = 1, \n           D = 1,\n           xreg = cbind(DiasLaborables, DiasNoLaborables, \n                        LunesNavidad, DiasPreSanta, DiasPascua))\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5775  -0.4194          0.0352            0.0159         0.028\ns.e.   0.0496   0.0531          0.0057            0.0057         0.008\n      DiasPreSanta  DiasPascua\n           -0.0577     -0.0248\ns.e.        0.0042      0.0070\n\nsigma^2 = 0.0003214:  log likelihood = 718.25\nAIC=-1420.51   AICc=-1419.97   BIC=-1391.58\n\n\nLa identificación automática muestra el proceso de las aerolíneas. Además, parece que todas las variables de intervención son significativas.\n\n\n\n\nIdentificación automática con seas\nLa función seas de seasonal incluye automáticamente durante la identificación las variables de intervención necesarias.\n\nsummary(seas(log(Pasajeros)))\n\n\nCall:\nseas(x = log(Pasajeros))\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \nLeap Year          0.026067   0.005643   4.620 3.84e-06 ***\nWeekday            0.005285   0.000265  19.946  < 2e-16 ***\nEaster[1]         -0.086964   0.003186 -27.296  < 2e-16 ***\nAO2005.Jul        -0.054099   0.013221  -4.092 4.28e-05 ***\nAR-Nonseasonal-01 -0.980370   0.271938  -3.605 0.000312 ***\nAR-Nonseasonal-02 -0.597933   0.121438  -4.924 8.49e-07 ***\nAR-Nonseasonal-03 -0.305587   0.086942  -3.515 0.000440 ***\nMA-Nonseasonal-01 -0.561895   0.281101  -1.999 0.045619 *  \nMA-Seasonal-12     0.382738   0.055910   6.846 7.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (3 1 1)(0 1 1)  Obs.: 288  Transform: none\nAICc: -1375, BIC: -1339  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 54.28 *** Shapiro (normality): 0.9895 *\n\n\nEn este caso el proceso identificado en la parte regular es más complejo que el obtenido con auto.arima, ARIMA(3,1,1). Además, se han incluido variables de intervención asociadas a los años bisiestos, la Semana Santa y días laborables. Conjuntamente estas variables de intervención recogen los mismos efectos considerados por nosotros.\nConcluimos que el modelo de partida para Pasajeros será \\(\\log(Pasajeros) \\sim ARIMA_{12}(0, 1, 1)(0, 1, 1) + AI\\)."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#estimación-del-modelo-e-identificación-de-otras-componentes-de-intervención",
    "href": "03-14-Ejemplo-Pasajeros.html#estimación-del-modelo-e-identificación-de-otras-componentes-de-intervención",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.3 Estimación del modelo e identificación de otras componentes de intervención",
    "text": "5.3 Estimación del modelo e identificación de otras componentes de intervención\nVamos a estimar el modelo identificado y a analizar la presencia de otros valores atípicos en el residuo.\n\nPasajerosAri <- Arima(Pasajeros, \n                      lambda = 0,\n                      order = c(0, 1, 1),  \n                      seasonal = c(0, 1, 1),\n                      xreg = cbind(DiasLaborables, DiasNoLaborables, \n                                   LunesNavidad, DiasPreSanta, DiasPascua))\nPasajerosAri\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5775  -0.4194          0.0352            0.0159         0.028\ns.e.   0.0496   0.0531          0.0057            0.0057         0.008\n      DiasPreSanta  DiasPascua\n           -0.0577     -0.0248\ns.e.        0.0042      0.0070\n\nsigma^2 = 0.0003214:  log likelihood = 718.25\nAIC=-1420.51   AICc=-1419.97   BIC=-1391.58\n\n\n\nerror <- residuals(PasajerosAri)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 0, 2, 3)*sderror, \n             colour = c(\"red\", \"blue\", \"black\", \"blue\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 14: Error + Intervención. Modelo ARIMA\n\n\n\n\nEn la Figura 14 identificamos tres meses en los que el error supera o casi alcanza las tres desviaciones típicas y son candidatos a valores atípicos: abril de 2002, agosto de 2005 y marzo de 2010. No es fácil conocer las causas para estos valores atípicos.\nTras incluir las correspondientes variables artificiales en el modelo y estimarlo, identificamos otro valor extremo en agosto de 2006 y procedemos a incluirlo en el modelo y repetir el análisis. En esta ocasión ya no identificamos más valores atípicos (véase Figura 15).\n\nd0402 <- 1*(trunc(time(Pasajeros)) == 2002 & cycle(Pasajeros) == 4)\nd0805 <- 1*(trunc(time(Pasajeros)) == 2005 & cycle(Pasajeros) == 8)\nd0806 <- 1*(trunc(time(Pasajeros)) == 2006 & cycle(Pasajeros) == 8)\nd0310 <- 1*(trunc(time(Pasajeros)) == 2010 & cycle(Pasajeros) == 3)\n\nPasajerosAri <- Arima(Pasajeros,\n                      lambda = 0,\n                      order = c(0, 1, 1),  \n                      seasonal =  c(0, 1, 1),\n                      xreg = cbind(DiasLaborables, DiasNoLaborables, \n                                   LunesNavidad, DiasPreSanta, DiasPascua,\n                                   d0402, d0805, d0806, d0310))\nPasajerosAri\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5276  -0.3752          0.0343            0.0158        0.0259\ns.e.   0.0539   0.0591          0.0051            0.0052        0.0072\n      DiasPreSanta  DiasPascua   d0402   d0805   d0806   d0310\n           -0.0579     -0.0253  0.0333  0.0644  0.0284  0.0377\ns.e.        0.0042      0.0067  0.0130  0.0126  0.0126  0.0126\n\nsigma^2 = 0.0002842:  log likelihood = 737.61\nAIC=-1451.23   AICc=-1450.03   BIC=-1407.82\n\n\n\nerror <- residuals(PasajerosAri)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 0, 2, 3)*sderror, \n             colour = c(\"red\", \"blue\", \"black\", \"blue\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 15: Error + Intervención. Modelo ARIMA\n\n\n\n\nAntes de finalizar el proceso de identificación vamos a confirmar la significatividad de todos los parámetros estimados haciendo uso del estadístico de Wald (véase Tabla 2).\n\ndatos <- NULL\nfor(i in 1:length(coef(PasajerosAri))) {\n  datos <- rbind(datos,\n                 data.frame(\n                   \"Coeficiente\" = names(coef(PasajerosAri))[i],\n                   \"Valor de p\" = wald.test(b = coef(PasajerosAri), \n                                            Sigma = vcov(PasajerosAri), \n                                            Terms = i)$result$chi2[3])\n                 )\n}\n\n\nkable(datos, digits = 4, row.names = FALSE)\n\n\n\nTabla 2: Contrastes de significatividad\n\n\nCoeficiente\nValor.de.p\n\n\n\n\nma1\n0.0000\n\n\nsma1\n0.0000\n\n\nDiasLaborables\n0.0000\n\n\nDiasNoLaborables\n0.0023\n\n\nLunesNavidad\n0.0003\n\n\nDiasPreSanta\n0.0000\n\n\nDiasPascua\n0.0001\n\n\nd0402\n0.0106\n\n\nd0805\n0.0000\n\n\nd0806\n0.0246\n\n\nd0310\n0.0028"
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#validación-del-modelo",
    "href": "03-14-Ejemplo-Pasajeros.html#validación-del-modelo",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.4 Validación del modelo",
    "text": "5.4 Validación del modelo\nEn el proceso de validación analizaremos la calidad de ajuste y predicción del modelo estimado.\n\n\n\nCalidad del ajuste\nAnalizando los criterios de bondad de ajuste (sobre el error de predicción intramuestral a un periodo vista) se tiene un error medio (ME) de -0.05 millones de pasajeros, prácticamente cero por lo que no parece que haya sesgo en las predicciones; en media nos equivocamos 3.7 millones de pasajeros (RMSE) y el error porcentual medio (MAPE) es 1.3%, muy bajo. Para ambos indicadores de bondad de ajuste el error obtenido es la mitad que el visto con Alisado Exponencial.\n\n\n                ME RMSE  MAE  MPE MAPE MASE ACF1\nTraining set -0.05 3.72 2.92 0.01 1.26 0.35 0.02\n\n\n\n\n\n\nCalidad de las predicciones\nSe completará el proceso de validación estimado el error de predicción extramuestral según el horizonte temporal. Se considerarán 12 años para el periodo de estimación y un año para el de predicción.\n\nk <- 144                  \nh <- 12                   \nT <- length(Pasajeros)    \ns<-T - k - h            \n\nmapeArima <- matrix(NA, s + 1, h)\n\nX <- cbind(DiasLaborables, DiasNoLaborables, \n           LunesNavidad, DiasPreSanta, DiasPascua,\n           d0402, d0805, d0806, d0310)\n\nfor (i in 0:s) {\n  train.set <- subset(Pasajeros, start = i + 1, end = i + k)\n  test.set <-  subset(Pasajeros, start = i + k + 1, end = i + k + h) \n  \n  X.train <- X[(i + 1):(i + k),]\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- X[(i + k + 1):(i + k + h),]\n  X.test <- X.test[, hay>0]\n  \n  fit <- try(Arima(train.set, \n                   lambda = 0,\n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   xreg=X.train), silent = TRUE)\n  \n  if (!is.element(\"try-error\", class(fit))) {\n    fcast <- forecast(fit, h = h, xreg = X.test)\n    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n\nerrorArima <- colMeans(mapeArima, na.rm = TRUE)\nround(errorArima, 2)\n\n [1] 1.39 1.53 1.61 1.72 1.87 1.96 2.08 2.22 2.35 2.48 2.62 2.79\n\n\nEl error es creciente en el horizonte temporal de predicción. Para predicciones extramuestrales a un periodo vista vale 1.4%, algo superior al error de estimación, pero realmente bajo. Incluso para predicciones a doce meses vista, el error sigue siendo reducido, 2.8%. Recordemos que para Alisado Exponencial era de 3.6%."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#interpretación-del-modelo-estimado",
    "href": "03-14-Ejemplo-Pasajeros.html#interpretación-del-modelo-estimado",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.5 Interpretación del modelo estimado",
    "text": "5.5 Interpretación del modelo estimado\nEl modelo estimado y validado corresponde al modelo de las aerolíneas con intervención: \\(ARIMA_{12}(0,1,1)(0,1,1) + AI\\). La ecuación teórica completa del modelo es:\n\\[(1-L)(1-L^{12})\\log(Pasajeros) = (1+\\theta_1 L)(1 + \\theta_{12} L^{12})\\varepsilon_t+\\]\n\\[\\gamma_1 DiasLaborables +\\gamma_2 DiasNoLaborables +\\gamma_3 LunesNavidad+\\]\n\\[\\gamma_4 DiasPreSanta + \\gamma_5 DiasPascua +\\]\n\\[\\gamma_6 d0402 +\\gamma_7 d0805 +\\gamma_8 d0806 +\\gamma_9 d0310.\\]\nSi se desarrolla el modelo y se deja en función de la tasa de variación anual del número de pasajero, queda (la parte de intervención no cambia):\n\\[TVAPasajeros_t = TVAPasajeros_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12}+ \\theta_1 \\theta_{12} \\varepsilon_{t-13}+\\varepsilon_t + AI.\\]\nFinalmente, el modelo estimado es:\n\\[\\widehat{TVAPasajeros}_t = TVAPasajeros_{t-1} -0.53 \\varepsilon_{t-1} -0.37 \\varepsilon_{t-12}+ 0.20 \\varepsilon_{t-13} +\\]\n\\[0.034\\cdot DiasLaborables +0.016\\cdot DiasNoLaborables+ 0.026\\cdot LunesNavidad\\]\n\\[- 0.058\\cdot DiasPreSanta - 0.025\\cdot DiasPascua +\\]\n\\[0.033\\cdot d0402 +0.064\\cdot d0805 +0.028\\cdot d0806 +0.038\\cdot d0310.\\] Interpretación:\n\nLa tasa de variación anual del número de pasajeros en transporte urbano para un mes dado es la misma que la observada en el mes previo.\nSi hace uno, doce o trece meses se observó un número atípico de pasajeros, se debe tener en cuenta para corregir la predicción.\nCada día laborable adicional en un mes supone un incremento en el número de pasajeros del 3.4% y cada día no laborable un incremento adicional del 1.6%.\nSi la Navidad cae en lunes y por tanto Nochebuena en domingo, ese mes de diciembre el número de pasajeros será un 2.6% superior al de un mes de diciembre donde la Navidad no cae en lunes.\nSi los días laborables (lunes a jueves) de la Semana Santa caen íntegramente en marzo, ese mes el número de pasajeros cae un 5.8% respecto de un marzo sin Semana Santa. Lo mismo pasaría con abril.\nDe la misma forma, si los días laborables (lunes a viernes) de la semana posterior a Domingo de Resurrección (semana de Pascua) caen en marzo, ese mes el número de pasajeros cae un 2.5% respecto de un marzo sin Pascua. Lo mismo para abril.\nAdemas, para cuatro meses se observó una tasa de variación anual atípicamente superior a la esperada."
  },
  {
    "objectID": "03-14-Ejemplo-Pasajeros.html#predicción-del-número-de-pasajeros-en-transporte-urbano",
    "href": "03-14-Ejemplo-Pasajeros.html#predicción-del-número-de-pasajeros-en-transporte-urbano",
    "title": "Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.6 Predicción del número de pasajeros en transporte urbano",
    "text": "5.6 Predicción del número de pasajeros en transporte urbano\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones para los próximos años. Para las variables de intervención sujetas a fecha de calendario ya hemos ido creando sus valores previstos, para las demás los fijaremos a cero.\n\npPasajerosAri <- forecast(PasajerosAri, \n                          h = 60,\n                          xreg = cbind(pDiasLaborables, pDiasNoLaborables, \n                                       pLunesNavidad, pDiasPreSanta, pDiasPascua,\n                                       rep(0, 60), rep(0, 60), \n                                       rep(0 ,60), rep(0, 60)), \n                          level = 95)\nautoplot(pPasajerosAri, \n         xlab = \"\",\n         ylab = \"\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1996, 2024, 4))\n\n\n\n\nFigura 16: Pasajeros (1996-2019) y predicción (2020-2024). Modelo ARIMA\n\n\n\n\nAsí, en 2020 se esperan 3180 millones de pasajeros y para 2021 un total de 3247 millones de pasajeros."
  },
  {
    "objectID": "04-01-Bootstrapping.html#idea-general",
    "href": "04-01-Bootstrapping.html#idea-general",
    "title": "Bootstrapping para obtener intervalos de predicción",
    "section": "2.1 Idea general",
    "text": "2.1 Idea general\nVeamos primero la idea general y luego los detalles:\n\nPartimos de una serie temporal \\(\\{y_t\\}_{t=1}^T\\) y un horizonte de predicción \\(h\\).\nA partir de la serie original vamos a generar una nueva serie que es similar a la original. Luego veremos como.\nAjustamos nuestro modelo a la nueva serie y obtenemos una predicción \\(h\\) periodos hacia adelante, que llamaremos \\(\\hat y_{T+h|T}^1\\).\nRepetimos el paso 2 y 3 un numero \\(n\\) de veces (típicamente \\(n=5000\\)), de forma que al final del proceso tenemos \\(n\\) predicciones \\(h\\) periodos hacia adelante: \\(\\hat y_{T+h|T}^1,\\; \\hat y_{T+h|T}^2,\\; \\ldots,\\; \\hat y_{T+h|T}^n\\).\nPor último, obtenemos el intervalo de predicción calculando los percentiles correspondientes a partir de estas \\(n\\) predicciones.\n\nEste proceso hay que repetirlo para cada horizonte de predicción en que estemos interesados."
  },
  {
    "objectID": "04-01-Bootstrapping.html#detalles",
    "href": "04-01-Bootstrapping.html#detalles",
    "title": "Bootstrapping para obtener intervalos de predicción",
    "section": "2.2 Detalles",
    "text": "2.2 Detalles\n\nPaso 2: nueva serie\nEl paso clave del proceso es el paso 2, donde se obtiene una nueva serie similar a la original. En lo que viene a continuación no voy a ser riguroso para no perdernos en cuestiones matemáticas, pero sí suficientemente preciso para entender bien el proceso.\n\nDada la serie original, la descomponemos en sus tres componentes: tendencia, estacionalidad y error.\nA continuación, obtenemos una versión barajada de la componente del error. (Aquí es donde no estoy siendo preciso porque el proceso de barajado se tiene que hacer por bloques y es con reemplazamiento.)\nAhora se combinan –sumando o multiplicando, según el esquema– la tendencia, la estacionalidad y el error barajado para obtener una nueva serie que se parecerá a la original.\n\nVeamos un ejemplo muy sencillo, para una serie simulada de orden estacional 5.\n\nLa dos primeras columnas indican la estación de cada dato y el valor de la serie, para un total de 15 observaciones.\nLas columnas Tendencia, Estacionalidad y Error han sido obtenidas aplicando el método de descomposición por regresiones locales ponderadas. Observa que cada dato de la serie es la suma de estas tres componentes.\nLa columna ErrorBootstrapping se ha obtenido como un muestreo con reemplazamiento de los datos de la columna Error. Observa que como es una muestra con reemplazamiento hay algunos errores repetidos.\nPor último, la nueva serie (columna NuevaSerie) se ha obtenido sumando las columnas Tendencia, Estacionalidad y ErrorBootstrapping. Esta serie tiene la misma tendencia y estacionalidad que la serie original y solo se diferencia en el error, así que la nueva serie debería parecerse a la serie original.\n\n\n\n   Estacion Serie Tendencia Estacionalidad  Error ErrorBootstrapping NuevaSerie\n1         1 27.00     19.96           9.60  -2.56              -2.96      26.59\n2         2 16.72     25.12         -13.33   4.93               9.43      21.22\n3         3 15.08     30.14         -12.09  -2.96              -2.56      15.49\n4         4 18.79     34.77          -9.11  -6.87               3.10      28.76\n5         5 75.53     38.58          24.94  12.01              12.01      75.53\n6         1 63.31     39.24           9.60  14.47               3.10      51.94\n7         2 17.28     38.48         -13.33  -7.87               9.43      34.58\n8         3 18.00     34.84         -12.09  -4.75              -6.87      15.88\n9         4 24.84     30.85          -9.11   3.10              -0.40      21.34\n10        5 54.67     30.13          24.94  -0.40              -6.87      48.21\n11        1 30.85     31.11           9.60  -9.86              12.01      52.72\n12        2 22.02     30.52         -13.33   4.84              -9.86       7.32\n13        3 26.51     29.17         -12.09   9.43               3.10      20.18\n14        4 24.14     27.94          -9.11   5.31              -7.87      10.96\n15        5 41.31     26.54          24.94 -10.17              12.01      63.49\n\n\n\n\nPaso 5: Predicción por intervalos\nVamos a aclarar también el paso 5.\nPartimos de \\(n\\) predicciones a \\(h\\) periodos vista (\\(\\hat y_{T+h|T}^1,\\; \\hat y_{T+h|T}^2,\\; \\ldots,\\; \\hat y_{T+h|T}^n\\)) y queremos obtener a partir de ellas el intervalo de confianza.\nSupongamos que el nivel de confianza deseado es del 95%. Entonces, debemos calcular para las predicciones el percentil 2.5% y 97.5%. Recuerda que el percentil 2.5% es el valor numérico que deja un 2.5% de las predicciones por debajo de él; y que el percentil 97.5% es el valor numérico que deja un 97.5% de las predicciones por debajo de él. La función de R quantile() permite obtener estos valores.\nSi denominamos \\(l_h\\) al percentil 2.5% y \\(u_h\\) al percentil 97.5%, el intervalo de confianza de la predicción a \\(h\\) periodos vista es \\([l_h,\\; u_h]\\).\n\n\n¿Y la prediccion puntual?\nPara la predicción puntual tenemos dos opciones: obtener la predicción a partir de la serie original (como hemos visto en clase); u obtenerla como media de las \\(n\\) predicciones obtenidas\n\\[\\frac{1}{n}\\sum_{j=1}^n \\hat y_{T+h|T}^j\\]\nEste segundo método es el usual y se denomina bagging de bootstrap aggregating."
  },
  {
    "objectID": "05-Recursos-R.html",
    "href": "05-Recursos-R.html",
    "title": "Recursos de la asignatura",
    "section": "",
    "text": "Durante el curso usaremos diferentes ficheros de datos para los ejemplos y para el código en R. Desde esta página puedes descargarte todo el material\n\n\n\n\n\n\nFicheros de datos\n\nTítulos publicados: Títulos publicados en España (fuente Instituto Nacional de Estadística). Es una serie anual de 1993 a 2019.\nNacimientos: Nacimientos en España (fuente Instituto Nacional de Estadística). Serie mensual de enero de 1975 a diciembre de 2019.\nConsumo eléctrico: Consumo eléctrico en España en GWh (fuente Red Eléctrica de España). Es una serie diaria desde el 1 de enero de 2021 hasta el 31 de diciembre de 2021.\nAforo de vehículos en Oropesa: Aforo de vehículos por Oropesa, carretera N-340, km. 996,48 (fuente Ministerio de Fomento). La serie es anual de 1960 a 2019.\nConsumo de alimentos per cápita: Consumo alimentario en hogar per cápita en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (fuente Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (fuente Instituto Nacional de Estadística). Es una serie anual de 1987 a 2021 y la unidad es el Kg per cápita.\nExportaciones de España a la UE27: volumen, en millones de euros, de exportaciones de bienes desde España hacía la UE27 (conjunto de 27 países de la Unión Europea, con Reino Unido ya ha excluido), desde enero de 1999 hasta diciembre de 2021.\nProducción de Chocolate en Australia: Producción de chocolate en Australia desde enero de 1958 hasta diciembre de 1994.\nPernoctaciones: Número de pernoctaciones que los turistas extranjeros realizan en España en alojamientos turísticos autorizados (fuente Eurostat). Es una serie mensual de enero de 2000 a diciembre de 2019.\nPasajeros en transporte urbano: Número de pasajeros en transporte urbano en España (fuente Instituto Nacional de Estadística). Serie mensual de enero 1996 a diciembre de 2021.\n\n\n\n\nTodos los ficheros de datos en un único fichero comprimido\n\n\n\n\n\n\nCódigo R\nTienes el código para los temas de teoría aquí:\n\nCódigo para el tema 2\nCódigo para el tema 3\nCódigo para el tema 4\nCódigo para el tema 5\nCódigo para el tema 6\nCódigo para el tema 7\n\n\nPara el ejemplo de Pernoctaciones tienes el código aquí:\n\nCódigo para el ejemplo del tema 2\nCódigo para el ejemplo del tema 3\nCódigo para el ejemplo del tema 4\nCódigo para el ejemplo del tema 5\nCódigo para el ejemplo del tema 6\nCódigo para el ejemplo del tema 7\n\nY para el ejemplo de Pasajeros en transporte urbano aquí:\n\nCódigo para el ejemplo Pasajeros en transporte urbano\n\n\n\nY si quieres todos los ficheros de código de golpe aquí:\n\nTodos los ficheros de código en un único fichero comprimido"
  },
  {
    "objectID": "03-13-Ejemplo7.html#diferenciación-y-logaritmo",
    "href": "03-13-Ejemplo7.html#diferenciación-y-logaritmo",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "2.1 Diferenciación y logaritmo",
    "text": "2.1 Diferenciación y logaritmo\nEl esquema multiplicativo de la serie aconseja el uso de la transformación logarítmica. Además, vamos a ver que para que la serie sea estacionaria es necesario diferenciarla tanto regular como estacionalmente, así que el uso de logaritmo vuelve a ser aconsejable si queremos ganar en interpretabilidad.\nLa Figura 2 muestra la FAC para la serie Pernoctaciones (log) y algunas de sus transformaciones. En los paneles a) y b) las autocorrelaciones estacionales decrecen muy lentamente, indicando que la serie analizada no es ergódica. El panel c) muestra que las autocorrelaciones en la parte regular decrecen lentamente, indicando que la serie analizada no es estacionaria. Solo la doble diferenciación regular y estacional de la serie muestra un rápido descenso en los coeficiente de autocorrelación (panel d), indicando que la serie así transformada es estacionaria en media y ergódica.\n\nggAcf(log(Pernoctaciones), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pernoctaciones)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pernoctaciones), lag = 12), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(diff(log(Pernoctaciones), lag=12)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n(a) Log serie\n\n\n\n\n\n\n\n(b) Dif. regular log serie\n\n\n\n\n\n\n\n\n\n(c) Dif. estacional log serie\n\n\n\n\n\n\n\n(d) Dif. regular y estacional log serie\n\n\n\n\nFigura 2: FAC para Pernoctaciones\n\n\n\nPor otro lado, la identificación automática de la diferenciación también concluye que es necesaria la doble diferenciación.\n\nndiffs(log(Pernoctaciones))\n\n[1] 1\n\nnsdiffs(log(Pernoctaciones))\n\n[1] 1\n\n\nVamos a asumir que el proceso debe ser doblemente diferenciado \\(\\log(Pernoctaciones) \\sim I(1)I_{12}(1)\\)."
  },
  {
    "objectID": "03-13-Ejemplo7.html#identificación-del-orden-regular-y-estacional",
    "href": "03-13-Ejemplo7.html#identificación-del-orden-regular-y-estacional",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "2.2 Identificación del orden regular y estacional",
    "text": "2.2 Identificación del orden regular y estacional\nVamos a identificar los valores de \\(p\\), \\(q\\), \\(P\\) y \\(Q\\). Para ello solicitaremos con auto.arima y seas una identificación automática.\nCon auto.arima incluiremos dos efectos calendarios, uno para el número de días del mes y otro para el efecto Semana Santa.\n\nDiasMes <- monthdays(Pernoctaciones)\nSemanaSanta <- easter(Pernoctaciones)\n\nauto.arima(Pernoctaciones, \n           d = 1, \n           D = 1,\n           lambda = 0,\n           xreg = cbind(DiasMes, SemanaSanta))\n\nSeries: Pernoctaciones \nRegression with ARIMA(0,1,1)(2,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sar1     sar2    sma1     sma2  DiasMes  SemanaSanta\n      -0.3207  -0.7187  -0.0442  0.3377  -0.4315   0.0293       0.0426\ns.e.   0.0709   0.1720   0.1677  0.1683   0.1689   0.0077       0.0060\n\nsigma^2 = 0.0008503:  log likelihood = 483.35\nAIC=-950.7   AICc=-950.04   BIC=-923.3\n\n\nLa función auto.arima identifica un proceso \\(ARIMA_{12}(0,1,1)(2,1,2)\\), donde el coeficiente para sar2 no parece ser significativo (el coeficiente no supera los dos errores estándar).\nLa identificación alcanzada por seas es un proceso \\(ARIMA_{12}(0,1,1)(0,1,1)\\) (modelo de las aerolíneas) de la transformación logarítmica, con intervención en Semana Santa y días laborables. Además, se identifica un cambio de nivel desde marzo de 2005.\n\nsummary(seas(Pernoctaciones))\n\n\nCall:\nseas(x = Pernoctaciones)\n\nCoefficients:\n                    Estimate Std. Error z value             Pr(>|z|)    \nWeekday           -0.0009593  0.0003988  -2.406             0.016143 *  \nEaster[8]          0.0425480  0.0051831   8.209 0.000000000000000223 ***\nLS2005.Mar        -0.0819394  0.0225926  -3.627             0.000287 ***\nMA-Nonseasonal-01  0.2976707  0.0633552   4.698 0.000002621542191344 ***\nMA-Seasonal-12     0.4286878  0.0590994   7.254 0.000000000000405599 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 240  Transform: log\nAICc:  6634, BIC:  6654  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.):  38.2 * Shapiro (normality): 0.9964  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nVemos que ambas identificaciones difieren notablemente. Así, vamos a partir de la identificación obtenida por seas, el clásico modelo de las aerolíneas: \\[log(Pernoctaciones) \\sim ARIMA_{12}(0,1,1)(0,1,1)\\].\nRespecto de la intervención, incluiremos días del mes y Semana Santa. No se incluirá el cambio de nivel identificado por seas dado que no existe una justificación para ello. En 2005 no ocurrió nada que justifique una caída permanente del 8.2% en el número de pernoctaciones internacionales (coeficiente -0.0819)."
  },
  {
    "objectID": "03-13-Ejemplo7.html#calidad-de-ajuste",
    "href": "03-13-Ejemplo7.html#calidad-de-ajuste",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "7.1 Calidad de ajuste",
    "text": "7.1 Calidad de ajuste\nLa serie Pernoctaciones la hemos ajustado por el método ingenuo con estacionalidad, el método de Alisado Exponencial y procesos ARIMA. La Tabla 1 recoge el error medio (RMSE) y porcentual (MAPE) al usar estas tres aproximaciones con la serie original y su transformación logarítmica.\n\n\n\n\n\nTabla 1: Criterios de calidad para previsiones intramuestrales a un periodo vista. Varios modelos\n\n\n\n\n\n\n\n\nSerie\nMétodo\nRMSE\nMAPE\n\n\n\n\nPernoctaciones\nIngenuo\n1243441\n4.60\n\n\nlog(Pernoctaciones)\nIngenuo\n1243441\n4.60\n\n\nPernoctaciones\nAlisado\n727645\n2.61\n\n\nlog(Pernoctaciones)\nAlisado\n738036\n2.62\n\n\nPernoctaciones\nARIMA\n620624\n2.53\n\n\nlog(Pernoctaciones)\nARIMA\n606418\n2.19\n\n\n\n\nPodemos extraer varias conclusiones: i) la transformación logarítmica no mejora necesariamente el ajuste de los datos; ii) el proceso ARIMA con transformación logarítmica es la mejor aproximación; y iii) el método de alisado muestra una calidad de ajuste comparable a la de los procesos ARIMA. La ligera mejora en el ajuste de los modelos ARIMA respecto del método de Alisado se debe a la incorporación de variables ficticias para recoger la intervención."
  },
  {
    "objectID": "03-13-Ejemplo7.html#predicciones-extramuestrales",
    "href": "03-13-Ejemplo7.html#predicciones-extramuestrales",
    "title": "Pernoctaciones en alojamientos turísticos de turistas extranjeros",
    "section": "7.2 Predicciones extramuestrales",
    "text": "7.2 Predicciones extramuestrales\nVamos a determinar si también la aplicación de modelos ARIMA mejora la calidad de las predicciones extra-muestrales lo suficiente como para justificar su uso –frente a los métodos de alisado, mucho más sencillos. Para ello, aplicaremos la metodología de origen de predicción móvil para estimar la capacidad predictiva del modelo ARIMA y compararla con el modelo de Alisado y el ingenuo con estacionalidad.\n\nk <- 120                   \nh <- 12                    \nT <- length(Pernoctaciones)     \ns <- T - k - h               \n\nmapeIngenuo <- matrix(NA, s + 1, h)\nmapeAlisado <- matrix(NA, s + 1, h)\nmapeAlisadoLog <- matrix(NA, s + 1, h)\nmapeArima <- matrix(NA, s + 1, h)\nmapeArimaLog <- matrix(NA, s + 1, h)\n\n\nX <- data.frame(cbind(DiasMes, SemanaSanta, d0305, d0406, d0513))\n\nfor (i in 0:s) {\n  train.set <- subset(Pernoctaciones, start = i + 1, end = i + k)\n  test.set <-  subset(Pernoctaciones, start = i + k + 1, end = i + k + h) \n  \n  X.train <- data.frame(X[(i + 1):(i + k),])\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- data.frame(X[(i + k + 1):(i + k + h),])\n  X.test <- X.test[, hay>0]\n  \n  #Ingenuo\n  fit <- snaive(train.set, h = h)\n  mapeIngenuo[i + 1,] <- 100*abs(test.set - fit$mean)/test.set\n  \n  #Alisado sin log\n  fit <- ets(train.set, model = \"MNM\")\n  fcast <- forecast(fit, h = h) \n  mapeAlisado[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n  #Alisado con log\n  fit <- ets(train.set, model = \"ANA\", lambda = 0)\n  fcast <- forecast(fit, h = h) \n  mapeAlisadoLog[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n  #ARIMA sin log\n  fit <- try(Arima(train.set, \n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   xreg = as.matrix(X.train)), silent = TRUE)\n  \n  if (!is.element(\"try-error\", class(fit))) {\n    fcast <- forecast(fit, h = h, xreg = as.matrix(X.test))\n    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n  \n  #ARIMA con log\n  fit <- try(Arima(train.set, \n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   lambda = 0,\n                   xreg = as.matrix(X.train)), silent = TRUE)\n  \n  if (!is.element(\"try-error\", class(fit))) {\n    fcast <- forecast(fit, h = h, xreg = as.matrix(X.test)) \n    mapeArimaLog[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n  \n}\n\nmapeIngenuo <- colMeans(mapeIngenuo)\nmapeAlisado <- colMeans(mapeAlisado)\nmapeAlisadoLog <- colMeans(mapeAlisadoLog)\nmapeArima <- colMeans(mapeArima, na.rm = TRUE)\nmapeArimaLog <- colMeans(mapeArimaLog, na.rm = TRUE)\n\n\nggplot() +\n  geom_line(aes(x = 1:12, y = mapeIngenuo, colour = \"Ingenuo\")) +\n  geom_line(aes(x = 1:12, y = mapeAlisado, colour = \"Alisado\")) + \n  geom_line(aes(x = 1:12, y = mapeAlisadoLog, colour = \"Alisado (log)\")) +\n  geom_line(aes(x = 1:12, y = mapeArima, colour = \"Arima\")) +\n  geom_line(aes(x = 1:12, y = mapeArimaLog, colour = \"Arima (log)\")) +\n  ggtitle(\"\") +\n  xlab(\"\") +\n  ylab(\"MAPE\") +\n  scale_x_continuous(breaks= 1:12) +\n  scale_color_discrete(name = \"Modelos\")\n\n\n\n\nFigura 6: Errores de previsión extra-muestral. Varios modelos\n\n\n\n\nLa Figura 6 revela que ARIMA con transformación logarítmica es siempre superior a Alisado en calidad de predicciones.\nTambién se observa que la transformación logarítmica no afecta significativamente la calidad de las predicciones en los modelos de Alisado, pero es muy determinante en los procesos ARIMA.\nComo ya vimos, el error con el método ingenuo parece independiente del horizonte temporal. A corto plazo es la peor aproximación, pero a largo plazo es tan bueno como el resto de métodos."
  },
  {
    "objectID": "03-07-Tema7.html#diferenciación-estacional",
    "href": "03-07-Tema7.html#diferenciación-estacional",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "2.1 Diferenciación estacional",
    "text": "2.1 Diferenciación estacional\nLa diferencia estacional consiste en restar a la observación de un periodo la observación precedente de la misma estación. Si el orden estacional es \\(m\\), entonces la diferencia estacional de \\(y_t\\) es \\[\\nabla_m y_t = y_t - y_{t-m}.\\] Una serie no estacionaria en media puede pasar a serlo tras diferenciarla estacionalmente. Es general, cualquiera de las dos diferenciaciones vistas (regular o estacional) o ambas a la vez son alternativas para obtener la estacionariedad.\nAdemás, la diferenciación (regular, estacional o ambas) también permite alcanzar la ergodicidad.\nLa Figura 1 muestra un ejemplo de diferenciación regular y/o estacional. En el primer panel aparece la serie original Nacimientos \\(y_t\\); el segundo panel muestra la serie diferenciada regularmente \\(\\nabla y_t\\); en el tercer panel la serie diferenciada estacionalmente \\(\\nabla_{12} y_t\\); y en el cuarto panel muestra la serie diferenciada regular y estacionalmente \\(\\nabla\\nabla_{12} y_t\\).\n\n\n\n\n\nFigura 1: Nacimientos mensuales y diferenciaciones\n\n\n\n\n¿Qué transformación para nacimientos consideras que genera una serie estacionaria, tanto en media como en varianza? Siempre hay un cierto grado de subjetividad en la elección de las diferencias que hay que aplicar a una serie. En la Figura 1 podemos considerar que la diferenciación regular (panel 2) es suficiente para lograr la estacionariedad en media y en varianza y terminar el proceso de diferenciación. Pero también podemos considerar que la serie es estacionaria en media pero no lo suficiente en varianza, y optar por la doble diferenciación, regular y estacional (panel 4).\n\nDiferenciación con R\nRecuerda que diff(x, lag = m) calcula la diferencia de orden \\(m\\). Si necesitas calcular una diferencia regular y otra estacional, \\(\\nabla\\nabla_m y_t\\), debes usar diff(diff(x, lag = m)). El orden de las diferenciaciones no cambia el resultado.\nAdemás, en forecast está disponible la función nsdiffs que estima el número de diferencias estacionales necesarias usando un criterio ad-hoc. Recuerda que también dispones de ndiffs para determinar el número de diferencias regulares."
  },
  {
    "objectID": "03-07-Tema7.html#diferencia-logaritmo-y-tasa-de-variación",
    "href": "03-07-Tema7.html#diferencia-logaritmo-y-tasa-de-variación",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "2.2 Diferencia, Logaritmo y Tasa de variación",
    "text": "2.2 Diferencia, Logaritmo y Tasa de variación\nYa vimos que la diferencia regular del logaritmo es una tasa de variación, \\(\\nabla \\log(y_t) =TV y_t\\). Por ejemplo, para una serie mensual, la diferencia regular del logaritmo es la Tasa de Variación Mensual de la serie \\((\\nabla \\log(y_t)=TVM y_t)\\).\nSi se opta por la diferencia estacional, entonces \\(\\nabla_m \\log(y_t) \\approx \\frac{y_t - y_{t-m}}{y_{t-m}} =TV_m y_t\\). Es decir, para una serie mensual la diferencia estacional del logaritmo es la Tasa de Variación Anual de la serie \\((\\nabla_{12} \\log(y_t)=TVA y_t)\\).\nCuando una serie tiene que ser diferenciada tanto regular como estacionalmente para conseguir su estacionariedad, podemos escribir\n\\[\\nabla_m \\nabla \\log(y_t) = \\nabla_m TV y_t = TV y_t - TV y_{t-m},\\] o podemos escribir\n\\[\\nabla \\nabla_m \\log(y_t) = \\nabla TV_m y_t = TV_m y_t - TV_m y_{t-1}.\\]\nPor ejemplo, para una serie mensual una doble diferenciación sobre el logaritmo se puede interpretar o bien como una diferencia estacional de tasas mensuales \\(TVM y_t - TVM y_{t-12}\\) (primer caso) o bien como una diferencia regular de tasas anuales \\(TVA y_t - TVA y_{t-1}\\) (segundo caso).\n\n\n\n\n\n\n\n\nLa diferenciacion y el logaritmo en series con estacionalidad\n\n\n\n\nSi se tiene una serie con estacionalidad y no estacionaria, lo usual es tener que diferenciarla una vez (regular o estacionalmente) o aplicar ambas diferenciaciones para que sea estacionaria en media, en sentido amplio y ergódica.\nSi hay una doble diferenciación, la transformación logarítmica se puede usar para ganar en interpretabilidad o para intentar mejorar las predicciones.\nLa transformación logarítmica no es necesaria para alcanzar la estacionariedad de la serie."
  },
  {
    "objectID": "03-07-Tema7.html#procesos-autorregresivos-ar_mp",
    "href": "03-07-Tema7.html#procesos-autorregresivos-ar_mp",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "3.1 Procesos autorregresivos \\(AR_m(P)\\)",
    "text": "3.1 Procesos autorregresivos \\(AR_m(P)\\)\n\nDefinición\nEl modelo general autorregresivo estacional de orden P, \\(y_t \\sim AR_m(P)\\) viene definido por \\[y_t=c + \\phi_m y_{t-m} + \\phi_{2m} y_{t-2m} + \\ldots + \\phi_{Pm} y_{t-Pm} + \\varepsilon_t,\\] que usando el operador retardo queda \\[(1 - \\phi_m L^m - \\phi_{2m} L^{2m} - \\ldots - \\phi_{Pm} L^{Pm})y_t = c + \\varepsilon_t.\\]\n\n\nEjemplos\n\n\\(y_t \\sim AR_{12}(1):\\;\\;y_t = c + \\phi_{12} y_{t-12} + \\varepsilon_t\\) o \\((1 - \\phi_{12} L^{12})y_t = c + \\varepsilon_t\\)\n\\(y_t \\sim AR_7(2):\\;\\;y_t = c + \\phi_7 y_{t-7} + \\phi_{14} y_{t-14} + \\varepsilon_t\\) o \\((1 - \\phi_7 L^7 - \\phi_{14} L^{14})y_t = c + \\varepsilon_t\\)"
  },
  {
    "objectID": "03-07-Tema7.html#procesos-en-medias-móviles-ma_mq",
    "href": "03-07-Tema7.html#procesos-en-medias-móviles-ma_mq",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "3.2 Procesos en medias móviles \\(MA_m(Q)\\)",
    "text": "3.2 Procesos en medias móviles \\(MA_m(Q)\\)\n\nDefinición\nEl modelo general en medias móviles estacional de orden Q, \\(y_t \\sim MA_m(Q)\\) viene definido por \\[y_t=c + \\varepsilon_t + \\theta_m \\varepsilon_{t-m} + \\theta_{2m} \\varepsilon_{t-2m} + \\ldots +\n  \\theta_{Qm} \\varepsilon_{t-Qm},\\] que usando el operador retardo queda \\[y_t = c + (1 + \\theta_m L^m + \\theta_{2m} L^{2m} + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t.\\]\n\n\nEjemplos\n\n\\(y_t \\sim MA_7(1):\\;\\;y_t = c + \\varepsilon_t + \\theta_7 \\varepsilon_{t-7}\\) o \\(y_t = c + (1 + \\theta_7 L^7)\\varepsilon_t\\)\n\\(y_t \\sim MA_{12}(2):\\;\\;y_t=c + \\varepsilon_t + \\theta_{12} \\varepsilon_{t-12} + \\theta_{24} \\varepsilon_{t-24}\\) o \\(y_t = c + (1 + \\theta_{12} L^{12} + \\theta_{24} L^{24})\\varepsilon_t\\)"
  },
  {
    "objectID": "03-07-Tema7.html#procesos-arma_mpq",
    "href": "03-07-Tema7.html#procesos-arma_mpq",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "3.3 Procesos \\(ARMA_m(P,Q)\\)",
    "text": "3.3 Procesos \\(ARMA_m(P,Q)\\)\n\nDefinición\nEl modelo general \\(y_t \\sim ARMA_m(P,Q)\\) viene definido por\n\\[ y_t = c + \\phi_m y_{t-m} + \\phi_{2m} y_{t-2m} + \\ldots + \\phi_{Pm} y_{t-Pm}  +\n  \\varepsilon_t + \\theta_m \\varepsilon_{t-m} + \\theta_{2m} \\varepsilon_{t-2m} + \\ldots +\n  \\theta_{Qm} \\varepsilon_{t-Qm},\n\\] que usando el operador retardo queda\n\\[(1 - \\phi_m L^m - \\ldots - \\phi_{Pm} L^{Pm})y_t = c + (1 + \\theta_m L^m + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t.\\]\n\n\nEjemplos\n\n\\(y_t \\sim ARMA_7(1, 1):\\;\\;y_t = c + \\phi_7 y_{t-7} + \\theta_7 \\varepsilon_{t-7} + \\varepsilon_{t}\\) o \\((1 - \\phi_7 L^7)y_t = c + (1 + \\theta_7 L^7)\\varepsilon_t\\).\n\\(y_t \\sim ARMA_{12}(1, 1):\\;\\;y_t = c + \\phi_{12} y_{t-12} + \\theta_{12} \\varepsilon_{t-12} + \\varepsilon_{t}\\) o \\((1 - \\phi_{12} L^{12})y_t = c + (1 + \\theta_{12} L^{12})\\varepsilon_t\\)."
  },
  {
    "objectID": "03-07-Tema7.html#procesos-arima_mpdq",
    "href": "03-07-Tema7.html#procesos-arima_mpdq",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "3.4 Procesos \\(ARIMA_m(P,D,Q)\\)",
    "text": "3.4 Procesos \\(ARIMA_m(P,D,Q)\\)\nSi la serie \\(y_t\\) no es estacionaria en su parte estacional, pero tras diferenciarla \\(D\\) veces se hace estacionaria, diremos que la serie es integrada estacionalmente de orden \\(D\\): \\(y_t \\sim I_m(D)\\). Por tanto,\n\nuna serie estacionaria estacionalmente se indicará como \\(y_t \\sim I_m(0)\\).\n\\(y_t \\sim I_m(1)\\) es equivalente a \\(\\nabla_m y_t = (1 - L^m)y_t \\sim I_m(0)\\)\n\nUna serie \\(y_t\\) sigue un proceso \\(ARIMA_m(P,D,Q)\\) si:\n\n\\(y_t \\sim I_m(D)\\), hay que diferenciarla estacionalmente \\(D\\) veces para hacerla estacionaria, y\n\\(\\nabla_m^D y_t \\sim ARMA_m(P,Q)\\).\n\nEntonces, podemos escribir \\(y_t \\sim ARIMA_m(P,D,Q)\\) como \\[\\begin{equation*}\n\\begin{array}{c@{\\quad}ccc}\n  (1 - \\phi_m L^m - \\ldots - \\phi_{Pm} L^{Pm}) & (1- L^m)^D y_t & = & c + (1 + \\theta_m L^m + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t \\\\\n  \\uparrow                                     & \\uparrow       &   & \\uparrow \\\\\n   AR_m(P)                                     & I_m(D)         &   & MA_m(Q)\n\\end{array}\n\\end{equation*}\\]\n\nEjemplo\n\n\\(y_t \\sim ARIMA_{12}(1, 1, 1):\\;\\;(1 - \\phi_{12} L^{12})(1- L^{12}) y_t = c + (1 + \\theta_{12} L^{12}) \\varepsilon_t\\) o \\(y_t = c + y_{t-12} + \\phi_{12}(y_{t-12} - y_{t-24}) + \\theta_{12} \\varepsilon_{t-12} + \\varepsilon_t\\)."
  },
  {
    "objectID": "03-07-Tema7.html#proceso-arima_mpdqpdq",
    "href": "03-07-Tema7.html#proceso-arima_mpdqpdq",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "3.5 Proceso \\(ARIMA_m(p,d,q)(P,D,Q)\\)",
    "text": "3.5 Proceso \\(ARIMA_m(p,d,q)(P,D,Q)\\)\nLa realidad nos muestra que la mayoría de las series con estacionalidad se ajustan a una combinación de procesos regulares y estacionales.\nEl proceso \\(ARIMA_m(p, d, q)(P, D, Q)\\) puede ser expresado como \\[\\begin{equation*}\n\\begin{array}{ccccc}\n  AR(p) & AR_m(P) & I(d) & I_m(D) &  \\\\\n  \\downarrow & \\downarrow & \\downarrow & \\downarrow  &  \\\\\n  (1 - \\phi_1 L - \\ldots - \\phi_p L^p) & (1 - \\phi_m L^m - \\ldots - \\phi_{Pm} L^{Pm}) & (1 - L)^d & (1- L^m)^Dy_t & = \\\\\n  c + (1 + \\theta_1 L + \\ldots + \\theta_q L^q) & (1 + \\theta_m L^m + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t & & & \\\\\n  \\uparrow & \\uparrow & & & \\\\\n  MA(q) & MA_m(Q)  & & &\n\\end{array}\n\\end{equation*}\\]\nPor ejemplo, entre las series mensuales uno de los procesos más comunes es \\(ARIMA_{12}(0, 1, 1)(0, 1, 1)\\), denominado modelo de las aerolíneas por ser el proceso generador de datos de muchas series mensuales de transporte de pasajeros, en concreto la serie mensual de pasajeros de avión. La ecuación de este modelo es\n\\[(1-L)(1-L^{12})y_t = (1+ \\theta_1L)(1 + \\theta_{12}L^{12})\\varepsilon_t\\] que si desarrollamos queda \\[y_t = y_{t-1} + (y_{t-12} - y_{t-13}) + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12} + \\theta_{1}\\theta_{12} \\varepsilon_{t-13} + \\varepsilon_t \\]\n\nEl número de pasajeros del mes \\(t\\) es el mismo que el del mes previo \\(t-1\\), más la diferencia entre estos meses observada el año pasado.\nSi en los meses usados para la predicción (\\(t-1\\), \\(t-12\\) y \\(t-12\\)) ha ocurrido algo extraordinario, hay que tenerlo en cuenta a la hora de afinar la predicción.\n\nSi usamos la transformación logarítmica, tendríamos \\[(1-L)(1-L^{12})\\log(y_t) = (1+ \\theta_1L)(1 + \\theta_{12}L^{12})\\varepsilon_t\\] o \\[(1-L)TVAy_t = (1+ \\theta_1L)(1 + \\theta_{12}L^{12})\\varepsilon_t\\] que desarrollando queda \\[TVAy_t = TVAy_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12} + \\theta_{1}\\theta_{12} \\varepsilon_{t-13} + \\varepsilon_t \\]\n\nLa tasa de variación anual en el número de pasajeros del mes \\(t\\) es la misma que la del mes previo \\(t-1\\).\nSi en los meses usados para la predicción (\\(t-1\\), \\(t-12\\) y \\(t-12\\)) ha ocurrido algo extraordinario, hay que tenerlo en cuenta a la hora de afinar la predicción."
  },
  {
    "objectID": "03-07-Tema7.html#nacimientos",
    "href": "03-07-Tema7.html#nacimientos",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "4.1 Nacimientos",
    "text": "4.1 Nacimientos\nVamos a aplicar la metodología de Box-Jenkins a la serie de nacimientos en España desde el año 2000 (véase Figura 2).\n\nnacimientos <- read.csv2(\"./series/nacimientos.csv\", header = TRUE)\nnacimientos <- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  freq = 12)\nnacimientos <- window(nacimientos, start = 2000)\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\")\n\n\n\n\nFigura 2: Nacimientos mensuales\n\n\n\n\n\nTransformación de la serie\nPara nacimientos las funciones ndiffs y nsdiffs sugieren la doble diferenciación regular y estacional para alcanzar la estacionariedad.\n\nndiffs(nacimientos)\n\n[1] 1\n\nnsdiffs(nacimientos)\n\n[1] 1\n\n\nVeamos ahora que transformación es necesaria para que sea ergódica. Además, trabajaremos con el logaritmo de la serie para ganar en interpretabilidad.\nObserva el diferente uso que hacemos del argumento lag. Como argumento de la función diff indica el orden estacional de la diferenciación y vale 12 (lag = 12). Como argumento de la función ggAcf indica el número de autocorrelaciones que queremos mostrar gráficamente y lo hemos fijado en 48 (lag = 48) para ver al menos lo que ocurre en 4 años.\n\nggAcf(log(nacimientos), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(nacimientos)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(nacimientos), lag = 12), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(diff(log(nacimientos), lag = 12)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n(a) Serie\n\n\n\n\n\n\n\n(b) Diferencia regular\n\n\n\n\n\n\n\n\n\n(c) Diferencia estacional\n\n\n\n\n\n\n\n(d) Diferencia regular y estacional\n\n\n\n\nFigura 3: FAC para Nacimientos (log)\n\n\nPara que la serie sea ergódica también es necesaria la doble diferenciación regular y estacional. Es decir, trabajaremos con la siguiente serie transformada \\[\\nabla\\nabla_{12}\\log(nacimientos_t) \\sim I(0)I_{12}(0).\\]\n\n\nIdentificación\n¿Qué nos indica auto.arima? Primero vamos a generar e incluir en el proceso de autoidentificación las variables asociadas a los efectos de intervención que hemos detectado en los temas previos. En concreto, hemos visto que el número de días del mes explica el número de nacimientos. Este efecto era muy claro para los meses de febrero bisiestos. Para el calculo de la variable que recoge el número de días del mes usaremos la función monthdays de la librería forecast que devuelve el número de días de cada mes o trimestre de una serie.\n\nmonthdays(nacimientos)\n\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015  31  28  31  30  31  30  31  31  30  31  30  31\n2016  31  29  31  30  31  30  31  31  30  31  30  31\n2017  31  28  31  30  31  30  31  31  30  31  30  31\n2018  31  28  31  30  31  30  31  31  30  31  30  31\n2019  31  28  31  30  31  30  31  31  30  31  30  31\n\n\nPor otro lado, los periodos vacacionales pueden afectar la programación de las cesáreas e influir en el número de nacimientos. Como la Semana Santa es un periodo festivo que puede caer en marzo o abril, dependiendo del año, los nacimientos en estos dos meses pueden variar según como cae la Semana Santa. Para capturar este efecto, usaremos la función easter de la librería forecast que devuelve para cada mes la proporción de días de la Semana Santa que contiene (considerando solo del Viernes Santo al Domingo de Resurrección, tres días).\n\neaster(nacimientos)\n\n\n\n      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n2015 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2016 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2017 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2018 0.00 0.00 0.67 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2019 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n\n\nAdemás, en enero de 2011 el número de nacimientos era atípico.\n\nDiasMes <- monthdays(nacimientos)\nSemanaSanta <- easter(nacimientos)\nd0111 <- 1*(cycle(nacimientos) == 1 & trunc(time(nacimientos)) == 2011)\n\nauto.arima(nacimientos, \n           d = 1, \n           D = 1, \n           lambda = 0,\n           xreg = cbind(DiasMes, SemanaSanta, d0111))\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1     sma2  DiasMes  SemanaSanta    d0111\n      -0.4544  -0.6131  -0.1738   0.0341      -0.0156  -0.0694\ns.e.   0.0666   0.0674   0.0668   0.0078       0.0047   0.0164\n\nsigma^2 = 0.0004364:  log likelihood = 555.16\nAIC=-1096.31   AICc=-1095.8   BIC=-1072.34\n\n\nIndica \\(ARIMA_{12}(0,1,1)(0,1,2)\\).\n\n\nUna alternativa a auto.arima es la función seas de la librería seasonal. La función seas tiene como ventajas que también analiza la conveniencia de usar la transformación logarítmica, que identifica posibles efectos calendario y valores extremos, y que suele ser más parsimoniosa que auto.arima. Su desventaja es que sólo se puede aplicar para series mensuales o trimestrales. Veamos que identificación ofrece seas:\n\nsummary(seas(nacimientos))\n\n\nCall:\nseas(x = nacimientos)\n\nCoefficients:\n                    Estimate Std. Error z value             Pr(>|z|)    \nWeekday            0.0028161  0.0002681  10.504 < 0.0000000000000002 ***\nEaster[1]         -0.0127198  0.0037801  -3.365             0.000766 ***\nAO2010.Dec         0.0656998  0.0133509   4.921         0.0000008611 ***\nMA-Nonseasonal-01  0.3447018  0.0612765   5.625         0.0000000185 ***\nMA-Seasonal-12     0.7477045  0.0466619  16.024 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 240  Transform: log\nAICc:  3597, BIC:  3617  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 27.65   Shapiro (normality): 0.993  \n\n\nEn primer lugar, la función identifica el modelo de las aerolíneas para la transformación logarítmica de Nacimientos. Este resultado está en consonancia con el alcanzado con la función auto.arima. Además, identifica un valor extremo en diciembre de 2010, un efecto calendario Semana Santa y un efecto calendario días laborables del mes (que podemos entender similar a nuestro efecto días del mes).\nTras estos análisis alternativos, decidimos que la identificación de partida es \\(ARIMA_{12}(0,1,1)(0,1,2)+AI\\), \\[(1 - L^{12})(1 - L)\\log(nacimientos_t) = (1 + \\theta_1 L)(1 + \\theta_{12} L^{12} + \\theta_{24} L^{24})\\varepsilon_t + AI.\\] donde AI recoge las variables de intervención incluidas en auto.arima y la identificada por seas.\n\n\nEstimación (y valores extremos)\nVamos a estimar el modelo identificado, incluidas las variables de intervención.\n\nd1210 <- 1*(cycle(nacimientos) == 12 & trunc(time(nacimientos)) == 2010)\nnac.ar1 <- Arima(nacimientos, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 2),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, d1210,  d0111))\nnac.ar1\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1     sma2  DiasMes  SemanaSanta   d1210    d0111\n      -0.4835  -0.6222  -0.1545   0.0330      -0.0156  0.0583  -0.0532\ns.e.   0.0649   0.0698   0.0698   0.0077       0.0046  0.0168   0.0166\n\nsigma^2 = 0.0004167:  log likelihood = 561.17\nAIC=-1106.34   AICc=-1105.68   BIC=-1078.94\n\n\nYa tenemos un modelo de partida, veamos si es necesaria más intervención.\n\nerror <- residuals(nac.ar1)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2,2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(2000, 2020, 2)) \n\n\n\n\nFigura 4: Error + Intervención\n\n\n\n\nSe observa que hay tres candidatos a valores atípicos en diciembre 2006, abril de 2016 y junio de 2016, dado que los errores asociados se acercan o superan las 3 desviaciones típicas. Por tanto,\n\nSe crea una variable de intervención para cada caso\nSe estima de nuevo el modelo incluyendo estas variables (auto.arima identifica el mismo modelo).\nSe vuelve a analizar si quedan valores atípicos\n\n\nd1206 <- 1*(cycle(nacimientos) == 12 & trunc(time(nacimientos)) == 2006)\nd0416 <- 1*(cycle(nacimientos) == 4 & trunc(time(nacimientos)) == 2016)\nd0616 <- 1*(cycle(nacimientos) == 6 & trunc(time(nacimientos)) == 2016)\n\nnac.ar2 <- Arima(nacimientos, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 2),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, \n                              d1206, d1210, d0111, d0416, d0616))\nnac.ar2\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1     sma2  DiasMes  SemanaSanta    d1206   d1210    d0111\n      -0.4427  -0.6395  -0.1399   0.0321      -0.0206  -0.0404  0.0559  -0.0545\ns.e.   0.0690   0.0730   0.0742   0.0073       0.0046   0.0155  0.0160   0.0158\n        d0416   d0616\n      -0.0504  0.0293\ns.e.   0.0161  0.0152\n\nsigma^2 = 0.0003855:  log likelihood = 571.64\nAIC=-1121.28   AICc=-1120.05   BIC=-1083.61\n\nerror <- residuals(nac.ar2)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(2000, 2020, 2)) \n\n\n\n\nFigura 5: Error + Intervención\n\n\n\n\nEs posible que alguno de los errores más elevados corresponda a un intervención, pero a fin de no alargar este ejemplo vamos a asumir que ya no hay valores extremos.\n\n\nCompensación\nPodemos observar que en todos los modelos estimados los coeficientes de las variables de intervención de los meses consecutivos diciembre de 2010 y enero de 2011 son iguales pero de signo opuesto. A este tipo de intervención se le denomina compensación: el efecto extraordinario en un periodo se compensa con un efecto de igual magnitud pero signo opuesto en el periodo siguiente.\nLa causa detrás de esta compensación puede ser tan prosaica como que por error muchos nacimientos ocurridos en enero de 2011 se asignaron informáticamente a diciembre de 2010. O quizás algo pasó en esos meses que adelanto un número considerable de nacimientos.\nVamos a crear una variable de intervención asociada a esta compensación. Es tan sencillo como definir una variable ficticia que valga cero siempre excepto para los meses de diciembre de 2010 y enero de 2011 que valdrá 1 y - 1 respectivamente.\n\nd12100111 <- d1210 - d0111\n\nAhora vamos a sustituir las dos variables ficticias d1210 y d0111 del modelo por la compensación d12100111.\n\nnac.ar3 <- Arima(nacimientos, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 2),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, \n                              d1206, d12100111, d0416, d0616))\nnac.ar3\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1     sma2  DiasMes  SemanaSanta    d1206  d12100111\n      -0.4420  -0.6393  -0.1402   0.0321      -0.0206  -0.0405     0.0552\ns.e.   0.0679   0.0729   0.0740   0.0073       0.0046   0.0155     0.0095\n        d0416   d0616\n      -0.0503  0.0293\ns.e.   0.0161  0.0152\n\nsigma^2 = 0.0003837:  log likelihood = 571.64\nAIC=-1123.28   AICc=-1122.26   BIC=-1089.03\n\n\nLos coeficientes estimados en este modelo son prácticamente iguales a los obtenidos en el modelo previo (sin la compensación).\n\n\nValidación\nVamos si todos los coeficientes del modelo son significativos. Para ello, aplicamos la prueba de Wald.\n\nancho <- max(nchar(names(coef(nac.ar3)))) + 2\nfor(i in 1:length(coef(nac.ar3))) {\n  wt <- wald.test(b = coef(nac.ar3), \n                  Sigma = vcov(nac.ar3), \n                  Terms = i)\n  cat(\"\\nCoeficiente: \", \n      format(names(coef(nac.ar3))[i], width = ancho), \n      \"valor de p: \", \n      formatC(wt$result$chi2[3], digits = 4, format = \"f\"))\n}\n\n\nCoeficiente:  ma1           valor de p:  0.0000\nCoeficiente:  sma1          valor de p:  0.0000\nCoeficiente:  sma2          valor de p:  0.0582\nCoeficiente:  DiasMes       valor de p:  0.0000\nCoeficiente:  SemanaSanta   valor de p:  0.0000\nCoeficiente:  d1206         valor de p:  0.0091\nCoeficiente:  d12100111     valor de p:  0.0000\nCoeficiente:  d0416         valor de p:  0.0018\nCoeficiente:  d0616         valor de p:  0.0544\n\n\nEl coeficiente estacional en medias móviles de orden dos no resulta individualmente significativo al 5% (aunque si lo sería al 10%). Vamos a optar por eliminarlo. Tampoco es significativo el coeficiente de la intervención de junio de 2016, sin embargo su efecto (Figura 4) es tan patente que vamos a dejarlo en el modelo.\n\nnac.ar4 <- Arima(nacimientos, \n                 order = c(0, 1, 1),\n                 seasonal =  c(0, 1, 1),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, \n                              d1206, d12100111, d0416, d0616))\nnac.ar4\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasMes  SemanaSanta    d1206  d12100111    d0416\n      -0.4737  -0.7523   0.0317      -0.0209  -0.0471     0.0582  -0.0510\ns.e.   0.0619   0.0479   0.0076       0.0049   0.0156     0.0097   0.0166\n       d0616\n      0.0295\ns.e.  0.0158\n\nsigma^2 = 0.000388:  log likelihood = 569.86\nAIC=-1121.72   AICc=-1120.89   BIC=-1090.89\n\n\n\n\nError de estimación\nEl error medio de -76 es muy bajo en comparación con el valor medio de la serie, es decir, no hay sesgo.\nEn media nos equivocamos en 706 bebés (RMSE) y el error porcentual medio es del 1.5%.\n\naccuracy(nac.ar4)\n\n\n\n                 ME  RMSE    MAE   MPE MAPE MASE ACF1\nTraining set -75.66 705.7 568.84 -0.22 1.54 0.41 0.02\n\n\n\n\nError de predicción estramuestral según horizonte temporal\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k = 120\\), y fijaremos el horizonte temporal en un año, \\(h = 12\\) meses.\nEl código es aun más complejo que el visto en el tema previo. Por un lado, hemos de tener en cuenta que hay variables de intervención y por otro lado que la función Arima podría fallar en el proceso de estimación.\n\nk <- 120                   \nh <- 12                    \nT <- length(nacimientos)   \ns<-T - k - h               \n\nmapeArima <- matrix(NA, s + 1, h)\n\nX <- data.frame(cbind(DiasMes, SemanaSanta, d1206, d12100111, d0416, d0616))\n\nfor (i in 0:s) {\n  train.set <- subset(nacimientos, start = i + 1, end = i + k)\n  test.set <-  subset(nacimientos, start = i + k + 1, end = i + k + h) \n  \n  X.train <- data.frame(X[(i + 1):(i + k),])\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- data.frame(X[(i + k + 1):(i + k + h),])\n  X.test <- X.test[, hay>0]\n  \n  if (length(X.train) > 0) {\n    fit <- try(Arima(train.set, \n                     order = c(0, 1, 1),\n                     seasonal = c(0, 1, 1),\n                     lambda = 0,\n                     xreg=as.matrix(X.train)))\n  } else {\n    fit <- try(Arima(train.set, \n                     order = c(0, 1, 1),\n                     seasonal = c(0, 1, 1),\n                     lambda = 0))\n  }\n  \n  if(!is.element(\"try-error\", class(fit))) {\n    if (length(X.train) > 0) \n      fcast <- forecast(fit, h = h, xreg = as.matrix(X.test)) else\n        fcast <- forecast(fit, h = h)\n      \n    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n  \nerrorArima <- colMeans(mapeArima, na.rm = TRUE)\nerrorArima\n\n [1] 1.814097 1.995340 2.173217 2.396488 2.430399 2.457098 2.621557 2.582823\n [9] 2.655552 2.834819 2.905893 3.001149\n\n\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorArima), colour = \"Blue\") +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"%\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\nFigura 6: Error de predicción (MAPE) según horizonte temporal\n\n\n\n\nLa Figura 6 revela que el error de predicción aumenta de forma regular según aumenta el horizonte de predicción, pero incluso a un año vista, se mantiene bajo.\n\n\nInterpretación\nEl modelo teórico es \\(log(nacimientos) \\sim ARIMA_{12}(0, 1, 1)(0, 1, 1) + AI\\),\n\\[(1 - L^{12})(1 - L)\\log(nacimientos_t) = (1 + \\theta_1 L)(1 + \\theta_{12} L^{12})\\varepsilon_t + AI.\\]\nSi sustituimos \\((1 - L^{12})\\log(nacimientos_t)\\) por \\(TVA_{nacimientos_t}\\), la tasa de variación anual de los nacimientos, y desarrollamos queda\n\\[TVA_{nacimientos_t} = TVA_{nacimientos_{t-1}} + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12} + \\theta_1\\theta_{12} \\varepsilon_{t-13} +  \\varepsilon_{t}+\\]\n\\[\\gamma_1 \\cdot DiasMes_t + \\gamma_2 \\cdot SemanaSanta +\\gamma_3 \\cdot d1206_t + \\gamma_4 \\cdot d12100111_t +\\]\n\\[\\gamma_5 \\cdot d0416_t + \\gamma_6 \\cdot d0616_t.\\]\nFinalmente, el modelo estimado es,\n\\[\\widehat{TVA}_{nacimientos_{t}} = TVA_{nacimientos_{t-1}}  - 0.47\\varepsilon_{t-1} - 0.75\\varepsilon_{t-12} + 0.36\\varepsilon_{t-13} + \\]\n\\[0.032 \\cdot DiasMes_t - 0.021 \\cdot SemanaSanta - 0.047 \\cdot d1206_t + 0.058 \\cdot d12100111_t\\]\n\\[- 0.051 \\cdot d0416_t + 0.029 \\cdot d0616_t.\\]\n\nEn cada mes, la tasa de variación anual de los nacimientos es la misma que la del mes pasado (\\(\\widehat{TVA}_{nacimientos_t} = TVA_{nacimientos_{t-1}}\\)).\nAdemás, si algunos de los meses necesarios para predecir fue anómalo, el error hay que tenerlo en cuenta para afinar la previsión.\nRespecto de la intervención, cada día adicional de un mes nacen un 3.2% más bebés. El mes en que cae la Semana Santa los nacimientos caen un 2.1%. Por alguna razón, en diciembre de 2006 hubo un 4.7% menos de nacimientos de lo esperado; en diciembre de 2010 hubo un 5.8% más de nacimientos de lo esperado que fue compensado en enero de 2011; en abril de 2016 hubo un 5.1% menos de nacimientos de lo esperado; y en junio de 2016 hubo un 2.9% más de nacimientos de lo esperado.\n\n\n\nPredicción de la serie\nUna vez dado por válido el modelo podemos pasar a realizar predicciones. Hay que tener en cuenta que hay siete variables de intervención, dos de ellas son efectos calendario (DiasMes y SemanaSanta), para las que debemos indicar qué valores tomarán en el periodo de predicción. Vamos a fijar el horizonte de predicción en cuatro años y mostrar los resultados numérica (solo para el primer año) y gráficamente (Figura 7).\n\ntmp <- ts(rep(0, 48), start = 2020, freq = 12)\npdm <- monthdays(tmp)\npss <- easter(tmp)\npnac.ar4 <- forecast(nac.ar4, \n                     h = 48,\n                     xreg = cbind(pdm, pss, rep(0,48), rep(0,48), \n                                  rep(0,48), rep(0,48)), \n                     level = 95)\npnac.ar4\n\n\n\n         Point Forecast    Lo 95    Hi 95\nJan 2020       29286.47 28177.31 30439.30\nFeb 2020       26971.70 25820.26 28174.49\nMar 2020       28725.16 27375.42 30141.46\nApr 2020       27437.38 26040.81 28908.83\nMay 2020       28706.72 27141.88 30361.77\nJun 2020       27880.06 26266.37 29592.88\nJul 2020       29973.84 28144.05 31922.59\nAug 2020       29933.06 28016.08 31981.21\nSep 2020       29782.18 27790.01 31917.16\nOct 2020       30477.52 28355.93 32757.86\nNov 2020       28583.15 26518.90 30808.08\nDec 2020       28335.58 26218.20 30623.97\n\n\n\nautoplot(pnac.ar4, \n     ylab = \"Nacimientos\",\n     main = \"\") +\n  scale_x_continuous(breaks= seq(2000, 2024, 4)) \n\n\n\n\nFigura 7: Nacimientos (2000-2019) y predicción (2020-2023)"
  },
  {
    "objectID": "03-07-Tema7.html#exportaciones",
    "href": "03-07-Tema7.html#exportaciones",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "4.2 Exportaciones",
    "text": "4.2 Exportaciones\nConsideremos las serie de exportaciones de bienes desde España hacía la UE27 (conjunto de 27 países de la Unión Europea, con Reino Unido ya ha excluido). La serie va de enero de 1999 hasta diciembre de 2021 y está en millones de euros.\n\nexportaciones <- read.csv2(\"./series/Exportaciones.csv\", header = TRUE)\nexportaciones <- ts(exportaciones,\n                    start = c(1999, 1),\n                    freq = 12)\n\nautoplot(exportaciones,\n         xlab = \"\",\n         ylab = \"Millones de €\",\n         main = \"\")\n\n\n\n\nFigura 8: Exportaciones de España a la EU27\n\n\n\n\n\nTransformación de la serie\nLa Figura 8 deja claro que la serie precisa ser diferenciada para ser estacionaria. Por la naturaleza de la serie previsiblemente será necesaria la doble diferenciación regular y estacional, y trabajaremos con la transformación logarítmica.\n\nggAcf(log(exportaciones), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(exportaciones)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(exportaciones), lag = 12), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(diff(log(exportaciones), lag = 12)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n(a) Serie\n\n\n\n\n\n\n\n(b) Diferencia regular\n\n\n\n\n\n\n\n\n\n(c) Diferencia estacional\n\n\n\n\n\n\n\n(d) Diferencia regular y estacional\n\n\n\n\nFigura 9: FAC para Exportaciones (log)\n\n\n\nndiffs(log(exportaciones))\n\n[1] 1\n\nnsdiffs(log(exportaciones))       \n\n[1] 1\n\n\nPara que la serie sea estacionaria y ergódica es necesaria la doble diferenciación regular y estacional. Es decir, trabajaremos con la siguiente serie transformada \\[\\nabla\\nabla_{12}\\log(exportaciones_t) \\sim I(0)I_{12}(0).\\]\n\n\nIdentificación\nLa Figura 8 muestra dos periodos con una marcada intervención: al inicio de la Gran Recesión (2008-2014) y durante el periodo más duro de la Covid-19 en el año 2020. Además, por la naturaleza de la serie es previsible que exista un efecto días del mes o días laborables y un efecto Semana Santa.\nSi probamos con la función auto.arima, indicando la doble diferenciación, los días del mes y la Semana Santa, no sugiere \\(ARIMA_{12}(1,1,1)(2,1,2)\\). Demasiado complejo.\nVeamos qué nos indica seas.\n\nsummary(seas(exportaciones))\n\n\nCall:\nseas(x = exportaciones)\n\nCoefficients:\n                    Estimate Std. Error z value             Pr(>|z|)    \nWeekday            0.0094000  0.0005091  18.462 < 0.0000000000000002 ***\nEaster[1]         -0.0671147  0.0073688  -9.108 < 0.0000000000000002 ***\nAO2007.Dec        -0.0992204  0.0268412  -3.697             0.000219 ***\nLS2008.Dec        -0.2042548  0.0289028  -7.067   0.0000000000015837 ***\nLS2020.Mar        -0.2066604  0.0310729  -6.651   0.0000000000291460 ***\nAO2020.Apr        -0.3803478  0.0300162 -12.671 < 0.0000000000000002 ***\nAO2020.May        -0.1820426  0.0293634  -6.200   0.0000000005659057 ***\nMA-Nonseasonal-01  0.4151852  0.0555305   7.477   0.0000000000000762 ***\nMA-Seasonal-12     0.6278005  0.0481551  13.037 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 276  Transform: log\nAICc:  3871, BIC:  3906  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 43.82 ** Shapiro (normality): 0.9953  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nEl modelo identificado es el de las aerolíneas para la transformación logarítmica de Exportaciones. Respecto de la intervención, identifica dos efectos calendario: días laborables y Semana Santa. También se identifican hasta cinco intervenciones no asociadas a efectos calendario: tres pulsos –intervenciones que afectan un solo mes (AO)– y dos cambios permanentes –intervenciones que afectan un rango elevado de meses (LS):1\n\nUn cambio permanente (level shift o LS) empieza en diciembre de 2008. Es decir, la Gran Recesión generó una caída permanente de las exportaciones españolas a la UE27.\nEl otro cambio permanente empieza en marzo de 2020: el confinamiento generó otra caída permanente de las exportaciones.\nTambién asociado a la Covid-19 están los pulsos de abril y mayo de 2020.\nPor último, hay un pulso asociado a diciembre de 2007\n\n\n\nEstimación (y valores extremos)\nVamos a estimar el modelo identificado con seas, incluidas las variables de intervención. A este respecto unas palabras sobre como obtener los días laborables de un mes.\nEntenderemos por días laborables los lunes a viernes de cada mes, menos los días festivos. Este valor se puede obtener con la función bizdays. Un inconveniente de esta función, es que su valor depende de los festivos de cada país. Por ejemplo, en Estados Unidos el día del trabajador se celebra el primer lunes de septiembre, en Reino Unido el primer lunes de mayo y en España el 1 de mayo.\nLa función bizdays devuelve el número de días laborables de cada mes para determinados centros financieros (equivalentes a países). Por proximidad geográfica, usaremos el calendario de Londres para España.2\n\nDiasLaborables <- bizdays(exportaciones, FinCenter = \"London\")\nSemanaSanta <- easter(exportaciones)\n\nPara los pulsos, que solo afectan un mes, se crea una variable que vale cero excepto para el mes a intervenir que vale 1.\nPara los cambios permanentes que afectan desde un mes en adelante, se crea una variable que vale cero antes del mes de inicio de la intervención y 1 desde ese mes en adelante.\n\nl1208 <- 1*(trunc(time(exportaciones)) > 2008) + \n  1*(cycle(exportaciones) == 12 & trunc(time(exportaciones)) == 2008)\n\nl0320 <- 1*(trunc(time(exportaciones)) > 2020) + \n  1*(cycle(exportaciones)   > 2 & trunc(time(exportaciones)) == 2020)\n\nd1207 <- 1*(cycle(exportaciones) == 12 & trunc(time(exportaciones)) == 2007)\nd0420 <- 1*(cycle(exportaciones) ==  4 & trunc(time(exportaciones)) == 2020)\nd0520 <- 1*(cycle(exportaciones) ==  5 & trunc(time(exportaciones)) == 2020)\n\nexp.ar1 <- Arima(exportaciones, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 1),\n                 lambda = 0,\n                 xreg = cbind(DiasLaborables, SemanaSanta, \n                              l1208, l0320, d1207, d0420, d0520))\nexp.ar1\n\nSeries: exportaciones \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  SemanaSanta    l1208    l0320    d1207\n      -0.4248  -0.6283          0.0305      -0.0235  -0.2030  -0.2104  -0.1052\ns.e.   0.0527   0.0547          0.0017       0.0085   0.0293   0.0324   0.0274\n        d0420    d0520\n      -0.3842  -0.1864\ns.e.   0.0310   0.0301\n\nsigma^2 = 0.001308:  log likelihood = 501.67\nAIC=-983.34   AICc=-982.46   BIC=-947.61\n\n\nYa tenemos un modelo de partida, en el que parece que todos los coeficientes son significativos. Veamos si es necesaria más intervención.\n\nerror <- residuals(exp.ar1)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2,2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1998, 2022, 2)) \n\n\n\n\nFigura 10: Error + Intervención\n\n\n\n\nSe observan hasta siete candidatos a valores atípicos por superar el error las 2.5 desviaciones típicas. Sin embargo, a fin de aligerar este ejemplo no se van a incluir las variables de intervención asociadas.\n\n\nValidación\nVeamos si los coeficientes del modelo son significativos. Para ello, aplicamos la prueba de Wald.\n\nancho <- max(nchar(names(coef(exp.ar1)))) + 2\nfor(i in 1:length(coef(exp.ar1))) {\n  wt <- wald.test(b = coef(exp.ar1), \n                  Sigma = vcov(exp.ar1), \n                  Terms = i)\n  cat(\"\\nCoeficiente: \", \n      format(names(coef(exp.ar1))[i], width = ancho), \n      \"valor de p: \", \n      formatC(wt$result$chi2[3], digits = 4, format = \"f\"))\n}\n\n\nCoeficiente:  ma1              valor de p:  0.0000\nCoeficiente:  sma1             valor de p:  0.0000\nCoeficiente:  DiasLaborables   valor de p:  0.0000\nCoeficiente:  SemanaSanta      valor de p:  0.0059\nCoeficiente:  l1208            valor de p:  0.0000\nCoeficiente:  l0320            valor de p:  0.0000\nCoeficiente:  d1207            valor de p:  0.0001\nCoeficiente:  d0420            valor de p:  0.0000\nCoeficiente:  d0520            valor de p:  0.0000\n\n\nTodos los coeficientes son significativos.\n\n\nError de estimación\nEn media nos equivocamos en 365 millones de euros (RMSE) y el error porcentual medio es del 2.6%, muy reducido.\nLa serie no presenta sesgo y el intervalo de confianza para las predicciones es válido.\n\naccuracy(exp.ar1)\n\n\n\n                ME   RMSE    MAE   MPE MAPE MASE  ACF1\nTraining set -5.75 365.16 274.31 -0.17 2.63  0.3 -0.02\n\n\n\n\nError de predicción estramuestral según horizonte temporal\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k = 120\\), y fijaremos el horizonte temporal en un año, \\(h = 12\\) meses.\n\nk <- 120                   \nh <- 12                    \nT <- length(exportaciones)   \ns<-T - k - h               \n\nmapeArima <- matrix(NA, s + 1, h)\n\nX <- data.frame(cbind(DiasLaborables, SemanaSanta, \n                      l1208, l0320, d1207, d0420, d0520))\n\nfor (i in 0:s) {\n  train.set <- subset(exportaciones, start = i + 1, end = i + k)\n  test.set <-  subset(exportaciones, start = i + k + 1, end = i + k + h) \n  \n  X.train <- data.frame(X[(i + 1):(i + k),])\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- data.frame(X[(i + k + 1):(i + k + h),])\n  X.test <- X.test[, hay>0]\n  \n  if (length(X.train) > 0) {\n    fit <- try(Arima(train.set, \n                     order = c(0, 1, 1),\n                     seasonal = c(0, 1, 1),\n                     lambda = 0,\n                     xreg=as.matrix(X.train),\n                     optim.method = \"BFGS\"))\n  } else {\n    fit <- try(Arima(train.set, \n                     order = c(0, 1, 1),\n                     seasonal = c(0, 1, 1),\n                     lambda = 0))\n  }\n  \n  if(!is.element(\"try-error\", class(fit))) {\n    if (length(X.train) > 0) \n      fcast <- forecast(fit, h = h, xreg = as.matrix(X.test)) else\n        fcast <- forecast(fit, h = h)\n      \n    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n\n\nerrorArima <- colMeans(mapeArima, na.rm = TRUE)\nerrorArima\n\n [1] 2.794000 3.050736 3.338473 3.650616 3.892606 4.116590 4.313629 4.535789\n [9] 4.626596 4.963904 5.050367 5.351044\n\n\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorArima), colour = \"Blue\") +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"%\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\nFigura 11: Error de predicción (MAPE) según horizonte temporal\n\n\n\n\nLa Figura 11 revela que el error de predicción aumenta lentamente según aumenta el horizonte de predicción, pasando del 2.8% a un mes vista hasta el 5.4% a 12 meses vista.\n\n\nInterpretación\nLa parte regular del modelo estimado es la misma que la obtenida para la serie Nacimientos y su interpretación es, por tanto, idéntica: en cada mes, la tasa de variación anual de las exportaciones es la misma que la del mes pasado. Además, si algunos de los meses necesarios para predecir fue anómalo, el error hay que tenerlo en cuenta para afinar la previsión.\nVamos por tanto a centrarnos en la interpretación de la intervención:\n\nCada día laborable adicional en un mes aumenta las exportaciones en un 3% (coeficiente \\(0.0305\\) de DiasLAborables)\nEl mes en que cae la Semana Santa las exportaciones caen un 2.3%. (coeficiente \\(0.0235\\) de SemanaSanta)\nA raíz de la Gran Recesión, las exportaciones se redujeron de forma permanente un 20.3% desde diciembre de 2008. Es decir, sin la Gran Recesión, las exportaciones ahora serían un 20% superiores (coeficiente \\(-0.2030\\) de l1208)\nA raíz de la Covid-19, las exportaciones se redujeron de forma permanente un 21% desde marzo de 2020, inicio del confinamiento (coeficiente \\(-0.2104\\) de l0320).\nAdemás, la caída de las exportaciones por la Covid-19 fue aún más acusada en los meses de abril y mayo de 2020, con una caída adicional (a añadir al 21% ya comentado) del 38% y 19% respectivamente (coeficiente \\(-0.3842\\) de d0420 y \\(-0.1864\\) de d0520).\nEn diciembre de 2007 se observó una caída de las exportaciones del 10.5%.\n\n\n\nPredicción de la serie\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones teniendo en cuenta las siete variables de intervención:\n\nDos de ellas son efectos calendario (DiasLaborables y SemanaSanta), para las que debemos indicar qué valores tomarán en el periodo de predicción\nOtras dos son cambios permanentes y su valor debe ser 1 en el futuro.\nSolo para los pulsos, se fijará un valor futuro de 0.\n\nVamos a fijar el horizonte de predicción en cuatro años y mostrar los resultados numérica (solo para el primer año) y gráficamente (Figura 12).\nRecuerda siempre incluir las variables ficticias en la función forecast en el mismo orden que aparecen en la estimación con Arima.\n\ntmp <- ts(rep(0, 48), start = 2022, freq = 12)\npdl <- bizdays(tmp, FinCenter = \"London\")\npss <- easter(tmp)\npexp.ar1 <- forecast(exp.ar1, \n                     h = 48,\n                     xreg = cbind(pdl, pss, \n                                  rep(1,48), rep(1,48), \n                                  rep(0,48), rep(0,48), rep(0,48)), \n                     level = 95)\npexp.ar1\n\n\n\n         Point Forecast    Lo 95    Hi 95\nJan 2022       17038.34 15872.47 18289.85\nFeb 2022       17914.41 16507.87 19440.79\nMar 2022       22244.01 20301.71 24372.15\nApr 2022       18695.72 16915.72 20663.01\nMay 2022       20202.45 18133.59 22507.36\nJun 2022       21456.66 19116.67 24083.07\nJul 2022       19338.28 17109.38 21857.54\nAug 2022       15532.75 13651.96 17672.65\nSep 2022       21436.31 18722.63 24543.31\nOct 2022       21014.04 18243.91 24204.78\nNov 2022       21848.13 18859.08 25310.93\nDec 2022       19956.25 17130.79 23247.73\n\n\n\nautoplot(pexp.ar1, \n         xlab = \"\",\n         ylab = \"Millones de euros\",\n         main = \"\",\n         PI = FALSE) +\n  scale_x_continuous(breaks= seq(1998, 2026, 4)) \n\n\n\n\nFigura 12: Exportaciones (1999-2021) y predicción (2022-2025)"
  },
  {
    "objectID": "03-07-Tema7.html#chocolate",
    "href": "03-07-Tema7.html#chocolate",
    "title": "Procesos ARIMA con estacionalidad",
    "section": "4.3 Chocolate",
    "text": "4.3 Chocolate\nVamos a aplicar la metodología de Box-Jenkins a la serie mensual de producción de chocolate en Australia desde enero de 1958 hasta diciembre de 1994, 444 meses.\n\nchocolate <- read.csv2(\"./series/Chocolate.csv\", header = TRUE)\nchocolate <- ts(chocolate, start = 1958, freq = 12)\n\nautoplot(chocolate, \n         xlab = \"\", \n         ylab = \"Toneladas\",\n         main = \"\")\n\n\n\n\nFigura 13: Producción de Chocolate (Australia)\n\n\n\n\nLa producción de chocolate presenta una tendencia creciente, especialmente desde los años 80 del siglo pasado, una marcada componente estacional (donde la máxima producción se da en el invierno austral, meses de mayo a agosto, y la menor producción en enero), combinados en un esquema multiplicativo. Trabajaremos con el logaritmo de la serie para linealizar las componentes y reducir la falta de estacionariedad en varianza.\n\nTransformación de la serie\nLa Figura 14 muestra que solo la doble diferenciación regular y estacional consigue que la transformación logarítmica de Chocolate sea estacionaria y ergódica.\n\nggAcf(log(chocolate), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(chocolate)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(chocolate), lag = 12), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(diff(log(chocolate), lag = 12)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n(a) Serie\n\n\n\n\n\n\n\n(b) Diferencia regular\n\n\n\n\n\n\n\n\n\n(c) Diferencia estacional\n\n\n\n\n\n\n\n(d) Diferencia regular y estacional\n\n\n\n\nFigura 14: FAC para Chocolate (log)\n\n\nLas funciones que identifican la diferenciación de forma automática avalan esta decisión.\n\nndiffs(log(chocolate))\n\n[1] 1\n\nnsdiffs(log(chocolate))\n\n[1] 1\n\n\nPodemos concluir que \\(d = 1\\) y \\(D = 1\\) o \\(\\log(chocolate_t) \\sim I(1)I_{12}(1)\\).\n\n\nIdentificación\n¿Qué nos indica auto.arima? Para ayudar a esta función lo máximo posible vamos a definir la variable de intervención días laborables. Por proximidad cultural, histórica y política, usaremos el calendario de Londres para Australia.\n\nDiasLaborables <-  bizdays(chocolate, FinCenter = \"London\")\n\nAhora la incluimos en la autoidentificación.\n\nauto.arima(chocolate, d = 1, D = 1, \n           lambda = 0,\n           xreg = cbind(DiasLaborables))\n\nSeries: chocolate \nRegression with ARIMA(1,1,1)(0,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1     sma1     sma2    xreg\n      0.3032  -0.9078  -0.6174  -0.0922  0.0339\ns.e.  0.0619   0.0326   0.0501   0.0487  0.0040\n\nsigma^2 = 0.009433:  log likelihood = 391.34\nAIC=-770.68   AICc=-770.48   BIC=-746.28\n\n\nIndica \\(ARIMA_{12}(1,1,1)(0,1,2)\\), donde el parámetro \\(\\theta_{24}\\) (sma2) no está claro si es significativo. Además, el efecto días laborables es claramente significativo.\n\n\nPor último, la función seas (también con un poco de ayuda) identifica un proceso \\(ARIMA_{12}(1,1,1)(0,1,1)\\), con un efecto Semana Santa (de quince días) un efecto día de la semana y un efecto año bisiesto, que conjuntamente equivalen al efecto días laborables.\n\nsummary(seas(diff(diff(log(chocolate), lag = 12))))\n\n\nCall:\nseas(x = diff(diff(log(chocolate), lag = 12)))\n\nCoefficients:\n                   Estimate Std. Error z value             Pr(>|z|)    \nLeap Year          0.037906   0.031418   1.207             0.227622    \nMon                0.021718   0.017931   1.211             0.225819    \nTue                0.007760   0.018019   0.431             0.666733    \nWed                0.025674   0.018144   1.415             0.157080    \nThu               -0.002852   0.018096  -0.158             0.874763    \nFri               -0.015197   0.018124  -0.838             0.401771    \nSat               -0.063503   0.018205  -3.488             0.000486 ***\nEaster[15]        -0.095747   0.045962  -2.083             0.037234 *  \nAR-Nonseasonal-01  0.226982   0.056597   4.010            0.0000606 ***\nMA-Nonseasonal-01  0.879176   0.027636  31.812 < 0.0000000000000002 ***\nMA-Seasonal-12     0.650157   0.040017  16.247 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (1 0 1)(0 0 1)  Obs.: 431  Transform: none\nAICc: -737.6, BIC: -689.5  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 16.86   Shapiro (normality): 0.9879 ***\nMessages generated by X-13:\nWarnings:\n- Automatic transformation selection cannot be done on a series\n  with zero or negative values.\n- At least one visually significant seasonal peak has been found\n  in one or more of the estimated spectra.\n- Series should not be a candidate for seasonal adjustment\n  because the spectrum of the prior adjusted series (Table B1)\n  has no visually significant seasonal peaks.\n\n\nAparentemente ambas funciones identifican el mismo modelo, si del modelo identificado con auto.arima excluimos el parámetro no significativo. Ten en cuenta que los días laborables ya descuentan el efecto de la Semana Santa, así que no es necesario incluir el efecto Semana Santa de forma independiente.\n\n\nEstimación\nVamos a realizar la primera estimación.\n\nchoco.ar1 <- Arima(chocolate, order=c(1, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   lambda = 0,\n                   xreg = DiasLaborables)\nchoco.ar1\n\nSeries: chocolate \nRegression with ARIMA(1,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1     sma1    xreg\n      0.2844  -0.8972  -0.6718  0.0339\ns.e.  0.0620   0.0333   0.0462  0.0038\n\nsigma^2 = 0.009501:  log likelihood = 389.55\nAIC=-769.1   AICc=-768.96   BIC=-748.77\n\n\nYa tenemos un modelo de partida, veamos si es necesaria más intervención.\n\nerror <- residuals(choco.ar1)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1958, 1994, 2)) \n\n\n\n\nFigura 15: Error + Intervención\n\n\n\n\nSe observan tres valores claramente atípicos en enero de 1986, 1991 y 1994. Además, hay otro candidato a valor extremo, por superar las 2.5 desviaciones típicas, en septiembre de 1975. Por tanto,\n\nSe crea una variable de intervención para cada caso\nSe estima de nuevo el modelo incluyendo estas variables\nSe vuelve a analizar si quedan valores atípicos\n\n\nd0975 <- 1*(cycle(chocolate) == 9 & trunc(time(chocolate)) == 1975)\nd0186 <- 1*(cycle(chocolate) == 1 & trunc(time(chocolate)) == 1986)\nd0191 <- 1*(cycle(chocolate) == 1 & trunc(time(chocolate)) == 1991)\nd0194 <- 1*(cycle(chocolate) == 1 & trunc(time(chocolate)) == 1994)\n\nchoco.ar2 <- Arima(chocolate, order=c(1, 1, 1),\n                   seasonal =  c(0, 1, 1), \n                   lambda = 0,\n                   xreg = cbind(DiasLaborables, d0975, d0186, d0191, d0194))\nchoco.ar2\n\nSeries: chocolate \nRegression with ARIMA(1,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1     sma1  DiasLaborables    d0975   d0186    d0191\n      0.3052  -0.8889  -0.6778          0.0363  -0.1630  0.1291  -0.3385\ns.e.  0.0646   0.0355   0.0413          0.0035   0.0753  0.0755   0.0769\n       d0194\n      0.5128\ns.e.  0.0838\n\nsigma^2 = 0.008197:  log likelihood = 423.4\nAIC=-828.8   AICc=-828.37   BIC=-792.2\n\n\nAhora debemos volver a verificar si en el nuevo modelo hay valores extremos. Observamos que no hay.\n\nerror <- residuals(choco.ar2)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) +\n  scale_x_continuous(breaks= seq(1958, 1994, 2)) \n\n\n\n\nFigura 16: Error + Intervención\n\n\n\n\n\n\nValidación\nPor último, veamos si todos los coeficientes del modelo son significativos.\n\nancho <- max(nchar(names(coef(choco.ar2)))) + 2\nfor(i in 1:length(coef(choco.ar2))) {\n  wt <- wald.test(b = coef(choco.ar2), \n                  Sigma = vcov(choco.ar2), \n                  Terms = i)\n  cat(\"\\nCoeficiente: \", \n      format(names(coef(choco.ar2))[i], width = ancho),\n      \"valor de p: \", \n      formatC(wt$result$chi2[3], digits = 4, format = \"f\"))\n}\n\n\nCoeficiente:  ar1              valor de p:  0.0000\nCoeficiente:  ma1              valor de p:  0.0000\nCoeficiente:  sma1             valor de p:  0.0000\nCoeficiente:  DiasLaborables   valor de p:  0.0000\nCoeficiente:  d0975            valor de p:  0.0303\nCoeficiente:  d0186            valor de p:  0.0874\nCoeficiente:  d0191            valor de p:  0.0000\nCoeficiente:  d0194            valor de p:  0.0000\n\n\nAunque uno de ellos (enero 1986) no lo es al 5%, si lo es al 10% así que optamos por dejarlo.\n\n\nError de estimación\nAnalizando los criterios de bondad de ajuste se tiene que: el error medio de 3 es prácticamente cero por lo que no parece que haya sesgo en las predicciones; en media nos equivocamos en 488 toneladas (RMSE); y el error porcentual medio es 7%, razonable.\n\naccuracy(choco.ar2)\n\n\n\n             ME   RMSE    MAE   MPE MAPE MASE  ACF1\nTraining set  3 488.05 360.71 -0.33 6.99 0.69 -0.05\n\n\n\n\nError de predicción estramuestral según horizonte temporal\nAsumimos que se precisan veinte años para hacer una buena estimación, \\(k = 240\\), y fijaremos el horizonte temporal en un año, \\(h = 12\\) meses.\n\nk <- 240                \nh <- 12                 \nT <- length(chocolate)  \ns<-T - k - h            \n\nmapeArima <- matrix(NA, s + 1, h)\n\nX <- data.frame(cbind(DiasLaborables, d0975, d0186, d0191, d0194))\n\nfor (i in 0:s) {\n  train.set <- subset(chocolate, start = i + 1, end = i + k)\n  test.set <-  subset(chocolate, start = i + k + 1, end = i + k + h) \n  \n  X.train <- data.frame(X[(i + 1):(i + k),])\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- data.frame(X[(i + k + 1):(i + k + h),])\n  X.test <- X.test[, hay>0]\n  \n  if (length(X.train) > 0) {\n    fit <- try(Arima(train.set, \n                     order = c(1, 1, 1),\n                     seasonal = c(0, 1, 1),\n                     lambda = 0,\n                     xreg=as.matrix(X.train)))\n  } else {\n    fit <- try(Arima(train.set, \n                     order = c(1, 1, 1),\n                     seasonal = c(0, 1, 1),\n                     lambda = 0))\n  }\n  \n  if(!is.element(\"try-error\", class(fit))) {\n    if (length(X.train) > 0) \n      fcast <- forecast(fit, h = h, xreg = as.matrix(X.test)) else\n        fcast <- forecast(fit, h = h)\n      \n    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n  \nerrorArima <- colMeans(mapeArima, na.rm = TRUE)\nerrorArima\n\n [1] 8.377471 8.600894 8.872539 9.262283 9.405912 9.251936 9.321219 9.354090\n [9] 9.070492 9.241635 9.157093 9.303983\n\n\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorArima), colour = \"Blue\") +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"%\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\nFigura 17: Error de predicción (MAPE) según horizonte temporal\n\n\n\n\nLa Figura 17 revela que el error de predicción a un periodo vista es 1.4 puntos porcentuales superior al error de ajuste. Hasta los 5 meses el error aumenta según aumenta el horizonte de predicción. Desde el sexto mes, el error se mantiene estable en torno al 9%-9.5%.\n\n\nInterpretación\nEl modelo teórico es \\(log(chocolate) \\sim ARIMA_{12}(1, 1, 1)(0, 1, 1) + AI\\), \\[(1 - \\phi_1 L)(1 - L^{12})(1 - L)\\log(chocolate_t) = (1 + \\theta_1 L)(1 + \\theta_{12} L^{12})\\varepsilon_t + AI.\\]\nSi sustituimos \\((1 - L^{12})\\log(chocolate_t)\\) por \\(TVA_{chocolate_t}\\), desarrollamos el modelo y sustituimos los parámetros por sus estimaciones, queda \\[\\widehat{TVA}_{chocolate_t} = TVA_{chocolate_{t-1}} + 0.31(TVA_{chocolate_{t-1}} - TVA_{chocolate_{t-2}})\\] \\[- 0.89\\varepsilon_{t-1} - 0.68\\varepsilon_{t-12} + 0.60\\varepsilon_{t-13}\\] \\[+0.036\\cdot DiasLaborables - 0.16\\cdot d0975 + 0.13 \\cdot d0186-0.34 \\cdot d0191 + 0.51\\cdot d0194.\\]\nPor tanto:\n\nLa tasa de variación anual en la producción de chocolate para un mes es la misma que la del mes pasado más un 31% del último incremento observado.\nSi en los meses pasados el valor esperado en la tasa de variación se alejó de la real, hay que tenerlo en cuenta para corregir las previsiones.\nCada día laborable adicional en un mes supone un incremento del 3.6% en la producción.\nEn determinados meses hubo unos valores de producción muy alejados de lo esperado.\n\n\n\nPredicción de la serie\nUna vez dado por válido el modelo podemos pasar a realizar predicciones. Hay que tener en cuenta que hay una variable de intervención de efecto calendario (Días laborables), para la que debemos indicar qué valores tomará en el periodo de predicción. Vamos a fijar el horizonte de predicción en cuatro años (1995 a 1998).\n\ntmp <- ts(rep(0, 48), start = 1995, frequency = 12)\npdl <- bizdays(tmp, FinCenter = \"London\")\n\npchoco.ar2 <- forecast(choco.ar2, \n                     h = 48,\n                     xreg = cbind(pdl, \n                                  rep(0,48), rep(0,48), rep(0,48), rep(0,48)), \n                     level = 95)\nautoplot(pchoco.ar2, \n         xlab = \"\",\n         ylab = \"Toneladas\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1958, 1998, 4)) \n\n\n\n\nFigura 18: Chocolate (1958-1994) y predicción (1995-1998)"
  },
  {
    "objectID": "03-01-Tema1.html#métodos-subjetivos",
    "href": "03-01-Tema1.html#métodos-subjetivos",
    "title": "Introducción",
    "section": "2.1 Métodos subjetivos",
    "text": "2.1 Métodos subjetivos\nRealizar predicciones a partir de opiniones y juicios subjetivos es práctica común. En muchas ocasiones las previsiones subjetivas son la única opción, tales como cuando no hay datos históricos, cuando un nuevo producto se va a sacar al mercado, cuando entra en el mercado un nuevo competidor, o cuando el entorno de mercado cambia notablemente. Por ejemplo, predecir el efecto que iba a tener la Covid-19, una situación sin precedentes, en el grado de ocupación hotelera de agosto de 2020 requiere necesariamente de juicios subjetivos.\nCon los años las predicciones por métodos subjetivos han ido ganando aceptación y reconocimiento como una ciencia. En parte porque la calidad de sus predicciones ha mejorado como resultado directo del uso de metodologías mejor estructuradas y más sistemáticas.\nLa predicciones a partir de opiniones y juicios son subjetivas y por tanto sujetas a un montón de sesgos y limitaciones. Por citar algunas:\n\nPueden ser perfectamente inconsistentes por muchos motivos: se presta más atención a acontecimientos presentes que a los pasados; podemos directamente pasar por alto información relevante; se pueden confundir algunas relaciones causales.\nPueden estar afectadas por la agenda política o personal.\nPueden sufrir del efecto ancla: las predicciones tienden a agruparse en torno a un valor inicial de referencia familiar.\no del efecto de confirmación, donde se da mas peso a la información que confirma nuestras hipótesis previas.\n… (la lista es larga)\n\n\nPrincipios básico en la implementación\nA fin de reducir el efecto adverso de estas limitaciones y sesgos, es fundamental usar una aproximación sistemática y bien estructurada, con independencia de la metodología que finalmente se vaya a seguir. Los principios a seguir son:\n\nDefinir la variable a predecir de forma clara y concisa.\nImplementar un método de previsión sistemático.\nDocumentar todo el proceso con el objetivo de promover la consistencia y la replicabilidad.\nJustificar las predicciones para reducir los sesgos.\nMonitorizar las predicciones para adelantarse a irregularidades o desviaciones importantes respecto de los valores reales.\nSeparar los que hacen las predicciones de los usuarios de dichas predicciones.\n\n\n\nAlgunos métodos subjetivos\n\nJurado de opinión: la predicción la realizan un conjunto de expertos por consenso.\nPredicción por analogía: se realiza la predicción buscando situaciones o entornos análogos donde se conoce la información solicitada.\nMétodo Delphi: similar al jurado de opinión, pero se preserva el anonimato de los expertos y el consenso se obtiene a partir de un proceso iterativo en varias rondas.\nMétodo de escenarios: se definen una serie de posibles escenarios considerando todos los factores relevantes y se les asigna una probabilidad. Para cada escenario se obtiene, siguiendo un determinado método, la predicción deseada.\nAjustes por juicios subjetivos: propiamente este no es un método subjetivo. Consiste en realizar una predicción con información histórica disponible usando herramientas estadísticas para después ser ajustada a partir del juicio de expertos."
  },
  {
    "objectID": "03-01-Tema1.html#métodos-objetivos",
    "href": "03-01-Tema1.html#métodos-objetivos",
    "title": "Introducción",
    "section": "2.2 Métodos objetivos",
    "text": "2.2 Métodos objetivos\nLa predicción se obtiene principalmente a partir de los valores pasados de la propia variable a predecir aunque también se puede utilizar el valor pasado de otras variables\n\\[\\hat y_{t+h} = f(y_t, y_{t-1},\\ldots)\\] donde \\(\\hat y_{t+h}\\) es la previsión que hacemos para el periodo \\(t+h\\) con información solo hasta el periodo \\(t\\).\nDentro de los métodos objetivos distinguiremos entre:\n\nNo paramétricos, donde no se asume ninguna hipótesis estadística y las predicciones se obtienen a partir de los datos pasados con cálculos sencillos que no comportan estimar ningún parámetro. Por ejemplo \\(\\hat y_{t+1} = y_t\\) (método ingenuo I).\nVeremos estas técnicas en los temas 3 a 5: métodos ingenuos, medias móviles y métodos de alisado exponencial.\nParamétricos, se asume una estructura estocástica en la generación de los datos, es necesario que se verifiquen ciertos supuestos estadísticos y hay que estimar parámetros. Por ejemplo \\(\\hat y_{t+1} = \\hat \\mu + \\hat \\phi_1 y_t\\) (modelo autorregresivo de orden 1).\nVeremos estas técnicas en los temas 6 y 7: modelos ARIMA."
  },
  {
    "objectID": "04-03-Redes_neuronales.html",
    "href": "04-03-Redes_neuronales.html",
    "title": "Autorregresión con redes neuronales",
    "section": "",
    "text": "1 Antecedentes\nEn las dos grandes familias de modelos que permiten ajustar y predecir series temporales –Alisado Exponencial y modelos ARIMA– se ajusta un modelo a una serie temporal y el resultado del ajuste nos permite no solo predecir, sino aprender y entender el comportamiento de la serie. Por ejemplo, el resultado del ajuste por alisado nos permite saber si la pendiente de la serie cambia en el tiempo (parámetro \\(\\beta\\) del ajuste) o el tipo de esquema de la serie según que la estacionalidad sea aditiva o multiplicativa. Con los modelos ARIMA podemos estimar el impacto en la serie de un efecto calendario (Semana Santa, días laborables…).\nOtra familia de modelos muy versátiles que permiten predecir con todo tipo de datos –transversales, series temporales, imágenes, espacio-temporales…– son las redes neuronales. Estos modelos son el embrión del Deep Learning y el motor de muchas AI y los estudiaréis en detalle el próximo año en la asignatura Técnicas Avanzadas de Predicción en Negocios.\nVamos a ver muy, pero que muy por encima en que consisten las redes neuronales y como se pueden aplicar para predecir series temporales. Esto es una pequeña píldora.\n\n\n2 Arquitectura de una red neuronal de una capa\nUna red neuronal puede ser entendida como una red de neuronas dispuestas en capas. Siempre hay una capa de entrada de los datos y una capa de salida de la respuesta. Entre estas dos capas se pueden disponer de tantas capas intermedias (ocultas) como se considere necesario.\nCada capa está formada por un número determinado y potencialmente diferente de neuronas o nodos. Los nodos de una capa están conectadas a los nodos de la siguiente. Por simplicidad asumiremos que todos los nodos de una capa se conectan con los nodos de la capa siguiente.\nAquí vamos a considerar solo redes neuronales con una capa intermedia y donde la capa de salida tiene solo una neurona. La Figura 1 es un ejemplo de este tipo de redes neuronales.\n\n\n\n\n\nFigura 1: Red neuronal con una sola capa intermedia\n\n\nEn esta red cada nodo de una capa recibe entradas de los nodos de la capa previa. Dicho de otra forma, las salidas de los nodos de una capa son las entradas de los nodos de la siguiente capa. Es lo que se denomina una multilayer feed-forward network.\nLas entradas que recibe cada nodo se combinan usando una función lineal ponderada. Por ejemplo, un nodo \\(j\\) de la capa intermedia recibe las dos entradas \\(x_1\\) y \\(x_2\\) de los nodos de la capa de entrada y los combina linealmente\n\\[z_j = b_j + \\sum_{i=1}^2 w_{i,j}x_i\\] Para los nodos de la capa intermedia el valor \\(z_j\\) se transforma usando una función no lineal, por ejemplo la sigmoidea:\n\\[s_j = \\frac{1}{1 + e^{-z_j}}\\] y este valor \\(s_j\\) es la salida del nodo \\(j\\) que va al nodo de la capa de salida.\nLos valores de los pesos \\(b_1\\), \\(b_2\\), \\(w_{1,1}\\), \\(w_{1,2}\\)…\\(w_{2,5}\\) se deben ajustar a partir de los datos. Estos valores suelen estar restringidos para evitar que sean demasiado grandes. El parámetro que restringe las ponderaciones se conoce como parámetro de decaimiento, y suele ser igual a \\(0.1\\).\nLos pesos toman valores aleatorios al principio y luego se actualizan con los datos observados en un proceso de aprendizaje. Por lo tanto, hay un elemento de aleatoriedad en las predicciones producidas por una red neuronal. Por este motivo, la red suele entrenarse varias veces utilizando diferentes puntos de partida aleatorios, y los resultados se promedian.\n\n\n3 Autoregresión de redes neuronales\nEn el contexto de series temporales, los valores de entrada pueden ser valores retardados de la serie y el valor de salida deseado el valor contemporáneo. De la misma forma que en un modelo AR usamos los datos pasados para predecir el futuro.\n\n\n\nFigura 2: Red neuronal para predecir una serie temporal. El dato del periodo \\(t\\) se predice a partir de los dos datos previos.\n\n\nVamos a extender estas ideas e ir añadiendo algo de notación.\nComo hemos indicado vamos a considerar solo redes simples con una capa intermedia y una capa de salida de un solo nodo, que denominaremos \\(NNAR\\). La notación \\(NNAR(p, k)\\) indica que hay \\(p\\) valores desfasados en la capa de entrada y \\(k\\) nodos en la capa intermedia. Por ejemplo, la red de la Figura 2 es modelo \\(NNAR(2,5)\\), donde \\(y_{t-1}\\) e \\(y_{t-2}\\) son usados para predecir \\(y_t\\). Así, un modelo \\(NNAR(p, 0)\\) sería equivalente a un modelo \\(ARIMA(p,0,0)\\).\nSi la serie tiene estacionalidad es conveniente que entre los datos de entrada estén las observaciones pasadas de la misma estación que se desea predecir. Por ejemplo, para la serie diaria de consumo eléctrico un modelo \\(NNAR(2, 1, k)\\) usaría como datos de entrada \\(y_{t-1}\\), \\(y_{t-2}\\) e \\(y_{t-7}\\) para predecir \\(y_t\\). En general, \\(NNAR(p,P,k)_m\\) usa como datos de entrada \\(y_{t-1}\\),\\(y_{t-2}\\),…,\\(y_{t-p}\\),\\(y_{t-m}\\), \\(y_{t-2m}\\),…,\\(y_{t-Pm}\\) y una capa intermedia de \\(k\\) neuronas. Por lo tanto, \\(NNAR(p,P,0)_m\\) es equivalente a \\(ARIMA(p,0,0)(P,0,0)_m\\).\n\n\n4 Aplicación\nLa función nnetar de la librería forecast permite estimar modelos \\(NNAR(p,P,k)_m\\). En su forma más sencilla el usuario no tiene que especificar los valor de los parámetros \\(p\\), \\(P\\) y \\(k\\) ya que la función los identifica según ciertos criterios.\nLa siguiente gráfica muestra el consumo eléctrico en España en GWh para 17 semanas desde febrero hasta mayo de 2021. Hay una fuerte componente estacional diaria de orden \\(7\\), donde el consumo es alto de lunes a viernes, algo mas reducido el sábado y aún menor el domingo.\n\nelectricidad <- read_csv2(\"./series/Consumo electrico.csv\")\n\nelectricidad <- ts(electricidad[, 2], \n                   start = c(1, 5),\n                   frequency = 7)\n\nelectricidad <- window(electricidad,\n                       start = c(6, 1),\n                       end = c(22, 7))\n\nautoplot(electricidad) + \n  ggtitle(\"\") +\n  ylab(\"GWh\") + \n  xlab(\"Semana\")\n\n\n\n\nFigura 3: Consumo eléctrico (febrero a mayo de 2021)\n\n\n\n\nLa Figura 4 muestra la serie y su predicción para los siguientes 14 días. El modelo ajustado es \\(NNAR(8,1,4)_7\\). Es decir, para predecir el consumo del día \\(t\\), \\(y_t\\), se usa el consumo de los ocho días previos \\(y_{t-1}\\) a \\(t_{t-8}\\), y de hace una semana \\(y_{t-7}\\), y la capa intermedia tiene cuatro nodos. Observa que en la capa de entrada, con 9 nodos, el valor de \\(y{t_7}\\) entra en dos nodos.\n\nfit <- nnetar(electricidad)\n\naccuracy(fit)\n\n                     ME     RMSE     MAE         MPE      MAPE      MASE\nTraining set 0.02916695 6.267122 4.50083 -0.01593382 0.6739243 0.2260588\n                    ACF1\nTraining set -0.03286461\n\npfit <- forecast(fit, h = 14)\n\nautoplot(pfit) +\n  ylab(\"GWh\") + \n  xlab(\"Semana\")\n\n\n\n\nFigura 4: Consumo eléctrico (febrero a mayo de 2021) y predicción\n\n\n\n\nEl cálculo de intervalos de confianza con redes neuronales es un proceso complejo y costoso temporalmente.\n\ntiempo <- Sys.time()\npfit <- forecast(fit, \n                 h = 14, \n                 level = 95,\n                 PI = TRUE)\n\nSys.time() - tiempo\n\nTime difference of 11.93189 secs\n\npfit\n\n         Point Forecast    Lo 95    Hi 95\n23.00000       660.1970 648.0293 673.0927\n23.14286       687.4519 669.7536 704.8511\n23.28571       692.2981 670.1886 711.5032\n23.42857       696.9493 662.7058 722.1158\n23.57143       692.7313 660.0361 712.7235\n23.71429       627.7911 593.0543 658.6375\n23.85714       576.4037 552.4308 603.8449\n24.00000       656.4954 617.8300 691.7628\n24.14286       682.8674 647.1780 717.4875\n24.28571       687.2124 659.3483 718.4406\n24.42857       690.8260 657.3454 725.1586\n24.57143       689.4707 655.0937 715.4719\n24.71429       630.4250 594.0594 679.4806\n24.85714       575.3611 551.6234 627.3078\n\n\nLa función nnetar admite la inclusión de variables de intervención de la forma usual a través del argumento xreg."
  }
]