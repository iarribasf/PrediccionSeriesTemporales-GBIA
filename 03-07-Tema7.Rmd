---
title: "Procesos ARIMA con estacionalidad"
subtitle: "Previsión con Datos Temporales (GBIA)"
author: "Iván Arribas (Depto. Análisis Económico. Universitat de València)"
output: 
  html_document:
    code_download: yes
    df_print: kable
    fig_caption: no
    highlight: pygments
    number_sections: yes
    self_contained: yes
    theme: cerulean
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
---

```{r chunk_setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      comment = "",
                      fig.align = "center", 
                      fig.show = "hold",
                      fig.height = 4,
                      fig.width = 8,
                      out.width = "80%") 
```

```{r options_setup, echo = FALSE}
options(scipen = 999) #- para quitar la notacion cientifica
```

```{r librerias, echo = FALSE}
library(forecast)
library(ggplot2); theme_set(theme_bw())
library(gridExtra)
library(grid)
library(aod)
library(seasonal)
```



# Introducción

En general, la capacidad de toma de datos posibilita que el fechado de las series sea inferior al año, apareciendo la componente estacional. No es difícil disponer de series mensuales, trimestrales, diarias o incluso con frecuencia superior.

En este tema revisaremos los conocimientos del tema previo, ampliándolos a fin de contemplar la presencia de estacionalidad en las series temporales

Se mantienen las hipótesis sobre el proceso generador de datos, _estacionario_ y _ergódico_.

Denotaremos por $m$ el orden estacional.

\
\

# Procesos ARIMA con estacionalidad

## Procesos autorregresivos $AR_m(P)$

### Definición {-}

El modelo general __autorregresivo estacional de orden P__, $y_t \sim AR_m(P)$ viene definido por
$$y_t=c + \phi_m y_{t-m} + \phi_{2m} y_{t-2m} + ... + \phi_{Pm} y_{t-Pm} + \varepsilon_t,$$
que usando el operador retardo queda
  $$(1 - \phi_m L^m - \phi_{2m} L^{2m} - ... - \phi_{Pm} L^{Pm})y_t = c + \varepsilon_t.$$

### Ejemplos {-}

* $y_t \sim AR_m(1):\;\;y_t = c + \phi_m y_{t-m} + \varepsilon_t$ o $(1 - \phi_m L^m)y_t = c + \varepsilon_t$
* $y_t \sim AR_m(2):\;\;y_t = c + \phi_m y_{t-m} + \phi_{2m} y_{t-2m} + \varepsilon_t$ o $(1 - \phi_m L^m - \phi_{2m} L^{2m})y_t = c + \varepsilon_t$


\

## Procesos en medias móviles $MA_m(Q)$

### Definición {-}

El modelo general __en medias móviles estacional de orden Q__, $y_t \sim MA_m(Q)$ viene definido por
$$y_t=c + \varepsilon_t + \theta_m \varepsilon_{t-m} + \theta_{2m} \varepsilon_{t-2m} + ... +
  \theta_{Qm} \varepsilon_{t-Qm},$$
que usando el operador retardo queda
$$y_t = c + (1 + \theta_m L^m + \theta_{2m} L^{2m} + ... + \theta_{Qm} L^{Qm}) \varepsilon_t.$$
  
### Ejemplos {-}

* $y_t \sim MA_m(1):\;\;y_t = c + \varepsilon_t + \theta_m \varepsilon_{t-m}$ o $y_t = c + (1 + \theta_m L^m)\varepsilon_t$
* $y_t \sim MA_m(2):\;\;y_t=c + \varepsilon_t + \theta_m \varepsilon_{t-m} + \theta_{2m} \varepsilon_{t-2m}$ o $y_t = c + (1 + \theta_m L^m + \theta_{2m} L^{2m})\varepsilon_t$

\

## Procesos $ARMA_m(P,Q)$

### Definición {-}

El modelo general __$y_t \sim ARMA_m(P,Q)$__ viene definido por
$$ y_t = c + \phi_m y_{t-m} + \phi_{2m} y_{t-2m} + ... + \phi_{Pm} y_{t-Pm}  + 
  \varepsilon_t + \theta_m \varepsilon_{t-m} + \theta_{2m} \varepsilon_{t-2m} + ... +
  \theta_{Qm} \varepsilon_{t-Qm},
$$
que usando el operador retardo queda
$$(1 - \phi_m L^m - ... - \phi_{Pm} L^{Pm})y_t = c + (1 + \theta_m L^m + ... + \theta_{Qm} L^{Qm}) \varepsilon_t.$$
  
### Ejemplo {-}

$y_t \sim ARMA_m(1, 1):\;\;y_t = c  + \phi_m y_{t-m} + \theta_m \varepsilon_{t-m} + \varepsilon_{t}$ o $(1 - \phi_m L^m)y_t = c + (1 + \theta_m L^m)\varepsilon_t$.


\

## Procesos $ARIMA_m(P,D,Q)$

Si la serie $y_t$ no es estacionaria en su parte estacional, pero tras diferenciarla $D$ veces se hace estacionaria, diremos que la serie es integrada estacionalmente de orden $D$: $y_t \sim I_m(D)$. Por tanto,

* una serie estacionaria estacionalmente se indicará como $y_t \sim I_m(0)$.
* $y_t \sim I_m(1)$ es equivalente a $\nabla_m y_t = (1 - L^m)y_t \sim I_m(0)$ 

Una serie $y_t$ sigue un proceso __$ARIMA_m(P,D,Q)$__ si:

1. $y_t \sim I_m(D)$ (hay que diferenciarla estacionalmente $D$ veces para hacerla estacionaria), y
2. $\nabla_m^D y_t \sim ARMA_m(P,Q)$.

Entonces, podemos escribir $y_t \sim ARIMA_m(P,D,Q)$ como
$$(1 - \phi_m L^m - ... - \phi_{Pm} L^{Pm})(1- L^m)^D y_t = c + (1 + \theta_m L^m + ... + \theta_{Qm} L^{Qm}) \varepsilon_t$$

### Ejemplo {-}

$y_t \sim ARIMA_m(1, 1, 1):\;\;(1 - \phi_m L^m)(1- L^m) y_t = c + (1 + \theta_m L^m) \varepsilon_t$ o 

$y_t = c + y_{t-m} + \phi_m(y_{t-m} - y_{t-2m}) + \theta_m \varepsilon_{t-m} + \varepsilon_t$.

\

## Proceso $ARIMA_m(p,d,q)(P,D,Q)$
  
La realidad nos muestra que la mayoría de las series con estacionalidad se ajustan a una combinación de _procesos regulares y estacionales_.

El proceso $ARIMA_m(p, d, q)(P, D, Q)$ puede ser expresado como
$$ (1 - \phi_1 L - ... - \phi_p L^p)(1 - \phi_m L^m - ... - \phi_{Pm} L^{Pm})(1- L^m)^D(1 - L)^d y_t = $$
$$c + (1 + \theta_1 L + ... + \theta_q L^q)(1 + \theta_m L^m + ... + \theta_{Qm} L^{Qm}) \varepsilon_t$$

Por ejemplo, entre las series mensuales uno de los procesos más comunes es $ARIMA_{12}(0, 1, 1)(0, 1, 1)$, denominado *modelo de las aerolíneas* por ser el PGD de muchas series mensuales de transporte de pasajeros, en concreto la serie mensual de pasajeros de avión. La ecuación de este modelo es

$$(1-L)(1-L^{12})y_t = (1+ \theta_1L)(1 + \theta_{12}L^{12})\varepsilon_t$$
que si desarrollamos queda
$$y_t = y_{t-1} + (y_{t-12} - y_{t-13}) + \theta_1 \varepsilon_{t-1} + \theta_{12} \varepsilon_{t-12} + \theta_{13} \varepsilon_{t-13} + \varepsilon_t $$

* El número de pasajeros del mes $t$ es el mismo que el del mes previo $t-1$, más la diferencia entre estos meses observada el año pasado. 
* Si en los meses usados para la predicción ($t-1$, $t-12$ y $t-12$) ha ocurrido algo extraordinario, hay que tenerlo en cuenta a la hora de afinar la predicción.

\
\

# Ejemplos
  
## Nacimientos
  
Vamos a aplicar la metodología de Box-Jenkins a la serie de __nacimientos en España__ desde el año 2000 (véase figura 1).

```{r}
nacimientos <- read.csv2("./series/nacimientos.csv", header = TRUE)
nacimientos <- ts(nacimientos[, 2],
                  start = c(1975, 1),
                  freq = 12)
nacimientos <- window(nacimientos, start = 2000)

autoplot(nacimientos,
         xlab = "",
         ylab = "Nacimientos",
         main = "Figura 1. Nacimientos en España")
```


### Transformación de la serie {-}

Ya vimos en el Tema 6 que para que la serie sea estacionaria y ergódica había que diferenciarla tanto regular como
estacionalmente (d = D = 1). Además, trabajaremos con el logaritmo de la serie para ganar en interpretabilidad. Es decir, trabajaremos con la siguiente serie transformada 
$$\nabla\nabla_{12}\log(nacimientos_t) \sim I(0)I_{12}(0).$$


### Identificación {-}

¿Qué nos indica `auto.arima`? Primero vamos a generar e incluir en el proceso de auto-identificación las variables asociadas a los efectos de intervención que hemos detectado en los temas previos. En concreto, hemos visto que en los meses de febrero de años bisiestos nacían más bebés, y que en abril de 2008 y enero de 2011 el número de nacimientos era atípico.

Para el calculo de la variable que recoge el efecto año bisiesto usaremos la función `monthdays` en la librería `forecast` que devuelve el número de días de cada mes o trimestre de una serie.
```{r, eval = FALSE}
monthdays(nacimientos)
```

```{r, echo = FALSE}
head(monthdays(nacimientos), n = 60)
```

```{r} 
Bisiestos <- 1*(monthdays(nacimientos) == 29)
d0408 <- 1*(cycle(nacimientos) == 4 & trunc(time(nacimientos)) == 2008)
d0111 <- 1*(cycle(nacimientos) == 1 & trunc(time(nacimientos)) == 2011)

auto.arima(nacimientos, 
           d = 1, 
           D = 1, 
           lambda = 0,
           xreg = cbind(Bisiestos, d0408, d0111))
```   

Indica $ARIMA_{12}(0,1,1)(0,1,2)$, aunque no está claro si coeficiente `sma2` será significativo (su valor a penas supera los dos errores estándar).
 
\

Una alternativa a `auto.arima` es la función `seas` de la librería `seasonal`. La función `seas` tiene como ventajas que también analiza la conveniencia de usar la transformación logarítmica, que identifica posibles efectos calendario y valores extremos, y que suele ser más parsimoniosa que `auto.arima`. Su desventaja es que sólo se puede aplicar para series mensuales o trimestrales. Veamos que identificación ofrece `seas`:
```{r} 
summary(seas(nacimientos))
``` 

En primer lugar, la función identifica el modelo de las aerolíneas para la transformación logarítmica de Nacimientos. Este resultado está en consonancia con el alcanzado con la función `auto.arima`. Además, identifica un valor extremo en diciembre de 2010, un efecto calendario Semana Santa (cuyo análisis puedes ver en el Ejemplo de Pasajeros) y un efecto calendario días laborables del mes (que podemos entender equivalente a nuestro efecto días del mes).

Tras estos análisis alternativos, decidimos que la identificación de partida es $ARIMA_{12}(0,1,1)(0,1,2)+AI$,
$$(1 - L^{12})(1 - L)\log(nacimientos_t) = (1 + \theta_1 L)(1 + \theta_{12} L^{12} + \theta_{24} L^{24})\varepsilon_t + AI.$$
donde _AI_ recoge las dos variables de intervención incluidas en `auto.arima` y la identificada por `seas`.

### Estimación (y valores extremos) {-}

Vamos a estimar el modelo identificado, incluidas las variables de intervención.

```{r}
d1210 <- 1*(cycle(nacimientos) == 12 & trunc(time(nacimientos)) == 2010)
nac.ar1 <- Arima(nacimientos, 
                 order = c(0, 1, 1),
                 seasonal = list(order = c(0, 1, 2), period = 12),
                 lambda = 0,
                 xreg = cbind(Bisiestos, d0408, d0111, d1210))
nac.ar1
```

Ya tenemos un modelo de partida, veamos si es necesaria más intervención.

```{r} 
error <- residuals(nac.ar1)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 2. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2,2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  scale_x_continuous(breaks= seq(2000, 2018, 2)) 
```

Se observa que hay dos candidatos a valores atípicos en diciembre 2006 y junio de 2016, dado que los errores asociados se acercan a las 3 desviaciones típicas. Por tanto,

* Se crea una variable de intervención para cada caso
* Se estima de nuevo el modelo incluyendo estas variables
* Se vuelve a analizar si quedan valores atípicos

```{r}
d0616 <- 1*(cycle(nacimientos) == 6 & trunc(time(nacimientos)) == 2016)
d1206 <- 1*(cycle(nacimientos) == 12 & trunc(time(nacimientos)) == 2006)

nac.ar2 <- Arima(nacimientos, 
                 order = c(0,1,1),
                 seasonal = list(order = c(0, 1, 2), period = 12),
                 lambda = 0,
                 xreg = cbind(Bisiestos, d1206, d0408, d1210, d0111, d0616))
nac.ar2
 
error <- residuals(nac.ar2)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 3. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  scale_x_continuous(breaks= seq(2000, 2018, 2)) 
```

Observamos que no hay valores extremos.

Por último, veamos si todos los coeficientes del modelo son significativos. Para ello, aplicamos la prueba de Wald.

```{r}
ancho <- max(nchar(names(coef(nac.ar2)))) + 2
for(i in 1:length(coef(nac.ar2))) {
  wt <- wald.test(b = coef(nac.ar2), 
                  Sigma = vcov(nac.ar2), 
                  Terms = i)
  cat("\nCoeficiente: ", format(names(coef(nac.ar2))[i], width = ancho), "valor de p: ", 
      formatC(wt$result$chi2[3], digits = 4, format = "f"))
}
```

Solo el coeficiente estacional en medias móviles de orden dos no resulta _individualmente_ significativo al 5% (aunque si lo sería al 10%). Vamos a optar por dejarlo.

### Error de estimación {-}

El error medio es `r round(accuracy(nac.ar2)[1],0)`, muy bajo en comparación con el valor medio de la serie.

Además, tenemos las diferentes medidas de bondad del ajuste. En media nos equivocamos en `r round(accuracy(nac.ar2)[2],0)` bebés (RMSE) y el error porcentual medio es del `r round(accuracy(nac.ar2)[5],1)`%.

```{r,eval=FALSE}
accuracy(nac.ar2)
```

```{r,echo=FALSE}
round(accuracy(nac.ar2),2)
```

### Error de predicción extra-muestral según horizonte temporal {-}

Asumimos que se precisan diez años para hacer una buena estimación, $k = 120$, y fijaremos el horizonte temporal en un año, $h = 12$ meses.

```{r}  
k <- 120                   #Minimo numero de datos para estimar
h <- 12                    #Horizonte de las predicicones
T <- length(nacimientos)   #Longitud serie
s<-T - k - h               #Total de estimaciones

mapeArima <- matrix(NA, s + 1, h)

X <- cbind(Bisiestos, d1206, d0408, d1210, d0111, d0616)

for (i in 0:s) {
  train.set <- subset(nacimientos, start = i + 1, end = i + k)
  test.set <-  subset(nacimientos, start = i + k + 1, end = i + k + h) 
  
  X.train <- X[(i + 1):(i + k),]
  hay <- colSums(X.train)
  X.train <- X.train[, hay>0]
  
  X.test <- X[(i + k + 1):(i + k + h),]
  X.test <- X.test[, hay>0]
  
  if (length(X.train) > 0) {
    fit <- Arima(train.set, 
                 order = c(0, 1, 1),
                 seasonal = list(order = c(0, 1, 2), period = 12),
                 lambda = 0,
                 xreg=X.train)} else {
                   fit <- Arima(train.set, 
                                order = c(0, 1, 1),
                                seasonal = list(order = c(0, 1, 2), period = 12),
                                lambda = 0)
                 }
  
  if (length(X.train) > 0) fcast <- forecast(fit, h = h, biasadj = TRUE, xreg = X.test) else
    fcast <- forecast(fit, h = h, biasadj = TRUE)
  
  mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set
}
  
errorArima <- colMeans(mapeArima)
errorArima
```

```{r}
ggplot() +
  geom_line(aes(x = 1:12, y = errorArima), colour = "Blue") +
  ggtitle("Figura 4. Error de predicción (MAPE) según horizonte temporal") +
  xlab("Horizonte temporal de predicción") +
  ylab("%") +
  scale_x_continuous(breaks= 1:12)
```

La figura 4 revela que el error de predicción aumenta de forma regular según aumenta el horizonte de predicción, pero incluso a un año vista, se mantiene bajo.

### Interpretación {-}

El _modelo teórico_ es $log(nacimientos) \sim ARIMA_{12}(0, 1, 1)(0, 1, 2) + AI$,
$$(1 - L^{12})(1 - L)\log(nacimientos_t) = (1 + \theta_1 L)(1 + \theta_{12} L^{12} + + \theta_{24} L^{24})\varepsilon_t + AI.$$
Si sustituimos $(1 - L^{12})\log(nacimientos_t)$ por $TVA_{nacimientos_t}$, la tasa de variación anual de los nacimientos, y desarrollamos queda
$$TVA_{nacimientos_t} = TVA_{nacimientos_{t-1}} + \theta_1 \varepsilon_{t-1} + \theta_{12} \varepsilon_{t-12} + \theta_{24} \varepsilon_{t-24} + \ldots + \varepsilon_{t}+$$
$$+\gamma_1 \cdot Bisiestos_t+\gamma_2 \cdot d1206_t + \gamma_3 \cdot d0408_t +\gamma_4 \cdot d0111_t+ \gamma_5 \cdot d0616_t.$$

Finalmente, el _modelo estimado_ es,
$$\widehat{TVA}_{nacimientos_{t-1}} = TVA_{nacimientos_{t-1}}  - 0.45\varepsilon_{t-1} - 0.62\varepsilon_{t-12} - 0.16\varepsilon_{t-24} + \ldots + $$
$$0.037\cdot DiasMes_t - 0.042\cdot d1206_t + 0.057\cdot d0408_t - 0.07\cdot d0111_t + 0.032\cdot d0616_t.$$
  
* En cada mes, __la tasa de variación anual de los nacimientos es la misma que la del mes pasado__ ($\widehat{TVA}_{nacimientos_t} = TVA_{nacimientos_{t-1}}$). 

* Además, si la tasa de variación anual del mes previo fue anómala ($\varepsilon_{t-1}$ diferente de cero) o la tasa de variación anual de hace uno o dos años fue anómala ($\varepsilon_{t-12}$ o $\varepsilon_{t-24}$ diferente de cero), corregimos la previsión para compensar (términos en medias móviles).
    
* Respecto de la intervención, los meses de febrero bisiestos nacen un 3.7% más de bebés. Por alguna razón, en diciembre de 2006 hubo un 4.2% menos de nacimientos de lo esperado; en abril de 2008 hubo un 5.7% más de nacimientos de lo esperado; en enero de 2011 hubo un 7% menos de nacimientos de lo esperado; y en junio de 2016 hubo un 3.2% más de nacimientos de lo esperado.

### Predicción de la serie {-}

Una vez dado por válido el modelo podemos pasar a realizar predicciones. Hay que tener en cuenta que hay cinco variables de intervención, una de ellas un efecto calendario (Bisiestos), para las que debemos indicar qué valores tomarán en el periodo de predicción. Vamos a fijar el horizonte de predicción en cuatro años y mostrar los resultado numérica (solo para el primer año) y gráficamente (figura 5).


```{r, results = "hide"}
pbi <- 1*(monthdays(ts(rep(0, 48), start = 2019, freq = 12)) == 29)
pnac.ar2 <- forecast(nac.ar2, 
                     h = 48,
                     xreg = cbind(pbi, rep(0,48), rep(0,48), rep(0,48), rep(0,48), rep(0,48)), 
                     level = 95)
pnac.ar2
```

```{r, echo = FALSE}
forecast(nac.ar2, 
         h = 12,
         xreg = cbind(rep(0,12), rep(0,12), rep(0,12), rep(0,12), rep(0,12), rep(0,12)), 
         level = 95)
```

```{r}
autoplot(pnac.ar2, 
     ylab = 'Nacimientos',
     main = 'Figura 5. Nacimientos (2000-2018) y predicción (2019-2022)') +
  scale_x_continuous(breaks= seq(2000, 2022, 4)) 
```

\

## Chocolate

Vamos a aplicar la metodología de Box-Jenkins a la serie mensual de __producción de chocolate en Australia__ desde enero de 1958 hasta diciembre de 1994, 444 meses.


```{r}
chocolate <- read.csv2("./series/Chocolate.csv", header = TRUE)
chocolate <- ts(chocolate, start = 1958, freq = 12)

autoplot(chocolate, 
         xlab = "", 
         ylab = "Toneladas",
         main = "Figura 6. Producción de Chocolate (Australia)")
```

La producción de chocolate presenta una tendencia creciente, especialmente desde los años 80 del siglo pasado, una marcada componente estacional (donde la máxima producción se da en el invierno austral, meses de mayo a agosto, y la menor producción en enero), combinados en un esquema multiplicativo. Trabajaremos con el logaritmo de la serie para linealizar las componentes y reducir la falta de estacionariedad en varianza.

### Transformación de la serie {-}

La figura 7 muestra que solo la doble diferenciación regular y estacional consigue que la transformación logarítmica de Chocolate sea estacionaria y ergódica.

```{r, eval = FALSE}
ggAcf(log(chocolate), lag = 48, 
      xlab = "", ylab = "d = D = 0", main = "Figura 13. FAC")
ggAcf(diff(log(chocolate)), lag = 48, 
      xlab = "", ylab = "d = 1 - D = 0", main = "")
ggAcf(diff(log(chocolate), lag = 12), lag = 48, 
      xlab = "", ylab = "d = 0 - D = 1", main = "")
ggAcf(diff(diff(log(chocolate), lag = 12)), lag = 48, 
      xlab = "", ylab = "d = 1 - D = 1", main = "")
```


```{r, echo = FALSE}
grid.arrange(
  ggAcf(log(chocolate), lag = 48, xlab = "", ylab = "d = D = 0", main = "Figura 7. FAC"),
  ggAcf(diff(log(chocolate)), lag = 48, xlab = "", ylab = "d = 1 - D = 0", main = ""),
  ggAcf(diff(log(chocolate), lag = 12), lag = 48, xlab = "", ylab = "d = 0 - D = 1", main = ""),
  ggAcf(diff(diff(log(chocolate), lag = 12)), lag = 48, xlab = "", ylab = "d =  D = 1", main = ""),
  nrow = 2
)
```

Las funciones que identifican la diferenciación de forma automática avalan esta decisión.

```{r}
ndiffs(chocolate)
nsdiffs(chocolate)
```


Podemos concluir que $d = 1$ y $D = 1$ o $\log(chocolate_t) \sim I(1)I_{12}(1)$.

### Identificación {-}

¿Qué nos indica `auto.arima`? Para ayudar a esta función lo máximo posible vamos a definir una variable de intervención que pueden ser relevante:

* __Días laborables__: al tratarse de datos de producción, el número de días laborables de cada mes puede ser un determinante del valor de la serie. Entenderemos por días laborables los lunes a viernes de cada mes, menos los días festivos. Este valor se puede obtener con la función `bizdays`. Un inconveniente de esta función, es que su valor depende de los festivos de cada país. Por ejemplo, en EE.UU es festivo el Día de Acción de Gracias (cuarto jueves de noviembre), pero no en España;  y aquí es festivo el día de la Constitución (6 de diciembre), que no lo es en EE.UU. Por proximidad cultural, histórica y política, usaremos el calendario de Lóndres para Australia. (`R` permite crear tu propio calendario de festivos y existen otras librerías que extienden las opciones de `bizdays`.)

```{r, eval = FALSE}
bizdays(chocolate, FinCenter = "London")
```

```{r, echo = FALSE}
subset(bizdays(chocolate, FinCenter = "London"), end = 60)
```

Vamos a crear esta variable y a incluirla en la auto-identificación.

```{r}
# Efecto año días laborables
DiasLaborables <- bizdays(chocolate, FinCenter = "London")

auto.arima(chocolate, d = 1, D = 1, 
           lambda = 0,
           xreg = cbind(DiasLaborables))
```

Indica $ARIMA_{12}(1,1,1)(0,1,2)$, donde el parámetro $\theta_{24}$ no está claro si es significativo. Además, hay un efecto días laborables significativo.

\

Por último, la función `seas` (también con un poco de ayuda) identifica un proceso $ARIMA_{12}(1,1,1)(0,1,1)$, con un efecto Semana Santa (de ocho días) un efecto día de la semana y un efecto año bisiesto, que conjuntamente equivalen al efecto días laborables.

```{r}
summary(seas(diff(diff(log(chocolate), lag = 12))))
```

Aparentemente ambas funciones identifican el mismo modelo, si del modelo identificado con `auto.arima` excluimos el parámetro no significativo.

### Estimación  {-}

Vamos a realizar la primera estimación.

```{r}
choco.ar1 <- Arima(chocolate, order=c(1, 1, 1),
                   seasonal = list(order = c(0, 1, 1), period = 12),
                   lambda = 0,
                   xreg = DiasLaborables)
choco.ar1
```

Ya tenemos un modelo de partida, veamos si es necesaria más intervención.

```{r} 
error <- residuals(choco.ar1)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 8. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  scale_x_continuous(breaks= seq(1958, 1994, 2)) 
```

Se observan tres valores claramente atípicos en enero de 1986, 1991 y 1994. Además, hay otro candidato a valor extremo, por superar las 2.5 desviaciones típicas, en septiembre de 1975. Por tanto,

* Se crea una variable de intervención para cada caso
* Se estima de nuevo el modelo incluyendo estas variables
* Se vuelve a analizar si quedan valores atípicos

```{r}
d0975 <- 1*(cycle(chocolate) == 9 & trunc(time(chocolate)) == 1975)
d0186 <- 1*(cycle(chocolate) == 1 & trunc(time(chocolate)) == 1986)
d0191 <- 1*(cycle(chocolate) == 1 & trunc(time(chocolate)) == 1991)
d0194 <- 1*(cycle(chocolate) == 1 & trunc(time(chocolate)) == 1994)

choco.ar2 <- Arima(chocolate, order=c(1, 1, 1),
                   seasonal = list(order = c(0, 1, 1), period = 12),
                   lambda = 0,
                   xreg = cbind(DiasLaborables, d0975, d0186, d0191, d0194))
choco.ar2
```

Ahora debemos volver a verificar si en el nuevo modelo hay valores extremos. Observamos que no hay.
```{r} 
error <- residuals(choco.ar2)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "Figura 9. Error + Intervención") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) +
  scale_x_continuous(breaks= seq(1958, 1994, 2)) 
```
Por último, veamos si todos los coeficientes del modelo son significativos. Aunque uno de ellos (enero 1986) no lo es al 5%, si lo es al 10% así que optamos por dejarlo.

```{r}
ancho <- max(nchar(names(coef(choco.ar2)))) + 2
for(i in 1:length(coef(choco.ar2))) {
  wt <- wald.test(b = coef(choco.ar2), 
                  Sigma = vcov(choco.ar2), 
                  Terms = i)
  cat("\nCoeficiente: ", 
      format(names(coef(choco.ar2))[i], width = ancho),
      "valor de p: ", 
      formatC(wt$result$chi2[3], digits = 4, format = "f"))
}
```


### Error de estimación {-}

Analizando los criterios de bondad de ajuste se tiene que el error medio `r round(accuracy(choco.ar2)[1],2)` es prácticamente cero por lo que no parece que haya sesgo en las predicciones; en media nos equivocamos en `r round(accuracy(choco.ar2)[2],0)` toneladas (RMSE); y el error porcentual medio es `r round(accuracy(choco.ar2)[5],1)`%, razonable.

```{r,eval=FALSE}
accuracy(choco.ar2)
```

```{r,echo=FALSE}
round(accuracy(choco.ar2),2)
```

### Error de predicción extra-muestral según horizonte temporal {-}

Asumimos que se precisan diez años para hacer una buena estimación, $k = 120$, y fijaremos el horizonte temporal en un año, $h = 12$ meses.

```{r}  
k <- 120                #Minimo numero de datos para estimar
h <- 12                 #Horizonte de las predicicones
T <- length(chocolate)  #Longitud serie
s<-T - k - h            #Total de estimaciones

mapeArima <- matrix(NA, s + 1, h)

X <- cbind(DiasLaborables, d0975, d0186, d0191, d0194)

for (i in 0:s) {
  train.set <- subset(chocolate, start = i + 1, end = i + k)
  test.set <-  subset(chocolate, start = i + k + 1, end = i + k + h) 
  
  X.train <- X[(i + 1):(i + k),]
  hay <- colSums(X.train)
  X.train <- X.train[, hay>0]
  
  X.test <- X[(i + k + 1):(i + k + h),]
  X.test <- X.test[, hay>0]
  
  if (length(X.train) > 0) {
    fit <- Arima(train.set, 
                 order = c(1, 1, 1),
                 seasonal = list(order = c(0, 1, 1), period = 12),
                 lambda = 0,
                 xreg=X.train)} else {
                   fit <- Arima(train.set, 
                                order = c(1, 1, 1),
                                seasonal = list(order = c(0, 1, 1), period = 12),
                                lambda = 0)
                 }
  
  if (length(X.train) > 0) fcast <- forecast(fit, h = h, biasadj = TRUE, xreg = X.test) else
    fcast <- forecast(fit, h = h, biasadj = TRUE)
  
  mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set
}
  
errorArima <- colMeans(mapeArima)
errorArima
```

```{r}
ggplot() +
  geom_line(aes(x = 1:12, y = errorArima), colour = "Blue") +
  ggtitle("Figura 10. Error de predicción (MAPE) según horizonte temporal") +
  xlab("Horizonte temporal de predicción") +
  ylab("%") +
  scale_x_continuous(breaks= 1:12)
```

La figura 10 revela que el error de predicción a un periodo vista es superior en un punto porcentual al error de ajuste. El error aumenta según aumenta el horizonte de predicción, más aceleradamente en los primeros meses y de forma más atenuada para los últimos meses.


### Interpretación {-}

El _modelo teórico_ es $log(chocolate) \sim ARIMA_{12}(1, 1, 1)(0, 1, 1) + AI$,
$$(1 - \phi_1 L)(1 - L^{12})(1 - L)\log(chocolate_t) = (1 + \theta_1 L)(1 + \theta_{12} L^{12})\varepsilon_t + AI.$$
Si sustituimos $(1 - L^{12})\log(chocolate_t)$ por $TVA_{chocolate_t}$, desarrollamos el modelo y sustituimos los parámetros por sus estimaciones, queda
$$\widehat{TVA}_{chocolate_t} = TVA_{chocolate_{t-1}} + 0.31(TVA_{chocolate_{t-1}} - TVA_{chocolate_{t-2}})$$
$$- 0.89\varepsilon_{t-1} - 0.68\varepsilon_{t-12} + 0.60\varepsilon_{t-13}$$
$$+0.036\cdot DiasLaborables - 0.16\cdot d0975 + 0.13 \cdot d0186-0.34 \cdot d0191 + 0.51\cdot d0194.$$
Por tanto:

* La tasa de variación anual en la producción de chocolate para un mes es la misma que la del mes pasado más un 31% del último incremento observado.
* Si en los meses pasados el valor esperado en la tasa de variación se alejó del real, hay que tenerlo en cuenta para corregir las previsiones.
* Cada día laborable adicional en un mes supone un incremento del 3.6% en la producción.
* En determinados meses hubo unos valores de producción muy alejados de los esperado.

### Predicción de la serie {-}

Una vez dado por válido el modelo podemos pasar a realizar predicciones. Hay que tener en cuenta que hay una variables de intervención de efecto calendario (Días laborables), para la que debemos indicar qué valores tomará en el periodo de predicción. Vamos a fijar el horizonte de predicción en cuatro años (1995 a 1998).

```{r}
dummy <- ts(rep(0, 48), start = 1995, frequency = 12)
pdl <- bizdays(dummy, FinCenter = "London")

pchoco.ar2 <- forecast(choco.ar2, 
                     h = 48,
                     xreg = cbind(pdl, rep(0,48), rep(0,48), rep(0,48), rep(0,48)), 
                     level = 95,
                     biasadj = TRUE)
autoplot(pchoco.ar2, 
         xlab = "",
         ylab = 'Toneladas',
         main = 'Figura 11. Chocolate (1958-1994) y predicción (1995-1998)') +
  scale_x_continuous(breaks= seq(1958, 1998, 4)) 
```

\

# Comparación con el método de alisado exponencial

Veamos una comparativa, para los dos ejemplos vistos, entre los resultados obtenidos con ARIMA y con Alisado exponencial

### Nacimientos (log) {-}

|Método   |Modelo                   |MAPE              |
|:--------|:------------------------|:-----:|:----------:|:----------:|:-----------:|
|         |                         |Ajuste |Extra h = 1 |Extra h = 6 |Extra h = 12 |
| ARIMA   |ARIMA(0,1,1)(0,1,2) + AI | 1.55  | 1.72       | 2.61       | 3.47        |
| Alisado |ETS(A,Ad,A)              | 1.75  | 1.91       | 2.90       | 3.83        |

Para Nacimientos, la mejora en los indicadores de calidad con ARIMA respecto de Alisado es tan pequeña que no justifica su aplicación. Incluso para previsiones extra-muestrales a 12 meses vista, Alisado sólo es inferior en medio punto porcentual a ARIMA.

### Chocolate (log) {-}

|Método   |Modelo                   |MAPE              |
|:--------|:------------------------|:-----:|:----------:|:----------:|:-----------:|
|         |                         |Ajuste |Extra h = 1 |Extra h = 6 |Extra h = 12 |
| ARIMA   |ARIMA(1,1,1)(0,1,1) + AI | 6.99  | 8.18       | 9.38       | 9.95        |
| Alisado |ETS(A,A,A)               | 8.15  | 9.13       | 10.46      | 10.15       |

En el caso de Chocolate, la mejora en la calidad del ajuste con ARIMA respecto de Alisado está cercana al punto porcentual en previsiones a unos pocos meses vista. Sin embargo, a doce meses vista ambos métodos ofrecen igual calidad en sus predicciones.



\
\
\
\

